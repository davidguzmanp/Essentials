{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "25011f7ad8b04654bdb5187567eac231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64810c7942694ade85fe707d5ff48570",
              "IPY_MODEL_ffa9474f867c49c0a3ce2c2436ab99d2",
              "IPY_MODEL_3e25abec8dc5475c871ef793aff75e0c"
            ],
            "layout": "IPY_MODEL_e6e03eb8778549abb7a4577a3ef29461"
          }
        },
        "64810c7942694ade85fe707d5ff48570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c13e7b0f5fa3478c92f4a6a900fa6497",
            "placeholder": "​",
            "style": "IPY_MODEL_1326460e83bd4c75b4628503c86354c8",
            "value": "Map: 100%"
          }
        },
        "ffa9474f867c49c0a3ce2c2436ab99d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf948b1afc5647d9a5ffcdfd76ecd5ea",
            "max": 9674,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_adbbf4dd24fa4f8e9b8d48b0e087e46b",
            "value": 9674
          }
        },
        "3e25abec8dc5475c871ef793aff75e0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d448a1d703b74d699b6b9ebab8309c8b",
            "placeholder": "​",
            "style": "IPY_MODEL_50c083cd9ef947e29468fca1cbe819b5",
            "value": " 9674/9674 [00:00&lt;00:00, 16441.33 examples/s]"
          }
        },
        "e6e03eb8778549abb7a4577a3ef29461": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c13e7b0f5fa3478c92f4a6a900fa6497": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1326460e83bd4c75b4628503c86354c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf948b1afc5647d9a5ffcdfd76ecd5ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adbbf4dd24fa4f8e9b8d48b0e087e46b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d448a1d703b74d699b6b9ebab8309c8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50c083cd9ef947e29468fca1cbe819b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07d983bd2c1f42829bda3c8265aab13c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09828f5e1af84b0bab05bdd4f4680deb",
              "IPY_MODEL_943c9b60b6bc40d4aea94bd98feec2da",
              "IPY_MODEL_8f8971a06a6e4afa9b36399e3ce81a56"
            ],
            "layout": "IPY_MODEL_d52c5dce79ef4c488d1fbeea18f1e427"
          }
        },
        "09828f5e1af84b0bab05bdd4f4680deb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_832e1feeabac451fa302554cd73c134c",
            "placeholder": "​",
            "style": "IPY_MODEL_e73578b2684e4296876d1d3797975ac8",
            "value": "Map:   0%"
          }
        },
        "943c9b60b6bc40d4aea94bd98feec2da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d923df2e04324b6db4e2b0577b89722c",
            "max": 9674,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64ba62bab4644bcb9ec52fc161cd81c3",
            "value": 0
          }
        },
        "8f8971a06a6e4afa9b36399e3ce81a56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21cd68aaa8734536a9c72316a65e3221",
            "placeholder": "​",
            "style": "IPY_MODEL_4d3bab8ae66246b1bfc6a201fb02c0da",
            "value": " 0/9674 [00:00&lt;?, ? examples/s]"
          }
        },
        "d52c5dce79ef4c488d1fbeea18f1e427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "832e1feeabac451fa302554cd73c134c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e73578b2684e4296876d1d3797975ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d923df2e04324b6db4e2b0577b89722c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64ba62bab4644bcb9ec52fc161cd81c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21cd68aaa8734536a9c72316a65e3221": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d3bab8ae66246b1bfc6a201fb02c0da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# `transformers` meets `bitsandbytes` for democratzing Large Language Models (LLMs) through 4bit quantization\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/huggingface/blog/blob/main/assets/96_hf_bitsandbytes_integration/Thumbnail_blue.png?raw=true\" alt=\"drawing\" width=\"700\" class=\"center\"/>\n",
        "</center>\n",
        "\n",
        "Welcome to this notebook that goes through the recent `bitsandbytes` integration that includes the work from XXX that introduces no performance degradation 4bit quantization techniques, for democratizing LLMs inference and training.\n",
        "\n",
        "In this notebook, we will learn together how to load a large model in 4bit (`gpt-neo-x-20b`) and train it using Google Colab and PEFT library from Hugging Face 🤗.\n",
        "\n",
        "[In the general usage notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing), you can learn how to propely load a model in 4bit with all its variants.\n",
        "\n",
        "If you liked the previous work for integrating [*LLM.int8*](https://arxiv.org/abs/2208.07339), you can have a look at the [introduction blogpost](https://huggingface.co/blog/hf-bitsandbytes-integration) to lean more about that quantization method.\n"
      ],
      "metadata": {
        "id": "XIyP_0r6zuVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall datasets -y\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "cheX4_C2xz2X",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50e5347a-8e3c-4f40-e522-50d64565bed8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping datasets as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 huggingface-hub-0.17.3 multiprocess-0.70.15 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlFxRGVGybOu",
        "outputId": "3d05010e-21a1-4fda-c568-06e79398f4b1",
        "execution": {
          "iopub.status.busy": "2023-10-04T13:05:30.808012Z",
          "iopub.execute_input": "2023-10-04T13:05:30.809285Z",
          "iopub.status.idle": "2023-10-04T13:05:41.968960Z",
          "shell.execute_reply.started": "2023-10-04T13:05:30.809252Z",
          "shell.execute_reply": "2023-10-04T13:05:41.967851Z"
        },
        "trusted": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers"
      ],
      "metadata": {
        "id": "hfgefLMDylI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76de7244-2769-4b99-c317-8b02b4026c2f",
        "execution": {
          "iopub.status.busy": "2023-10-04T13:05:41.970760Z",
          "iopub.execute_input": "2023-10-04T13:05:41.971164Z",
          "iopub.status.idle": "2023-10-04T13:05:59.275128Z",
          "shell.execute_reply.started": "2023-10-04T13:05:41.971128Z",
          "shell.execute_reply": "2023-10-04T13:05:59.273889Z"
        },
        "trusted": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "FuXIFTFapAMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d9f0bcd-7d55-45a2-8f66-cd270fb53021",
        "execution": {
          "iopub.status.busy": "2023-10-04T13:05:59.277565Z",
          "iopub.execute_input": "2023-10-04T13:05:59.277841Z",
          "iopub.status.idle": "2023-10-04T13:07:36.154522Z",
          "shell.execute_reply.started": "2023-10-04T13:05:59.277816Z",
          "shell.execute_reply": "2023-10-04T13:07:36.153231Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "ddJWcW-y3suD",
        "execution": {
          "iopub.status.busy": "2023-10-04T14:07:53.064814Z",
          "iopub.execute_input": "2023-10-04T14:07:53.065539Z",
          "iopub.status.idle": "2023-10-04T14:07:53.069858Z",
          "shell.execute_reply.started": "2023-10-04T14:07:53.065508Z",
          "shell.execute_reply": "2023-10-04T14:07:53.068866Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's load the model we are going to use - GPT-neo-x-20B! Note that the model itself is around 40GB in half precision"
      ],
      "metadata": {
        "id": "MJ-5idQwzvg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "id": "e4D5EfAWr7PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"microsoft/phi-1_5\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
      ],
      "metadata": {
        "id": "E0Nl5mWL0k2T",
        "execution": {
          "iopub.status.busy": "2023-10-04T13:07:37.293191Z",
          "iopub.execute_input": "2023-10-04T13:07:37.293706Z",
          "iopub.status.idle": "2023-10-04T13:58:27.142251Z",
          "shell.execute_reply.started": "2023-10-04T13:07:37.293674Z",
          "shell.execute_reply": "2023-10-04T13:58:27.141313Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
      ],
      "metadata": {
        "id": "Mp2gMi1ZzGET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metaphor Probabilities (zero-shot) for the pretrained model"
      ],
      "metadata": {
        "id": "9cx78s7wxz2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import pdb\n",
        "import pandas as pd\n",
        "import math\n",
        "from typing import List\n",
        "import random\n",
        "import argparse\n",
        "import torch\n",
        "\n",
        "\n",
        "def sent_scoring(model_tokenizer, text, cuda, score_type=\"loss\", output_attentions=False, length_normalize=False):\n",
        "    model = model_tokenizer[0]\n",
        "    tokenizer = model_tokenizer[1]\n",
        "    assert model is not None\n",
        "    assert tokenizer is not None\n",
        "    encoded_text = tokenizer.encode(text)\n",
        "    input_ids = torch.tensor(encoded_text).unsqueeze(0)\n",
        "    if cuda:\n",
        "        input_ids = input_ids.to('cuda')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids, output_attentions=output_attentions)\n",
        "    loss, logits = outputs[:2]\n",
        "\n",
        "    sentence_prob = loss.item()\n",
        "    if score_type == \"prob\":\n",
        "        if length_normalize:\n",
        "            mult = 2\n",
        "        else:\n",
        "            mult = len(encoded_text)\n",
        "\n",
        "        sentence_prob = math.exp(-1.0 * loss * (mult - 1))\n",
        "\n",
        "    if output_attentions:\n",
        "        attn = outputs[\"attentions\"]\n",
        "        return sentence_prob, attn, input_ids\n",
        "\n",
        "    return sentence_prob\n",
        "\n",
        "def confusion_matrix(P_forward_1, P_forward_2, P_backward_1, P_backward_2):\n",
        "    correct_forward = len(np.where(np.array(P_forward_1) >= 0.5)[0]) + len(np.where(np.array(P_forward_2) >=0.5)[0])\n",
        "    wrong_forward = len(P_forward_1) + len(P_forward_2) - correct_forward\n",
        "\n",
        "    correct_backward = len(np.where(np.array(P_backward_1) >= 0.5)[0]) + len(np.where(np.array(P_backward_2) >=0.5)[0])\n",
        "    wrong_backward = len(P_backward_1) + len(P_backward_2) - correct_backward\n",
        "\n",
        "    print(\"correct forward\", correct_forward, \"wrong forward\", wrong_forward, \"correct backward\", correct_backward, \"wrong_backward\", wrong_backward)\n",
        "\n",
        "    results = {\n",
        "        \"correct_forward\": correct_forward,\n",
        "        \"wrong_forward\": wrong_forward,\n",
        "        \"correct_backward\": correct_backward,\n",
        "        \"wrong_backward\": wrong_backward\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model(model, tokenizer, test_set, middle_phrase=\"\", use_prefix=0, verbose=True, score_type=\"prob\", use_cuda=False, return_acc=False, total = 1094) -> tuple:\n",
        "    preds = []\n",
        "    labels = []\n",
        "    x_1 = []\n",
        "    x_2 = []\n",
        "    y_1 = []\n",
        "    y_2 = []\n",
        "    P_x_1 = []\n",
        "    P_x_2 = []\n",
        "    P_y_1 = []\n",
        "    P_y_2 = []\n",
        "    P_x_1_y_1 = []\n",
        "    P_x_1_y_2 = []\n",
        "    P_x_2_y_1 = []\n",
        "    P_x_2_y_2 = []\n",
        "    P_x_1_correct = []\n",
        "    P_x_2_correct = []\n",
        "    P_y_1_correct = []\n",
        "    P_y_2_correct = []\n",
        "    correct = 0\n",
        "\n",
        "    for i, metaphor_data in tqdm(enumerate(test_set), total = total):\n",
        "        ctx, p1, p2 = metaphor_data[\"startphrase\"], metaphor_data[\"ending1\"], metaphor_data[\"ending2\"]\n",
        "        labels.append(int(metaphor_data[\"labels\"]))\n",
        "        if use_prefix > 0:\n",
        "            prefix_prompt = select_prefix_prompts(prompt_file, use_prefix) if use_prefix else \"\"\n",
        "        else:\n",
        "            prefix_prompt = \"\"\n",
        "\n",
        "        sent1 = prefix_prompt + ctx + \". \" + middle_phrase + p1 + \".\"\n",
        "        sent2 = prefix_prompt + ctx + \". \" + middle_phrase + p2 + \".\"\n",
        "\n",
        "        score1 = sent_scoring((model, tokenizer), sent1, use_cuda, score_type=score_type)\n",
        "        score2 = sent_scoring((model, tokenizer), sent2, use_cuda, score_type=score_type)\n",
        "\n",
        "        if score_type == \"loss\":\n",
        "            pred = 0 if score1 < score2 else 1\n",
        "        else:\n",
        "            pred = 1 if score1 < score2 else 0\n",
        "\n",
        "        pred_sent = sent1 if pred == 0 else sent2\n",
        "\n",
        "        if i % 2 == 0:\n",
        "            x_1.append(ctx)\n",
        "            x_1_score = sent_scoring((model, tokenizer), ctx + \".\", use_cuda, score_type=score_type)\n",
        "            P_x_1.append(x_1_score)\n",
        "            y_1.append(p1)\n",
        "            y_2.append(p2)\n",
        "            y1_score = sent_scoring((model, tokenizer), p1 + \".\", use_cuda, score_type=score_type)\n",
        "            y2_score = sent_scoring((model, tokenizer), p2 + \".\", use_cuda, score_type=score_type)\n",
        "            P_y_1.append(y1_score)\n",
        "            P_y_2.append(y2_score)\n",
        "\n",
        "            P_x_1_y_1.append(score1)\n",
        "            P_x_1_y_2.append(score2)\n",
        "            P_x_1_correct.append(score1/(score1 + score2))\n",
        "\n",
        "        else:\n",
        "            x_2.append(ctx)\n",
        "            x_2_score = sent_scoring((model, tokenizer), ctx + \".\", use_cuda, score_type=score_type)\n",
        "            P_x_2.append(x_2_score)\n",
        "            P_x_2_y_1.append(score1)\n",
        "            P_x_2_y_2.append(score2)\n",
        "            P_x_2_correct.append(score2/(score1 + score2))\n",
        "\n",
        "            P_y_1_correct.append(P_x_1_y_1[-1]/(P_x_1_y_1[-1] + score1))\n",
        "            P_y_2_correct.append(score2/(P_x_1_y_2[-1] + score2))\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Q: {ctx}: 1. {p1} 2. {p2}\")\n",
        "            print(f\"model says '{pred_sent}' is more likely\")\n",
        "            print(\"\\n\")\n",
        "        if pred == metaphor_data[\"labels\"]:\n",
        "            correct += 1\n",
        "        preds.append(pred)\n",
        "\n",
        "    cols = {\"x_1\": x_1, \"x_2\": x_2, \"y_1\": y_1, \"y_2\": y_2, \"P(x_1)\": P_x_1, \"P(x_2)\": P_x_2, \"P(y_1)\": P_y_1, \"P(y_2)\": P_y_2,\n",
        "        \"P(x_1, y_1)\": P_x_1_y_1, \"P(x_1, y_2)\": P_x_1_y_2, \"P(x_2, y_1)\": P_x_2_y_1, \"P(x_2, y_2)\": P_x_2_y_2,\n",
        "        \"P(y_1|x_1)\": P_x_1_correct, \"P(y_2|x_2)\": P_x_2_correct, \"P(x_1|y_1)\": P_y_1_correct, \"P(x_2|y_2)\": P_y_2_correct}\n",
        "    out_df = pd.DataFrame(cols)\n",
        "\n",
        "    if return_acc:\n",
        "        return correct/len(preds), out_df, preds, labels\n",
        "\n",
        "    return out_df, preds, labels\n",
        "\n",
        "def compute_stats(total_df: pd.DataFrame, all_preds: List, all_labels: List) -> None:\n",
        "    print(\"overall accuracy: \")\n",
        "    accuracyy = len(np.where(np.array(all_preds) == np.array(all_labels))[0])/len(all_labels)\n",
        "    print(accuracyy)\n",
        "    print(\"confusion matrix: \")\n",
        "    matrix_dic = confusion_matrix(list(total_df[\"P(y_1|x_1)\"]), list(total_df[\"P(y_2|x_2)\"]), list(total_df[\"P(x_1|y_1)\"]), list(total_df[\"P(x_2|y_2)\"]))\n",
        "\n",
        "    return accuracyy, matrix_dic\n"
      ],
      "metadata": {
        "id": "butPbYIbxz2a",
        "execution": {
          "iopub.status.busy": "2023-10-04T14:08:05.702293Z",
          "iopub.execute_input": "2023-10-04T14:08:05.703073Z",
          "iopub.status.idle": "2023-10-04T14:08:05.722730Z",
          "shell.execute_reply.started": "2023-10-04T14:08:05.703035Z",
          "shell.execute_reply": "2023-10-04T14:08:05.721774Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"nightingal3/fig-qa\")"
      ],
      "metadata": {
        "id": "-MD2kwlmxz2b",
        "execution": {
          "iopub.status.busy": "2023-10-04T14:08:08.814717Z",
          "iopub.execute_input": "2023-10-04T14:08:08.815066Z",
          "iopub.status.idle": "2023-10-04T14:08:10.018539Z",
          "shell.execute_reply.started": "2023-10-04T14:08:08.815040Z",
          "shell.execute_reply": "2023-10-04T14:08:10.017149Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['validation']"
      ],
      "metadata": {
        "id": "u-p0BAxiBCQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#subset_test_dataset = dataset['validation'].select(range(10))\n",
        "subset_test_dataset = dataset['validation']"
      ],
      "metadata": {
        "id": "UkmIOMfV3elH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_df, preds, labels = evaluate_model(model, tokenizer, subset_test_dataset, verbose = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAuyTVnr31VL",
        "outputId": "057336cb-0d3a-46a9-fa92-d20fa5f4179c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1094/1094 [09:55<00:00,  1.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_accuracy, conf_matrix_zero_shot =  compute_stats(out_df, preds, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV8w0TDc34fo",
        "outputId": "9d561724-5024-49c3-a402-4732c9747c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "overall accuracy: \n",
            "0.603290676416819\n",
            "confusion matrix: \n",
            "correct forward 660 wrong forward 434 correct backward 632 wrong_backward 462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "model_id_string = model_id.replace(\"/\", \"-\")\n",
        "\n",
        "# Saving DataFrame to CSV\n",
        "out_df.to_csv(f'output_df_{model_id_string}.csv', sep=\"\\t\", index=False)\n",
        "\n",
        "# Saving other data as JSON\n",
        "data_to_save = {\n",
        "    \"preds\": preds,\n",
        "    \"labels\": labels,\n",
        "    \"zero_shot_accuracy\": zero_shot_accuracy,\n",
        "    \"conf_matrix_zero_shot\": conf_matrix_zero_shot\n",
        "}\n",
        "\n",
        "with open(f'output_data_{model_id_string}.json', 'w') as file:\n",
        "    json.dump(data_to_save, file)\n",
        "\n",
        "import pickle\n",
        "\n",
        "data_to_save_pick = {\n",
        "    \"out_df\": out_df,\n",
        "    \"preds\": preds,\n",
        "    \"labels\": labels,\n",
        "    \"zero_shot_accuracy\": zero_shot_accuracy,\n",
        "    \"conf_matrix_zero_shot\": conf_matrix_zero_shot\n",
        "}\n",
        "\n",
        "with open(f'pickle_output_data_{model_id_string}.pkl', 'wb') as file:\n",
        "    pickle.dump(data_to_save_pick, file)\n",
        "\n"
      ],
      "metadata": {
        "id": "f-Oxn_ffXelA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning"
      ],
      "metadata": {
        "id": "d4uwXib2xz2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "a9EUEDAl0ss3",
        "execution": {
          "iopub.status.busy": "2023-10-01T14:53:00.363074Z",
          "iopub.status.idle": "2023-10-01T14:53:00.363751Z",
          "shell.execute_reply.started": "2023-10-01T14:53:00.363535Z",
          "shell.execute_reply": "2023-10-01T14:53:00.363557Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "4cad7803-46e9-4fc4-e0b2-a00f761af4a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d6b5f42e99b2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprepare_model_for_kbit_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing_enable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_model_for_kbit_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mgradient_checkpointing_enable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_gradient_checkpointing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1829\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__} does not support gradient checkpointing.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1830\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_gradient_checkpointing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_hf_peft_config_loaded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1615\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MixFormerSequentialForCausalLM' object has no attribute '_set_gradient_checkpointing'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_modules(model, prefix=''):\n",
        "    for name, module in model.named_children():\n",
        "        full_name = f\"{prefix}.{name}\" if prefix else name\n",
        "        print(full_name)\n",
        "        print_modules(module, full_name)\n",
        "\n",
        "print_modules(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S0tJHkaDEde",
        "outputId": "6c731ced-2ea4-4917-ab63-32058adfee1b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers\n",
            "layers.0\n",
            "layers.0.wte\n",
            "layers.0.drop\n",
            "layers.1\n",
            "layers.1.ln\n",
            "layers.1.resid_dropout\n",
            "layers.1.mixer\n",
            "layers.1.mixer.rotary_emb\n",
            "layers.1.mixer.Wqkv\n",
            "layers.1.mixer.out_proj\n",
            "layers.1.mixer.inner_attn\n",
            "layers.1.mixer.inner_attn.drop\n",
            "layers.1.mixer.inner_cross_attn\n",
            "layers.1.mixer.inner_cross_attn.drop\n",
            "layers.1.mlp\n",
            "layers.1.mlp.fc1\n",
            "layers.1.mlp.fc2\n",
            "layers.1.mlp.act\n",
            "layers.2\n",
            "layers.2.ln\n",
            "layers.2.resid_dropout\n",
            "layers.2.mixer\n",
            "layers.2.mixer.rotary_emb\n",
            "layers.2.mixer.Wqkv\n",
            "layers.2.mixer.out_proj\n",
            "layers.2.mixer.inner_attn\n",
            "layers.2.mixer.inner_attn.drop\n",
            "layers.2.mixer.inner_cross_attn\n",
            "layers.2.mixer.inner_cross_attn.drop\n",
            "layers.2.mlp\n",
            "layers.2.mlp.fc1\n",
            "layers.2.mlp.fc2\n",
            "layers.2.mlp.act\n",
            "layers.3\n",
            "layers.3.ln\n",
            "layers.3.resid_dropout\n",
            "layers.3.mixer\n",
            "layers.3.mixer.rotary_emb\n",
            "layers.3.mixer.Wqkv\n",
            "layers.3.mixer.out_proj\n",
            "layers.3.mixer.inner_attn\n",
            "layers.3.mixer.inner_attn.drop\n",
            "layers.3.mixer.inner_cross_attn\n",
            "layers.3.mixer.inner_cross_attn.drop\n",
            "layers.3.mlp\n",
            "layers.3.mlp.fc1\n",
            "layers.3.mlp.fc2\n",
            "layers.3.mlp.act\n",
            "layers.4\n",
            "layers.4.ln\n",
            "layers.4.resid_dropout\n",
            "layers.4.mixer\n",
            "layers.4.mixer.rotary_emb\n",
            "layers.4.mixer.Wqkv\n",
            "layers.4.mixer.out_proj\n",
            "layers.4.mixer.inner_attn\n",
            "layers.4.mixer.inner_attn.drop\n",
            "layers.4.mixer.inner_cross_attn\n",
            "layers.4.mixer.inner_cross_attn.drop\n",
            "layers.4.mlp\n",
            "layers.4.mlp.fc1\n",
            "layers.4.mlp.fc2\n",
            "layers.4.mlp.act\n",
            "layers.5\n",
            "layers.5.ln\n",
            "layers.5.resid_dropout\n",
            "layers.5.mixer\n",
            "layers.5.mixer.rotary_emb\n",
            "layers.5.mixer.Wqkv\n",
            "layers.5.mixer.out_proj\n",
            "layers.5.mixer.inner_attn\n",
            "layers.5.mixer.inner_attn.drop\n",
            "layers.5.mixer.inner_cross_attn\n",
            "layers.5.mixer.inner_cross_attn.drop\n",
            "layers.5.mlp\n",
            "layers.5.mlp.fc1\n",
            "layers.5.mlp.fc2\n",
            "layers.5.mlp.act\n",
            "layers.6\n",
            "layers.6.ln\n",
            "layers.6.resid_dropout\n",
            "layers.6.mixer\n",
            "layers.6.mixer.rotary_emb\n",
            "layers.6.mixer.Wqkv\n",
            "layers.6.mixer.out_proj\n",
            "layers.6.mixer.inner_attn\n",
            "layers.6.mixer.inner_attn.drop\n",
            "layers.6.mixer.inner_cross_attn\n",
            "layers.6.mixer.inner_cross_attn.drop\n",
            "layers.6.mlp\n",
            "layers.6.mlp.fc1\n",
            "layers.6.mlp.fc2\n",
            "layers.6.mlp.act\n",
            "layers.7\n",
            "layers.7.ln\n",
            "layers.7.resid_dropout\n",
            "layers.7.mixer\n",
            "layers.7.mixer.rotary_emb\n",
            "layers.7.mixer.Wqkv\n",
            "layers.7.mixer.out_proj\n",
            "layers.7.mixer.inner_attn\n",
            "layers.7.mixer.inner_attn.drop\n",
            "layers.7.mixer.inner_cross_attn\n",
            "layers.7.mixer.inner_cross_attn.drop\n",
            "layers.7.mlp\n",
            "layers.7.mlp.fc1\n",
            "layers.7.mlp.fc2\n",
            "layers.7.mlp.act\n",
            "layers.8\n",
            "layers.8.ln\n",
            "layers.8.resid_dropout\n",
            "layers.8.mixer\n",
            "layers.8.mixer.rotary_emb\n",
            "layers.8.mixer.Wqkv\n",
            "layers.8.mixer.out_proj\n",
            "layers.8.mixer.inner_attn\n",
            "layers.8.mixer.inner_attn.drop\n",
            "layers.8.mixer.inner_cross_attn\n",
            "layers.8.mixer.inner_cross_attn.drop\n",
            "layers.8.mlp\n",
            "layers.8.mlp.fc1\n",
            "layers.8.mlp.fc2\n",
            "layers.8.mlp.act\n",
            "layers.9\n",
            "layers.9.ln\n",
            "layers.9.resid_dropout\n",
            "layers.9.mixer\n",
            "layers.9.mixer.rotary_emb\n",
            "layers.9.mixer.Wqkv\n",
            "layers.9.mixer.out_proj\n",
            "layers.9.mixer.inner_attn\n",
            "layers.9.mixer.inner_attn.drop\n",
            "layers.9.mixer.inner_cross_attn\n",
            "layers.9.mixer.inner_cross_attn.drop\n",
            "layers.9.mlp\n",
            "layers.9.mlp.fc1\n",
            "layers.9.mlp.fc2\n",
            "layers.9.mlp.act\n",
            "layers.10\n",
            "layers.10.ln\n",
            "layers.10.resid_dropout\n",
            "layers.10.mixer\n",
            "layers.10.mixer.rotary_emb\n",
            "layers.10.mixer.Wqkv\n",
            "layers.10.mixer.out_proj\n",
            "layers.10.mixer.inner_attn\n",
            "layers.10.mixer.inner_attn.drop\n",
            "layers.10.mixer.inner_cross_attn\n",
            "layers.10.mixer.inner_cross_attn.drop\n",
            "layers.10.mlp\n",
            "layers.10.mlp.fc1\n",
            "layers.10.mlp.fc2\n",
            "layers.10.mlp.act\n",
            "layers.11\n",
            "layers.11.ln\n",
            "layers.11.resid_dropout\n",
            "layers.11.mixer\n",
            "layers.11.mixer.rotary_emb\n",
            "layers.11.mixer.Wqkv\n",
            "layers.11.mixer.out_proj\n",
            "layers.11.mixer.inner_attn\n",
            "layers.11.mixer.inner_attn.drop\n",
            "layers.11.mixer.inner_cross_attn\n",
            "layers.11.mixer.inner_cross_attn.drop\n",
            "layers.11.mlp\n",
            "layers.11.mlp.fc1\n",
            "layers.11.mlp.fc2\n",
            "layers.11.mlp.act\n",
            "layers.12\n",
            "layers.12.ln\n",
            "layers.12.resid_dropout\n",
            "layers.12.mixer\n",
            "layers.12.mixer.rotary_emb\n",
            "layers.12.mixer.Wqkv\n",
            "layers.12.mixer.out_proj\n",
            "layers.12.mixer.inner_attn\n",
            "layers.12.mixer.inner_attn.drop\n",
            "layers.12.mixer.inner_cross_attn\n",
            "layers.12.mixer.inner_cross_attn.drop\n",
            "layers.12.mlp\n",
            "layers.12.mlp.fc1\n",
            "layers.12.mlp.fc2\n",
            "layers.12.mlp.act\n",
            "layers.13\n",
            "layers.13.ln\n",
            "layers.13.resid_dropout\n",
            "layers.13.mixer\n",
            "layers.13.mixer.rotary_emb\n",
            "layers.13.mixer.Wqkv\n",
            "layers.13.mixer.out_proj\n",
            "layers.13.mixer.inner_attn\n",
            "layers.13.mixer.inner_attn.drop\n",
            "layers.13.mixer.inner_cross_attn\n",
            "layers.13.mixer.inner_cross_attn.drop\n",
            "layers.13.mlp\n",
            "layers.13.mlp.fc1\n",
            "layers.13.mlp.fc2\n",
            "layers.13.mlp.act\n",
            "layers.14\n",
            "layers.14.ln\n",
            "layers.14.resid_dropout\n",
            "layers.14.mixer\n",
            "layers.14.mixer.rotary_emb\n",
            "layers.14.mixer.Wqkv\n",
            "layers.14.mixer.out_proj\n",
            "layers.14.mixer.inner_attn\n",
            "layers.14.mixer.inner_attn.drop\n",
            "layers.14.mixer.inner_cross_attn\n",
            "layers.14.mixer.inner_cross_attn.drop\n",
            "layers.14.mlp\n",
            "layers.14.mlp.fc1\n",
            "layers.14.mlp.fc2\n",
            "layers.14.mlp.act\n",
            "layers.15\n",
            "layers.15.ln\n",
            "layers.15.resid_dropout\n",
            "layers.15.mixer\n",
            "layers.15.mixer.rotary_emb\n",
            "layers.15.mixer.Wqkv\n",
            "layers.15.mixer.out_proj\n",
            "layers.15.mixer.inner_attn\n",
            "layers.15.mixer.inner_attn.drop\n",
            "layers.15.mixer.inner_cross_attn\n",
            "layers.15.mixer.inner_cross_attn.drop\n",
            "layers.15.mlp\n",
            "layers.15.mlp.fc1\n",
            "layers.15.mlp.fc2\n",
            "layers.15.mlp.act\n",
            "layers.16\n",
            "layers.16.ln\n",
            "layers.16.resid_dropout\n",
            "layers.16.mixer\n",
            "layers.16.mixer.rotary_emb\n",
            "layers.16.mixer.Wqkv\n",
            "layers.16.mixer.out_proj\n",
            "layers.16.mixer.inner_attn\n",
            "layers.16.mixer.inner_attn.drop\n",
            "layers.16.mixer.inner_cross_attn\n",
            "layers.16.mixer.inner_cross_attn.drop\n",
            "layers.16.mlp\n",
            "layers.16.mlp.fc1\n",
            "layers.16.mlp.fc2\n",
            "layers.16.mlp.act\n",
            "layers.17\n",
            "layers.17.ln\n",
            "layers.17.resid_dropout\n",
            "layers.17.mixer\n",
            "layers.17.mixer.rotary_emb\n",
            "layers.17.mixer.Wqkv\n",
            "layers.17.mixer.out_proj\n",
            "layers.17.mixer.inner_attn\n",
            "layers.17.mixer.inner_attn.drop\n",
            "layers.17.mixer.inner_cross_attn\n",
            "layers.17.mixer.inner_cross_attn.drop\n",
            "layers.17.mlp\n",
            "layers.17.mlp.fc1\n",
            "layers.17.mlp.fc2\n",
            "layers.17.mlp.act\n",
            "layers.18\n",
            "layers.18.ln\n",
            "layers.18.resid_dropout\n",
            "layers.18.mixer\n",
            "layers.18.mixer.rotary_emb\n",
            "layers.18.mixer.Wqkv\n",
            "layers.18.mixer.out_proj\n",
            "layers.18.mixer.inner_attn\n",
            "layers.18.mixer.inner_attn.drop\n",
            "layers.18.mixer.inner_cross_attn\n",
            "layers.18.mixer.inner_cross_attn.drop\n",
            "layers.18.mlp\n",
            "layers.18.mlp.fc1\n",
            "layers.18.mlp.fc2\n",
            "layers.18.mlp.act\n",
            "layers.19\n",
            "layers.19.ln\n",
            "layers.19.resid_dropout\n",
            "layers.19.mixer\n",
            "layers.19.mixer.rotary_emb\n",
            "layers.19.mixer.Wqkv\n",
            "layers.19.mixer.out_proj\n",
            "layers.19.mixer.inner_attn\n",
            "layers.19.mixer.inner_attn.drop\n",
            "layers.19.mixer.inner_cross_attn\n",
            "layers.19.mixer.inner_cross_attn.drop\n",
            "layers.19.mlp\n",
            "layers.19.mlp.fc1\n",
            "layers.19.mlp.fc2\n",
            "layers.19.mlp.act\n",
            "layers.20\n",
            "layers.20.ln\n",
            "layers.20.resid_dropout\n",
            "layers.20.mixer\n",
            "layers.20.mixer.rotary_emb\n",
            "layers.20.mixer.Wqkv\n",
            "layers.20.mixer.out_proj\n",
            "layers.20.mixer.inner_attn\n",
            "layers.20.mixer.inner_attn.drop\n",
            "layers.20.mixer.inner_cross_attn\n",
            "layers.20.mixer.inner_cross_attn.drop\n",
            "layers.20.mlp\n",
            "layers.20.mlp.fc1\n",
            "layers.20.mlp.fc2\n",
            "layers.20.mlp.act\n",
            "layers.21\n",
            "layers.21.ln\n",
            "layers.21.resid_dropout\n",
            "layers.21.mixer\n",
            "layers.21.mixer.rotary_emb\n",
            "layers.21.mixer.Wqkv\n",
            "layers.21.mixer.out_proj\n",
            "layers.21.mixer.inner_attn\n",
            "layers.21.mixer.inner_attn.drop\n",
            "layers.21.mixer.inner_cross_attn\n",
            "layers.21.mixer.inner_cross_attn.drop\n",
            "layers.21.mlp\n",
            "layers.21.mlp.fc1\n",
            "layers.21.mlp.fc2\n",
            "layers.21.mlp.act\n",
            "layers.22\n",
            "layers.22.ln\n",
            "layers.22.resid_dropout\n",
            "layers.22.mixer\n",
            "layers.22.mixer.rotary_emb\n",
            "layers.22.mixer.Wqkv\n",
            "layers.22.mixer.out_proj\n",
            "layers.22.mixer.inner_attn\n",
            "layers.22.mixer.inner_attn.drop\n",
            "layers.22.mixer.inner_cross_attn\n",
            "layers.22.mixer.inner_cross_attn.drop\n",
            "layers.22.mlp\n",
            "layers.22.mlp.fc1\n",
            "layers.22.mlp.fc2\n",
            "layers.22.mlp.act\n",
            "layers.23\n",
            "layers.23.ln\n",
            "layers.23.resid_dropout\n",
            "layers.23.mixer\n",
            "layers.23.mixer.rotary_emb\n",
            "layers.23.mixer.Wqkv\n",
            "layers.23.mixer.out_proj\n",
            "layers.23.mixer.inner_attn\n",
            "layers.23.mixer.inner_attn.drop\n",
            "layers.23.mixer.inner_cross_attn\n",
            "layers.23.mixer.inner_cross_attn.drop\n",
            "layers.23.mlp\n",
            "layers.23.mlp.fc1\n",
            "layers.23.mlp.fc2\n",
            "layers.23.mlp.act\n",
            "layers.24\n",
            "layers.24.ln\n",
            "layers.24.resid_dropout\n",
            "layers.24.mixer\n",
            "layers.24.mixer.rotary_emb\n",
            "layers.24.mixer.Wqkv\n",
            "layers.24.mixer.out_proj\n",
            "layers.24.mixer.inner_attn\n",
            "layers.24.mixer.inner_attn.drop\n",
            "layers.24.mixer.inner_cross_attn\n",
            "layers.24.mixer.inner_cross_attn.drop\n",
            "layers.24.mlp\n",
            "layers.24.mlp.fc1\n",
            "layers.24.mlp.fc2\n",
            "layers.24.mlp.act\n",
            "layers.25\n",
            "layers.25.ln\n",
            "layers.25.linear\n",
            "loss\n",
            "loss.loss_fct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "gkIcwsSU01EB",
        "execution": {
          "iopub.status.busy": "2023-10-01T14:53:00.364992Z",
          "iopub.status.idle": "2023-10-01T14:53:00.365686Z",
          "shell.execute_reply.started": "2023-10-01T14:53:00.365471Z",
          "shell.execute_reply": "2023-10-01T14:53:00.365492Z"
        },
        "trusted": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    #target_modules=[\"query_key_value\"],\n",
        "    target_modules=[\"Wqkv\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "Ybeyl20n3dYH",
        "execution": {
          "iopub.status.busy": "2023-10-01T14:53:00.366914Z",
          "iopub.status.idle": "2023-10-01T14:53:00.367595Z",
          "shell.execute_reply.started": "2023-10-01T14:53:00.367381Z",
          "shell.execute_reply": "2023-10-01T14:53:00.367402Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40faf64d-694e-40e5-b751-ac4c642b9d66"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1572864 || all params: 815863808 || trainable%: 0.1927851173905731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load a common dataset, english quotes, to fine tune our model on famous quotes."
      ],
      "metadata": {
        "id": "FCc64bfnmd3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"nightingal3/fig-qa\")\n",
        "data = data.map(lambda samples: tokenizer(samples[\"startphrase\"]), batched=True)"
      ],
      "metadata": {
        "id": "s6f4z8EYmcJ6",
        "execution": {
          "iopub.status.busy": "2023-10-01T14:53:00.368844Z",
          "iopub.status.idle": "2023-10-01T14:53:00.369555Z",
          "shell.execute_reply.started": "2023-10-01T14:53:00.369342Z",
          "shell.execute_reply": "2023-10-01T14:53:00.369364Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "25011f7ad8b04654bdb5187567eac231",
            "64810c7942694ade85fe707d5ff48570",
            "ffa9474f867c49c0a3ce2c2436ab99d2",
            "3e25abec8dc5475c871ef793aff75e0c",
            "e6e03eb8778549abb7a4577a3ef29461",
            "c13e7b0f5fa3478c92f4a6a900fa6497",
            "1326460e83bd4c75b4628503c86354c8",
            "bf948b1afc5647d9a5ffcdfd76ecd5ea",
            "adbbf4dd24fa4f8e9b8d48b0e087e46b",
            "d448a1d703b74d699b6b9ebab8309c8b",
            "50c083cd9ef947e29468fca1cbe819b5"
          ]
        },
        "outputId": "af97daa9-1671-42f7-b5e8-98cd16b7c892"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/9674 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25011f7ad8b04654bdb5187567eac231"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIdI9fJjuOZ5",
        "outputId": "5731b788-2ec9-4854-9070-e246308cc816"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['startphrase', 'ending1', 'ending2', 'labels', 'valid', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 9674\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['startphrase', 'ending1', 'ending2', 'labels', 'valid', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1094\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['startphrase', 'ending1', 'ending2', 'labels', 'valid', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1146\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def map_concatenation_and_tokenization(samples):\n",
        "    concatenated_phrases = []\n",
        "    input_ids_list = []\n",
        "    attention_mask_list = []\n",
        "\n",
        "    for i in range(len(samples['startphrase'])):\n",
        "        # Choose the ending based on the labels value for each sample in the batch\n",
        "        ending = samples['ending1'][i] if samples['labels'][i] == 0 else samples['ending2'][i]\n",
        "        concatenated_phrase = samples['startphrase'][i] + ' ' + ending\n",
        "        concatenated_phrases.append(concatenated_phrase)\n",
        "\n",
        "        # Tokenize the concatenated_phrase\n",
        "        tokens = tokenizer(concatenated_phrase, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
        "        input_ids_list.append(tokens['input_ids'][0].tolist())\n",
        "        attention_mask_list.append(tokens['attention_mask'][0].tolist())\n",
        "\n",
        "    return {\n",
        "        'concatenated_phrase': concatenated_phrases,\n",
        "        'input_ids': input_ids_list,\n",
        "        'attention_mask': attention_mask_list\n",
        "    }\n",
        "\n",
        "# Apply the mapping function\n",
        "data = dataset.map(map_concatenation_and_tokenization, batched=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467,
          "referenced_widgets": [
            "07d983bd2c1f42829bda3c8265aab13c",
            "09828f5e1af84b0bab05bdd4f4680deb",
            "943c9b60b6bc40d4aea94bd98feec2da",
            "8f8971a06a6e4afa9b36399e3ce81a56",
            "d52c5dce79ef4c488d1fbeea18f1e427",
            "832e1feeabac451fa302554cd73c134c",
            "e73578b2684e4296876d1d3797975ac8",
            "d923df2e04324b6db4e2b0577b89722c",
            "64ba62bab4644bcb9ec52fc161cd81c3",
            "21cd68aaa8734536a9c72316a65e3221",
            "4d3bab8ae66246b1bfc6a201fb02c0da"
          ]
        },
        "id": "jBjU0fXKtEDi",
        "outputId": "d2435fc7-c5eb-48a9-c61a-afafafaf4a19"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/9674 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07d983bd2c1f42829bda3c8265aab13c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-3c6020edf8cc>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Apply the mapping function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_concatenation_and_tokenization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0mcache_file_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         return DatasetDict(\n\u001b[0;32m--> 853\u001b[0;31m             {\n\u001b[0m\u001b[1;32m    854\u001b[0m                 k: dataset.map(\n\u001b[1;32m    855\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    852\u001b[0m         return DatasetDict(\n\u001b[1;32m    853\u001b[0m             {\n\u001b[0;32m--> 854\u001b[0;31m                 k: dataset.map(\n\u001b[0m\u001b[1;32m    855\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m                     \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m         }\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3095\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3096\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 3097\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3098\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3099\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3472\u001b[0m                         )  # Something simpler?\n\u001b[1;32m   3473\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3474\u001b[0;31m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[1;32m   3475\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3476\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3351\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3352\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3353\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3355\u001b[0m                 processed_inputs = {\n",
            "\u001b[0;32m<ipython-input-63-3c6020edf8cc>\u001b[0m in \u001b[0;36mmap_concatenation_and_tokenization\u001b[0;34m(samples)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Tokenize the concatenated_phrase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcatenated_phrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0minput_ids_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mattention_mask_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2814\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2815\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2816\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2817\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2818\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2920\u001b[0m             )\n\u001b[1;32m   2921\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2922\u001b[0;31m             return self.encode_plus(\n\u001b[0m\u001b[1;32m   2923\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2924\u001b[0m                 \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m         \u001b[0;31m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2986\u001b[0;31m         padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n\u001b[0m\u001b[1;32m   2987\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m             \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m         \u001b[0;31m# Test if we have a padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpadding_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDO_NOT_PAD\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2721\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2722\u001b[0m                 \u001b[0;34m\"Asking to pad but the tokenizer does not have a padding token. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m                 \u001b[0;34m\"Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhSQLN0humoO",
        "outputId": "700a39dd-fda0-4562-f9a2-56f65f8fd18b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['startphrase', 'ending1', 'ending2', 'labels', 'valid', 'concatenated_phrase', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 9674\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['startphrase', 'ending1', 'ending2', 'labels', 'valid', 'concatenated_phrase', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1094\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['startphrase', 'ending1', 'ending2', 'labels', 'valid', 'concatenated_phrase', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1146\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset_test_dataset[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkBawcqSt7XO",
        "outputId": "d754f20b-897b-4168-eda8-84969800632c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'startphrase': 'The girl had the flightiness of a rock',\n",
              " 'ending1': 'The girl was very fickle.',\n",
              " 'ending2': 'The girl was very stable.',\n",
              " 'labels': 1,\n",
              " 'valid': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['validation'][1]"
      ],
      "metadata": {
        "id": "fwg6LQZhtrn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below to run the training! For the sake of the demo, we just ran it for few steps just to showcase how to use this integration with existing tools on the HF ecosystem."
      ],
      "metadata": {
        "id": "_0MOtwf3zdZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# needed for gpt-neo-x tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=2,\n",
        "        max_steps=20,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "jq0nX33BmfaC",
        "execution": {
          "iopub.status.busy": "2023-10-01T14:53:00.371175Z",
          "iopub.status.idle": "2023-10-01T14:53:00.371903Z",
          "shell.execute_reply.started": "2023-10-01T14:53:00.371675Z",
          "shell.execute_reply": "2023-10-01T14:53:00.371696Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "d13b7260-483f-4a15-af93-9e7383ce531b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`attention_mask` is not supported during training. Using it might lead to unexpected results.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-ddceef7f1c5d>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# silence the warnings. Please re-enable for inference!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1504\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1507\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1801\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1981\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1983\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1984\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(\"outputs\")"
      ],
      "metadata": {
        "id": "p66mZk1RAlOR",
        "execution": {
          "iopub.status.busy": "2023-10-01T14:53:00.373120Z",
          "iopub.status.idle": "2023-10-01T14:53:00.373808Z",
          "shell.execute_reply.started": "2023-10-01T14:53:00.373584Z",
          "shell.execute_reply": "2023-10-01T14:53:00.373605Z"
        },
        "trusted": true
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig.from_pretrained('outputs')\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "L2Hllu-bCuN6",
        "execution": {
          "iopub.status.busy": "2023-10-01T14:53:00.375019Z",
          "iopub.status.idle": "2023-10-01T14:53:00.375707Z",
          "shell.execute_reply.started": "2023-10-01T14:53:00.375489Z",
          "shell.execute_reply": "2023-10-01T14:53:00.375511Z"
        },
        "trusted": true
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"I'm selfish, impatient and a little insecure.\"\"\"\n",
        "\n",
        "device = \"cuda:0\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=150)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "T1TiIH6vAlr_",
        "execution": {
          "iopub.status.busy": "2023-10-01T14:53:00.376939Z",
          "iopub.status.idle": "2023-10-01T14:53:00.377628Z",
          "shell.execute_reply.started": "2023-10-01T14:53:00.377417Z",
          "shell.execute_reply": "2023-10-01T14:53:00.377438Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b37af0-a349-440f-a441-fdd013b6efe4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm selfish, impatient and a little insecure. I don't want to wait for anyone.\"\n",
            "\n",
            "\"Well, that's not a very good attitude,\" said Sarah. \"You should try to be more patient and understanding. It's not always easy, but it's worth it.\"\n",
            "\n",
            "\"I know, I know,\" said Emily. \"But it's hard when you're in a hurry or when you're feeling stressed.\"\n",
            "\n",
            "\"I understand,\" said Sarah. \"But maybe you could try to take a few deep breaths and calm down before you start. And maybe you could try to be more patient with yourself and with others.\"\n",
            "\n",
            "\"That's a good idea,\" said Emily. \"I'll try to remember that.\"\n",
            "\n",
            "As they continued to talk, they\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_df_finetuned, preds_finetuned, labels_finetuned = evaluate_model(model, tokenizer, subset_test_dataset, verbose = False)"
      ],
      "metadata": {
        "id": "kEESIVXyESi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "115712b3-de1b-416a-e62c-ccdd8857a50e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1094/1094 [08:10<00:00,  2.23it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_accuracy_finetuned, conf_matrix_zero_shot_finetuned =  compute_stats(out_df_finetuned, preds_finetuned, labels_finetuned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlJU5R24p9DM",
        "outputId": "b5b51801-1450-4352-83ce-418a79ae82c1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "overall accuracy: \n",
            "0.603290676416819\n",
            "confusion matrix: \n",
            "correct forward 660 wrong forward 434 correct backward 634 wrong_backward 460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "model_id_string = model_id.replace(\"/\", \"-\")\n",
        "\n",
        "# Saving DataFrame to CSV\n",
        "out_df_finetuned.to_csv(f'output_df_{model_id_string}_finetuned.csv', sep=\"\\t\", index=False)\n",
        "\n",
        "# Saving other data as JSON\n",
        "data_to_save = {\n",
        "    \"preds\": preds_finetuned,\n",
        "    \"labels\": labels_finetuned,\n",
        "    \"zero_shot_accuracy\": zero_shot_accuracy_finetuned,\n",
        "    \"conf_matrix_zero_shot\": conf_matrix_zero_shot_finetuned\n",
        "}\n",
        "\n",
        "with open(f'output_data_{model_id_string}_finetuned.json', 'w') as file:\n",
        "    json.dump(data_to_save, file)\n",
        "\n",
        "import pickle\n",
        "\n",
        "data_to_save_pick = {\n",
        "    \"out_df\": out_df_finetuned,\n",
        "    \"preds\": preds_finetuned,\n",
        "    \"labels\": labels_finetuned,\n",
        "    \"zero_shot_accuracy\": zero_shot_accuracy_finetuned,\n",
        "    \"conf_matrix_zero_shot\": conf_matrix_zero_shot_finetuned\n",
        "}\n",
        "\n",
        "with open(f'pickle_output_data_{model_id_string}_finetuned.pkl', 'wb') as file:\n",
        "    pickle.dump(data_to_save_pick, file)\n",
        "\n"
      ],
      "metadata": {
        "id": "0T8cL3YxqA6t"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}