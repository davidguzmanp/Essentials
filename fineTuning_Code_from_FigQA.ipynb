{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1348,"status":"ok","timestamp":1697037255211,"user":{"displayName":"gvvvbjj ffbkitb","userId":"08953530729188044176"},"user_tz":-120},"id":"pIG69wI1xTci","outputId":"a37fc116-77d1-4229-95f4-17df59197452"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'Fig-QA'...\n","remote: Enumerating objects: 639, done.\u001b[K\n","remote: Counting objects: 100% (208/208), done.\u001b[K\n","remote: Compressing objects: 100% (119/119), done.\u001b[K\n","remote: Total 639 (delta 130), reused 139 (delta 88), pack-reused 431\u001b[K\n","Receiving objects: 100% (639/639), 2.81 MiB | 14.55 MiB/s, done.\n","Resolving deltas: 100% (353/353), done.\n","/content/Fig-QA\n"]}],"source":["# Download\n","!git clone https://github.com/nightingal3/Fig-QA\n","%cd Fig-QA/"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47758,"status":"ok","timestamp":1697037302966,"user":{"displayName":"gvvvbjj ffbkitb","userId":"08953530729188044176"},"user_tz":-120},"id":"CmJt-X3y002o","outputId":"e3aaef04-8583-468b-bf8f-d9b234b4eabd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers[torch]\n","  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.4)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers[torch])\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers[torch])\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[torch])\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n","Collecting accelerate>=0.20.3 (from transformers[torch])\n","  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers[torch])\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (17.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, accelerate\n","Successfully installed accelerate-0.23.0 huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Collecting deepspeed\n","  Downloading deepspeed-0.11.1.tar.gz (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting hjson (from deepspeed)\n","  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ninja (from deepspeed)\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n","Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.10.13)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.0.1+cu118)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.66.1)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0->deepspeed) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.12.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->deepspeed) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->deepspeed) (17.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed) (1.3.0)\n","Building wheels for collected packages: deepspeed\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deepspeed: filename=deepspeed-0.11.1-py3-none-any.whl size=1047111 sha256=fd203f34061fcdc64c4508669140dcf52360ba8a4b4a63de0b0887d171c1ca47\n","  Stored in directory: /root/.cache/pip/wheels/fc/a5/a7/bc3e8d6eea7ae73f7f3ea95f4caaba73a2853d70d5e8a9301a\n","Successfully built deepspeed\n","Installing collected packages: ninja, hjson, deepspeed\n","Successfully installed deepspeed-0.11.1 hjson-3.1.0 ninja-1.11.1.1\n"]}],"source":["# install\n","%pip install transformers[torch]\n","%pip install accelerate -U\n","%pip install deepspeed"]},{"cell_type":"markdown","metadata":{"id":"hnl88B6s0WLq"},"source":["# Try Running Their Script\n","python3 src/models/train_lm_models.py {gpt2,gpt-neo-sm,gpt-neo-lg} \\\n","[--dont_train] \\\n","[--dont_eval] \\\n","[--train_path=TRAIN_PATH] \\\n","[--eval_path=EVAL_PATH] \\\n","[--seed=SEED] \\\n","[--cuda] \\\n","[--num_epochs=NUM_EPOCHS] \\\n","[--learning_rate=LR] \\\n","[--middle_phrase=SUFFIX_PROMPT] \\\n","[--prefix=N] \\\n","[--contrastive] \\\n","[--contrast_lambd=a] \\\n","[--log_history] \\\n","[--deepspeed] \\\n","[----out_path=PATH] \\\n","[----early_stopping]"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":398,"status":"ok","timestamp":1697036710855,"user":{"displayName":"gvvvbjj ffbkitb","userId":"08953530729188044176"},"user_tz":-120},"id":"5rC3hgik0Z4h","outputId":"78cb437f-9235-4a8f-e875-58fed086a902"},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: line 1: cd: Fig-QA: No such file or directory\n"]}],"source":["!python3 src/models/train_lm_models.py gpt2 --cuda"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1697036710856,"user":{"displayName":"gvvvbjj ffbkitb","userId":"08953530729188044176"},"user_tz":-120},"id":"E5YzyDt-0hU5","outputId":"552fe4f4-1859-48b9-8fd1-2540c0b6f159"},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: line 1: cd: Fig-QA: No such file or directory\n"]}],"source":["!python3 src/models/train_lm_models.py gpt-neo-sm --cuda"]},{"cell_type":"markdown","metadata":{"id":"yuZ70qniFC1F"},"source":["# Modifying code\n","\n","### Changes:\n","- modify model_init to use correct loader for model\n","- add model string to main function"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5900,"status":"ok","timestamp":1697037334331,"user":{"displayName":"gvvvbjj ffbkitb","userId":"08953530729188044176"},"user_tz":-120},"id":"TnKLGnfsSpyr"},"outputs":[],"source":["from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel, GPTNeoForCausalLM, AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","def model_init(model_string, cuda, output_attentions=False, fast=False):\n","    if model_string.startswith(\"gpt2\"):\n","        if fast:\n","            tokenizer = AutoTokenizer.from_pretrained(model_string)\n","            model = GPT2LMHeadModel.from_pretrained(model_string)\n","        else:\n","            tokenizer = GPT2Tokenizer.from_pretrained(model_string)\n","            model = GPT2LMHeadModel.from_pretrained(model_string)\n","    elif model_string.startswith(\"EleutherAI/gpt-neo\"):\n","        tokenizer = GPT2Tokenizer.from_pretrained(model_string, output_attentions=output_attentions)\n","        model = GPTNeoForCausalLM.from_pretrained(model_string, output_attentions=output_attentions)\n","    elif \"t5\" in model_string:\n","      tokenizer = AutoTokenizer.from_pretrained(model_string)\n","      model = AutoModelForSeq2SeqLM.from_pretrained(model_string)\n","    else:\n","        tokenizer = OpenAIGPTTokenizer.from_pretrained(model_string)\n","        model = OpenAIGPTLMHeadModel.from_pretrained(model_string)\n","    model.eval()\n","    if cuda:\n","        model.to('cuda')\n","    return model, tokenizer"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":6770,"status":"ok","timestamp":1697037344860,"user":{"displayName":"gvvvbjj ffbkitb","userId":"08953530729188044176"},"user_tz":-120},"id":"aYNe8zlfFGs0"},"outputs":[],"source":["import argparse\n","import logging\n","from typing import Optional\n","from glob import glob\n","from pathlib import Path\n","import os, sys\n","import torch\n","import numpy as np\n","import pandas as pd\n","import pickle\n","\n","import transformers\n","from transformers import (\n","    DataCollatorForLanguageModeling,\n","    LineByLineTextDataset,\n","    LineByLineWithRefDataset,\n","    PreTrainedTokenizer,\n","    TextDataset,\n","    Trainer,\n","    TrainingArguments,\n","    set_seed,\n","    GPT2LMHeadModel,\n","    GPTNeoForCausalLM,\n","    EarlyStoppingCallback\n",")\n","from torch.utils.data import ConcatDataset\n","import pdb\n","\n","# Add path for those local py modules\n","sys.path.append('src/models/')\n","from gpt_score import evaluate_model\n","\n","logger = logging.getLogger(__name__)\n","\n","def main(model_name: str, prompt: str, train_path: str, eval_path: str, contrastive_train: bool, contrastive_train_lambd: float, num_epochs: int, seed: int, lr: int, use_cuda: bool, dont_train: bool, dont_eval: bool, out_path: str, cache_dir: str = \"./lm_train_cache/\", prefix_prompt: int = 0, batch_size: int = 8, log_history: bool = False, deepspeed: bool = False, early_stopping: bool = False) -> None:\n","    # Set up models, random seed, and logging\n","    model_names = {\n","        \"gpt2\": \"gpt2\",\n","        \"gpt-neo-sm\": \"EleutherAI/gpt-neo-1.3B\",\n","        \"gpt-neo-lg\": \"EleutherAI/gpt-neo-2.7B\",\n","        # new update\n","        \"gpt-neo-sssm\": \"EleutherAI/gpt-neo-125m\",\n","        \"flan-t5-base\":\"google/flan-t5-base\"\n","    }\n","    model_id = model_names[model_name]\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO,\n","    )\n","    transformers.utils.logging.set_verbosity_info()\n","    transformers.utils.logging.enable_default_handler()\n","    transformers.utils.logging.enable_explicit_format()\n","    logger.info(\"Training/evaluation parameters %s\", {\"model\": model_name, \"train path\": train_path, \"num epochs\": num_epochs, \"seed\": seed, \"cuda\": use_cuda, \"cache dir\": cache_dir, \"deepspeed\": deepspeed, \"early stopping\": early_stopping})\n","\n","\n","    if deepspeed and not use_cuda:\n","        logger.info(\"You must have GPUs to use deepspeed. Turning cuda flag on...\")\n","        use_cuda = True\n","\n","    model, tokenizer = model_init(model_id, use_cuda, fast=True)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    #model.resize_token_embeddings(len(tokenizer))\n","    set_seed(seed)\n","\n","    # load datasets and initialize trainer\n","    train_dataset = (\n","        get_dataset(train_path, tokenizer=tokenizer, cache_dir=cache_dir)\n","    )\n","    eval_dataset = (\n","        get_dataset(eval_path, tokenizer=tokenizer, cache_dir=cache_dir)\n","    )\n","\n","    eval_df = pd.read_csv(\"./data/filtered/dev.csv\")\n","    eval_df[\"label\"] = eval_df[\"labels\"]\n","    test_df = pd.read_csv(\"./data/filtered/dev.csv\")\n","    test_df[\"label\"] = test_df[\"labels\"]\n","\n","    data_collator = DataCollatorForLanguageModeling(\n","                tokenizer=tokenizer, mlm=False\n","            )\n","    no_cuda = not use_cuda\n","\n","    default_arguments = {\n","        \"output_dir\": f\"./lm_train_outputs/{model_name}_{seed}/\",\n","        \"do_train\": True,\n","        \"prediction_loss_only\": False,\n","        \"num_train_epochs\": num_epochs,\n","        \"seed\": seed,\n","        \"learning_rate\": lr,\n","        \"per_device_train_batch_size\": batch_size,\n","        \"per_device_eval_batch_size\": batch_size,\n","        \"no_cuda\": no_cuda\n","    }\n","\n","    if deepspeed:\n","        default_arguments[\"deepspeed\"] = \"deepspeed_config.json\"\n","    if not contrastive_train:\n","        default_arguments[\"per_device_train_batch_size\"] = batch_size\n","        default_arguments[\"per_device_eval_batch_size\"] = batch_size\n","\n","    else:\n","        default_arguments[\"per_device_train_batch_size\"] = 2\n","\n","    if log_history:\n","        default_arguments[\"evaluation_strategy\"] = \"steps\"\n","        default_arguments[\"eval_steps\"] = 100\n","    if early_stopping:\n","        default_arguments[\"evaluation_strategy\"] = \"epoch\"\n","        default_arguments[\"load_best_model_at_end\"] = True\n","        default_arguments[\"metric_for_best_model\"] = \"eval_loss\"\n","        default_arguments[\"save_strategy\"] = \"epoch\"\n","\n","    training_args = transformers.TrainingArguments(**default_arguments)\n","\n","    if early_stopping:\n","        trainer = Trainer(\n","            args=training_args,\n","            model=model,\n","            data_collator=data_collator,\n","            train_dataset=train_dataset,\n","            eval_dataset=eval_dataset,\n","            callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n","        )\n","    elif not contrastive_train:\n","        #tokenizer.pad_token = tokenizer.eos_token\n","        #dummy_init = make_dummy(model_id)\n","        trainer = Trainer(\n","            args=training_args,\n","            model=model,\n","            data_collator=data_collator,\n","            train_dataset=train_dataset,\n","            eval_dataset=eval_dataset,\n","            #model_init=dummy_init,\n","            compute_metrics=compute_metrics\n","        )\n","    else:\n","        trainer = ContrastiveTrainer(\n","            model=model,\n","            args=training_args,\n","            data_collator=data_collator,\n","            train_dataset=train_dataset,\n","            eval_dataset=eval_dataset\n","        )\n","        trainer.set_lambd(contrastive_train_lambd)\n","\n","    # Train the model\n","    if not dont_train:\n","        logger.info(\"=== Training the model ===\")\n","        trainer.train()\n","        trainer.save_model(\"./lm_train_cache/\")\n","        if log_history:\n","            log_file = f\"{model_name}_epochs_{num_epochs}_eval_loss.p\"\n","            with open(log_file, \"wb\") as f:\n","                pickle.dump(trainer.state.log_history, f)\n","\n","    # Evaluate the model\n","    results = {}\n","    if not dont_eval: #Note: for hyperparameter tuning we do it by loss on\n","        model.eval()\n","        logger.info(\"=== Evaluating the model ===\")\n","        eval_output = trainer.evaluate()\n","        eval_loss = eval_output[\"eval_loss\"]\n","        results[\"eval_loss\"] = eval_loss\n","\n","        acc_test, out_df_test, preds_test, labels_test = evaluate_model(model, tokenizer, test_df.to_dict(orient=\"records\"), use_cuda=use_cuda, return_acc=True, middle_phrase=prompt, use_prefix=prefix_prompt)\n","        acc_dev, out_df_dev, preds_dev, labels_dev = evaluate_model(model, tokenizer, eval_df.to_dict(orient=\"records\"), use_cuda=use_cuda, return_acc=True, middle_phrase=prompt, use_prefix=prefix_prompt)\n","        results[\"accuracy (test)\"] = acc_test\n","        results[\"accuracy (dev)\"] = acc_dev\n","        results[\"preds\"] = preds_test\n","        results[\"labels\"] = labels_test\n","\n","\n","    if out_path is not None:\n","        Path(out_path).mkdir(parents=True, exist_ok=True)\n","        with open(f\"{out_path}/results_{model_name}.txt\", \"w\") as writer:\n","            logger.info(\"=== Outputting results ===\")\n","            for key in sorted(results.keys()):\n","                logger.info(\"  %s = %s\", key, str(results[key]))\n","                writer.write(\"%s = %s\\n\" % (key, str(results[key])))\n","\n","        out_df_test.to_csv(f\"{out_path}/prob_{model_name}_{seed}.csv\", index=False)\n","\n","    return results\n","\n","def training_setup(model, tokenizer, model_name, seed, lr, num_epochs, train_path, eval_path, contrastive_train=False, contrast_lambd=1, is_hyperparam_opt=False, cuda=True, deepspeed=False, batch_size=8) -> Trainer:\n","    # load datasets and initialize trainer\n","    train_dataset = (\n","        get_dataset(train_path, tokenizer=tokenizer)\n","    )\n","    eval_dataset = (\n","        get_dataset(eval_path, tokenizer=tokenizer)\n","    )\n","\n","    data_collator = DataCollatorForLanguageModeling(\n","                tokenizer=tokenizer, mlm=False\n","            )\n","    set_seed(seed)\n","\n","    default_train_args = {\n","        \"output_dir\": f\"./lm_train_outputs/{model_name}_{seed}/\",\n","        \"do_train\": True,\n","        \"do_eval\": False,\n","        \"prediction_loss_only\": True,\n","        \"seed\": seed,\n","        \"num_train_epochs\": num_epochs,\n","        \"learning_rate\": lr,\n","        \"no_cuda\": not cuda,\n","        \"per_device_train_batch_size\": batch_size,\n","        \"per_device_eval_batch_size\": batch_size\n","    }\n","\n","    if contrastive_train:\n","        default_train_args[\"per_device_train_batch_size\"] = 2\n","        training_args = transformers.TrainingArguments(output_dir=f\"./lm_train_outputs/{model_name}_{seed}/\", do_train=True, do_eval=False,\n","        prediction_loss_only=True, num_train_epochs=num_epochs, seed=seed,learning_rate=lr, per_device_train_batch_size=2)\n","    elif is_hyperparam_opt:\n","        default_train_args[\"evaluation_strategy\"] = \"steps\"\n","        default_train_args[\"eval_steps\"] = 500\n","        default_train_args[\"disable_tqdm\"] = True\n","    if deepspeed == True:\n","        default_train_args[\"deepspeed\"] = \"./deepspeed_config.json\"\n","\n","    training_args = transformers.TrainingArguments(**default_train_args)\n","\n","\n","    if is_hyperparam_opt:\n","        tokenizer.pad_token = tokenizer.eos_token\n","        dummy_init = make_dummy(model_name)\n","        trainer = Trainer(\n","            args=training_args,\n","            data_collator=data_collator,\n","            train_dataset=train_dataset,\n","            eval_dataset=eval_dataset,\n","            model_init=dummy_init,\n","            compute_metrics=compute_metrics\n","        )\n","    elif contrastive_train:\n","        trainer = ContrastiveTrainer(\n","            model=model,\n","            args=training_args,\n","            data_collator=data_collator,\n","            train_dataset=train_dataset,\n","            eval_dataset=eval_dataset\n","        )\n","        trainer.set_lambd(contrast_lambd)\n","    else:\n","        trainer = Trainer(\n","            model=model,\n","            args=training_args,\n","            data_collator=data_collator,\n","            train_dataset=train_dataset,\n","            eval_dataset=eval_dataset\n","        )\n","    return trainer\n","\n","# This is adapted from the huggingface LM training example here: https://github.com/huggingface/transformers/blob/master/examples/legacy/run_language_modeling.py\n","def get_dataset(\n","    train_data_file: str,\n","    tokenizer: PreTrainedTokenizer,\n","    line_by_line: bool = True,\n","    evaluate: bool = False,\n","    eval_data_file: str = None,\n","    cache_dir: Optional[str] = None,\n","):\n","    def _dataset(file_path, ref_path=None):\n","        if line_by_line:\n","            if ref_path is not None:\n","                if not args.whole_word_mask or not args.mlm:\n","                    raise ValueError(\"You need to set world whole masking and mlm to True for Chinese Whole Word Mask\")\n","                return LineByLineWithRefDataset(\n","                    tokenizer=tokenizer,\n","                    file_path=file_path,\n","                    block_size=tokenizer.model_max_length,\n","                    ref_path=ref_path,\n","                )\n","\n","            return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=tokenizer.model_max_length)\n","\n","    if evaluate:\n","        return _dataset(eval_data_file)\n","    else:\n","        return _dataset(train_data_file)\n","\n","def make_dummy(model_id):\n","    def dummy_init():\n","        if model_id == \"gpt2\":\n","            return GPT2LMHeadModel.from_pretrained(\"gpt2\", return_dict=True)\n","        elif \"gpt-neo\" in model_id:\n","            return GPTNeoForCausalLM.from_pretrained(model_id, return_dict=True)\n","    return dummy_init\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = predictions.argmax(axis=-1)\n","    acc = len(np.where(predictions == labels)[0])/len(labels)\n","    return {\"acc\": acc}\n","\n","class ContrastiveTrainer(Trainer):\n","    def set_lambd(self, lambd):\n","        self.lambd = lambd\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        # Assumes batch size of 2!\n","        if inputs[\"labels\"].shape[0] % 2 != 0:\n","            raise ValueError(\"Batch size must be a multiple of 2\")\n","\n","        correct_inputs = {\"input_ids\": torch.stack([row for i, row in enumerate(inputs[\"input_ids\"]) if i % 2 == 0]),\n","        \"attention_mask\": torch.stack([row for i, row in enumerate(inputs[\"attention_mask\"]) if i % 2 == 0]),\n","        \"labels\":  torch.stack([row for i, row in enumerate(inputs[\"labels\"]) if i % 2 == 0])}\n","        wrong_inputs = {\"input_ids\": torch.stack([row for i, row in enumerate(inputs[\"input_ids\"]) if i % 2 == 1]),\n","        \"attention_mask\": torch.stack([row for i, row in enumerate(inputs[\"attention_mask\"]) if i % 2 == 1]),\n","        \"labels\":  torch.stack([row for i, row in enumerate(inputs[\"labels\"]) if i % 2 == 1])}\n","\n","        outputs = model(**inputs)\n","\n","        correct_outputs = model(**correct_inputs)\n","        correct_loss = correct_outputs.get('loss')\n","\n","        wrong_outputs = model(**wrong_inputs)\n","        wrong_loss = wrong_outputs.get(\"loss\")\n","\n","        # Good = when the loss for the correct item is much lower than loss for wrong item\n","        # loss should be negative (good) when wrong loss > correct loss\n","        #lambd = self.lambd if self.lambd else 1\n","        lambd = 0.2\n","        relative_score = correct_loss - lambd * (wrong_loss + correct_loss)\n","        loss = -relative_score\n","\n","        return (loss, outputs) if return_outputs else loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":32776,"status":"error","timestamp":1697019090316,"user":{"displayName":"gvvvbjj ffbkitb","userId":"08953530729188044176"},"user_tz":-120},"id":"wHglxnBuGMKG","outputId":"07fd0e3a-7dae-4812-b78f-e663db12d019"},"outputs":[{"name":"stderr","output_type":"stream","text":["[INFO|tokenization_utils_base.py:2043] 2023-10-11 10:10:57,554 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--EleutherAI--gpt-neo-125m/snapshots/6cb0d322a3a484e99667e7cb240e22f1ac036b99/vocab.json\n","[INFO|tokenization_utils_base.py:2043] 2023-10-11 10:10:57,555 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--EleutherAI--gpt-neo-125m/snapshots/6cb0d322a3a484e99667e7cb240e22f1ac036b99/merges.txt\n","[INFO|tokenization_utils_base.py:2043] 2023-10-11 10:10:57,560 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2043] 2023-10-11 10:10:57,561 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--EleutherAI--gpt-neo-125m/snapshots/6cb0d322a3a484e99667e7cb240e22f1ac036b99/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2043] 2023-10-11 10:10:57,564 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--EleutherAI--gpt-neo-125m/snapshots/6cb0d322a3a484e99667e7cb240e22f1ac036b99/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2043] 2023-10-11 10:10:57,565 >> loading file tokenizer.json from cache at None\n","[INFO|configuration_utils.py:715] 2023-10-11 10:10:57,567 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--EleutherAI--gpt-neo-125m/snapshots/6cb0d322a3a484e99667e7cb240e22f1ac036b99/config.json\n","[INFO|configuration_utils.py:775] 2023-10-11 10:10:57,571 >> Model config GPTNeoConfig {\n","  \"_name_or_path\": \"EleutherAI/gpt-neo-125m\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPTNeoForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0,\n","  \"attention_layers\": [\n","    \"global\",\n","    \"local\",\n","    \"global\",\n","    \"local\",\n","    \"global\",\n","    \"local\",\n","    \"global\",\n","    \"local\",\n","    \"global\",\n","    \"local\",\n","    \"global\",\n","    \"local\"\n","  ],\n","  \"attention_types\": [\n","    [\n","      [\n","        \"global\",\n","        \"local\"\n","      ],\n","      6\n","    ]\n","  ],\n","  \"bos_token_id\": 50256,\n","  \"classifier_dropout\": 0.1,\n","  \"embed_dropout\": 0,\n","  \"eos_token_id\": 50256,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": null,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"gpt_neo\",\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"resid_dropout\": 0,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"transformers_version\": \"4.34.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257,\n","  \"window_size\": 256\n","}\n","\n","[INFO|tokenization_utils.py:493] 2023-10-11 10:10:57,640 >> Adding <|endoftext|> to the vocabulary\n","[INFO|configuration_utils.py:715] 2023-10-11 10:10:58,142 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--EleutherAI--gpt-neo-125m/snapshots/6cb0d322a3a484e99667e7cb240e22f1ac036b99/config.json\n","[INFO|configuration_utils.py:775] 2023-10-11 10:10:58,145 >> Model config GPTNeoConfig {\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPTNeoForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0,\n","  \"attention_layers\": [\n","    \"global\",\n","    \"local\",\n","    \"global\",\n","    \"local\",\n","    \"global\",\n","    \"local\",\n","    \"global\",\n","    \"local\",\n","    \"global\",\n","    \"local\",\n","    \"global\",\n","    \"local\"\n","  ],\n","  \"attention_types\": [\n","    [\n","      [\n","        \"global\",\n","        \"local\"\n","      ],\n","      6\n","    ]\n","  ],\n","  \"bos_token_id\": 50256,\n","  \"classifier_dropout\": 0.1,\n","  \"embed_dropout\": 0,\n","  \"eos_token_id\": 50256,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": null,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"gpt_neo\",\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"resid_dropout\": 0,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"transformers_version\": \"4.34.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257,\n","  \"window_size\": 256\n","}\n","\n","[INFO|modeling_utils.py:2993] 2023-10-11 10:10:58,148 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--EleutherAI--gpt-neo-125m/snapshots/6cb0d322a3a484e99667e7cb240e22f1ac036b99/model.safetensors\n","[INFO|configuration_utils.py:770] 2023-10-11 10:10:58,159 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","[INFO|modeling_utils.py:3775] 2023-10-11 10:10:59,864 >> All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n","\n","[INFO|modeling_utils.py:3783] 2023-10-11 10:10:59,865 >> All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at EleutherAI/gpt-neo-125m.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n","[INFO|modeling_utils.py:3352] 2023-10-11 10:11:00,118 >> Generation config file not found, using a generation config created from the model config.\n","/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  warnings.warn(\n","[INFO|language_modeling.py:130] 2023-10-11 10:11:00,290 >> Creating features from dataset file at ./data/lm_train_data/train.txt\n","[INFO|language_modeling.py:130] 2023-10-11 10:11:00,563 >> Creating features from dataset file at ./data/lm_train_data/dev.txt\n","[INFO|training_args.py:1345] 2023-10-11 10:11:00,741 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","[INFO|training_args.py:1798] 2023-10-11 10:11:00,742 >> PyTorch: setting up devices\n","[INFO|training_args.py:1519] 2023-10-11 10:11:00,745 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","[INFO|trainer.py:1760] 2023-10-11 10:11:01,012 >> ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-11 10:11:01,013 >>   Num examples = 1,458\n","[INFO|trainer.py:1762] 2023-10-11 10:11:01,015 >>   Num Epochs = 3\n","[INFO|trainer.py:1763] 2023-10-11 10:11:01,017 >>   Instantaneous batch size per device = 8\n","[INFO|trainer.py:1766] 2023-10-11 10:11:01,020 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:1767] 2023-10-11 10:11:01,023 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-11 10:11:01,024 >>   Total optimization steps = 549\n","[INFO|trainer.py:1769] 2023-10-11 10:11:01,028 >>   Number of trainable parameters = 125,198,592\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='101' max='549' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [101/549 00:11 < 00:52, 8.50 it/s, Epoch 0.55/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>\n","    <div>\n","      \n","      <progress value='111' max='137' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [111/137 00:16 < 00:03, 6.84 it/s]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3213] 2023-10-11 10:11:13,157 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 10:11:13,159 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 10:11:13,161 >>   Batch size = 8\n"]},{"ename":"OutOfMemoryError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-02718d4f5287>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# main(args.model, args.middle_phrase, args.train_path, args.eval_path, args.contrastive, args.contrast_lambd, args.num_epochs, args.seed, learning_rate, args.cuda, args.dont_train, args.dont_eval, out_path, prefix_prompt=args.prefix, log_history=args.log_history, deepspeed=deepspeed, early_stopping=args.early_stopping)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt-neo-sssm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./data/lm_train_data/train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./data/lm_train_data/dev.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrastive_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrastive_train_lambd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdont_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdont_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepspeed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-e7d53ee837c4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model_name, prompt, train_path, eval_path, contrastive_train, contrastive_train_lambd, num_epochs, seed, lr, use_cuda, dont_train, dont_eval, out_path, cache_dir, prefix_prompt, batch_size, log_history, deepspeed, early_stopping)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdont_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=== Training the model ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./lm_train_cache/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlog_history\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1592\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1982\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1984\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1985\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1986\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2326\u001b[0m                     \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m                 \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   3067\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3279\u001b[0m                     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_logits_for_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3280\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_for_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3281\u001b[0;31m                 \u001b[0mpreds_host\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreds_host\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3283\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         return type(tensors)(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# Now let's fill the result tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.21 GiB (GPU 0; 14.75 GiB total capacity; 8.17 GiB already allocated; 6.17 GiB free; 8.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["# main(args.model, args.middle_phrase, args.train_path, args.eval_path, args.contrastive, args.contrast_lambd, args.num_epochs, args.seed, learning_rate, args.cuda, args.dont_train, args.dont_eval, out_path, prefix_prompt=args.prefix, log_history=args.log_history, deepspeed=deepspeed, early_stopping=args.early_stopping)\n","\n","main(\"gpt-neo-sssm\", \"\", \"./data/lm_train_data/train.txt\", \"./data/lm_train_data/dev.txt\", contrastive_train=False, contrastive_train_lambd=1, num_epochs=3, seed=42, lr=5e-5, use_cuda=True, dont_train=False, dont_eval=False, out_path=None, prefix_prompt=0, log_history=True, deepspeed=False, early_stopping=False)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["541d94ea7d53489999d426b08cd6baaf","769cbe6abaf247c0919570b0496d9df8","646cd38235594cedad6ed80e56fb4e47","2b1583a156fa48ba94ec3ec0ee4b0003","d1a425818295408fa59a8d3acbccede7","3134a92e836f44daaddf7e7f039ad734","49b3ad660df54aa593c0ccef80bea50a","73baf6f146c34ba5b5c07be9e3f272aa","00e373beeefd46f096a578512fa47320","8faa55e7ccd4480ca39bb5e1c747cf80","3401855a561d448c9fcd1c4371093d87","a470b738ffc64f7fbbadb3718c9560e4","fd733f92ef63451ba15c491c2fd8058a","e84ddf2936e3464fa336f5d699b03428","9042349a3381442ab578fb761404a6fd","ef3df86094274b29aecb10797f441eb6","3f693a8ad15b447d966255be73f529af","56676b88a3024b76bbea11a58ebcf99f","7ccfe49a405847f8a6db12c56aa3d894","64d261c5cde64557a0d0a398d492877c","50f008b3b3194997b8f2a86ba495910b","98f387b79422401d8406a341bd160766","d4abae26745649c190327cf8e40d9911","58d5492177d042ec89b15688f4c16c19","40297d74a3604357ac2bac071b64821b","c61afc6de45b4d6fb7db961275bdde27","2f468530885f4eed86b44ec50bba5a58","069dcd08fd0f490980e1aefbbb93e123","4d3f3660f24c4b38805deba660db7465","db33f2165ac143eeac9df8de8a14c29b","1b9c45395b3e4886a046bd49708a6578","314f422561a14306a1492f998193eb16","721e706d762447c18d5bcfb274163a02","6d3c9a5d828b41d0b30668b853a68a4f","4ae3462b07424c24a2998a938f3d6e80","4aea8568ac434fb4955d380e37524436","34f2bf89da594d90b765346e7212c893","c4a95c72abfa4f449fcdf2f4105d956f","fc2cd9b9d3f94fe084626c01e20cc11d","aed90cf6cc7d4f17a07e626c228adca3","beb28585adfd454c82dd994813f5b143","84196de972124e91b9dbbb243973404c","53112a4825e64d46ba578adde2a445e7","d35d9a185cc04c1ebff2486a05ffe26b","7213c142ef9e4b8b8b276628802b41e5","abd92d83ccdb4fa0bf36e7516d35a93c","356bac18fd444b09bb0994f088b34a42","d2e43765f023467289c6df0dc26806bc","23739646c4d946cca356074ff4043595","9f3d1d58fa6d49269b1910ce02b7ce45","c01e09f838bb46d1968661ca9fed3f37","feed22d87090449a89644a961c1ad996","b0cf00768aed49c795cc231442c248c6","5c3a04d56ad145979ca22c463a0c4367","25fd776d3cd24eb4908be41929f0e2bb","e567e2883c90461db95ad6e3b3f24aac","28bf244993e6470bb7332231038438d2","c8a96debca54492a964e2cb3a3e0a57a","3ed6bdc067ff4d62967753fd3ea73319","0f08d5e4a2ff415d8dd87aef20e88493","56d58831a3174892b055cb916b4550cb","b1e4308701ea40f7a62a22bcd9fe27df","8e129f17d42540019ba9cfe7adbd367f","5eb3141a8233462e861c99988b19289e","cd6de165c6d841e191742f3939cba1e5","268653f0be014f2682ab5bf250da6fd8","3d2462e1aa974eb8807cf3c1c38a307f","e1321396a2c64d32891c99a71b51510c","78774aa070d04130965f5f8905a505ec","41c25770584248519f79ffefc5bf1306","7cef0f81ec1a481e8b87e53966df3fe6","3ab247fdf1c8488e809c98474211ada1","b5fabac8cd4f4d269cee5098b81f5564","62c7255f5e0a4fca8e3094870fa5ba3d","e4bf294946d6441ab4ccc90cdd27c5a3","b095e0a1c1984fd0abf2abae4b8ff049","3c3a6fc2e5b44dcbb9f72b26c111f2bc"]},"executionInfo":{"elapsed":1333469,"status":"error","timestamp":1697038743335,"user":{"displayName":"gvvvbjj ffbkitb","userId":"08953530729188044176"},"user_tz":-120},"id":"3kyZue0NT2tv","outputId":"1ca09c92-5488-4412-a394-ece0268e53da"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"541d94ea7d53489999d426b08cd6baaf","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a470b738ffc64f7fbbadb3718c9560e4","version_major":2,"version_minor":0},"text/plain":["Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d4abae26745649c190327cf8e40d9911","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d3c9a5d828b41d0b30668b853a68a4f","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|tokenization_utils_base.py:2043] 2023-10-11 15:16:52,299 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/spiece.model\n","[INFO|tokenization_utils_base.py:2043] 2023-10-11 15:16:52,300 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/tokenizer.json\n","[INFO|tokenization_utils_base.py:2043] 2023-10-11 15:16:52,302 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2043] 2023-10-11 15:16:52,303 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2043] 2023-10-11 15:16:52,304 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/tokenizer_config.json\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7213c142ef9e4b8b8b276628802b41e5","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|configuration_utils.py:715] 2023-10-11 15:16:52,563 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json\n","[INFO|configuration_utils.py:775] 2023-10-11 15:16:52,568 >> Model config T5Config {\n","  \"_name_or_path\": \"google/flan-t5-base\",\n","  \"architectures\": [\n","    \"T5ForConditionalGeneration\"\n","  ],\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"tie_word_embeddings\": false,\n","  \"transformers_version\": \"4.34.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e567e2883c90461db95ad6e3b3f24aac","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|modeling_utils.py:2993] 2023-10-11 15:17:16,357 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/model.safetensors\n","[INFO|configuration_utils.py:770] 2023-10-11 15:17:16,423 >> Generate config GenerationConfig {\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0\n","}\n","\n","[INFO|modeling_utils.py:3775] 2023-10-11 15:17:19,140 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:3783] 2023-10-11 15:17:19,142 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-base.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d2462e1aa974eb8807cf3c1c38a307f","version_major":2,"version_minor":0},"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|configuration_utils.py:730] 2023-10-11 15:17:19,303 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/generation_config.json\n","[INFO|configuration_utils.py:770] 2023-10-11 15:17:19,306 >> Generate config GenerationConfig {\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0\n","}\n","\n","/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  warnings.warn(\n","[INFO|language_modeling.py:130] 2023-10-11 15:17:19,911 >> Creating features from dataset file at ./data/lm_train_data/train.txt\n","[INFO|language_modeling.py:130] 2023-10-11 15:17:20,061 >> Creating features from dataset file at ./data/lm_train_data/dev.txt\n","[INFO|training_args.py:1345] 2023-10-11 15:17:20,249 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","[INFO|training_args.py:1798] 2023-10-11 15:17:20,251 >> PyTorch: setting up devices\n","[INFO|training_args.py:1519] 2023-10-11 15:17:20,255 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","[INFO|trainer.py:1760] 2023-10-11 15:17:20,647 >> ***** Running training *****\n","[INFO|trainer.py:1761] 2023-10-11 15:17:20,648 >>   Num examples = 1,458\n","[INFO|trainer.py:1762] 2023-10-11 15:17:20,653 >>   Num Epochs = 3\n","[INFO|trainer.py:1763] 2023-10-11 15:17:20,655 >>   Instantaneous batch size per device = 2\n","[INFO|trainer.py:1766] 2023-10-11 15:17:20,657 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n","[INFO|trainer.py:1767] 2023-10-11 15:17:20,658 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1768] 2023-10-11 15:17:20,662 >>   Total optimization steps = 2,187\n","[INFO|trainer.py:1769] 2023-10-11 15:17:20,666 >>   Number of trainable parameters = 247,577,856\n","[WARNING|logging.py:290] 2023-10-11 15:17:20,696 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2187' max='2187' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2187/2187 21:19, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>No log</td>\n","      <td>-15.406539</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>No log</td>\n","      <td>-78.104118</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>No log</td>\n","      <td>-94.428482</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>No log</td>\n","      <td>-102.373970</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>-61.238500</td>\n","      <td>-110.643326</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>-61.238500</td>\n","      <td>-115.399178</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>-61.238500</td>\n","      <td>-119.467453</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>-61.238500</td>\n","      <td>-122.845894</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>-61.238500</td>\n","      <td>-125.702354</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>-110.325500</td>\n","      <td>-128.001404</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>-110.325500</td>\n","      <td>-130.221344</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>-110.325500</td>\n","      <td>-132.107346</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>-110.325500</td>\n","      <td>-133.627319</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>-110.325500</td>\n","      <td>-135.260666</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>-123.501500</td>\n","      <td>-136.421768</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>-123.501500</td>\n","      <td>-137.656738</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>-123.501500</td>\n","      <td>-138.525864</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>-123.501500</td>\n","      <td>-139.229828</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>-123.501500</td>\n","      <td>-139.823868</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>-129.846500</td>\n","      <td>-140.189148</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>-129.846500</td>\n","      <td>-140.422668</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[INFO|trainer.py:3213] 2023-10-11 15:18:02,992 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:18:02,995 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:18:02,997 >>   Batch size = 8\n","[INFO|trainer.py:3213] 2023-10-11 15:19:01,163 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:19:01,164 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:19:01,167 >>   Batch size = 8\n","[INFO|trainer.py:3213] 2023-10-11 15:19:57,216 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:19:57,219 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:19:57,221 >>   Batch size = 8\n","[INFO|trainer.py:3213] 2023-10-11 15:20:51,847 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:20:51,848 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:20:51,853 >>   Batch size = 8\n","[INFO|trainer.py:3213] 2023-10-11 15:21:47,252 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:21:47,254 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:21:47,256 >>   Batch size = 8\n","[INFO|trainer.py:2939] 2023-10-11 15:22:03,894 >> Saving model checkpoint to ./lm_train_outputs/flan-t5-base_42/checkpoint-500\n","[INFO|configuration_utils.py:460] 2023-10-11 15:22:03,897 >> Configuration saved in ./lm_train_outputs/flan-t5-base_42/checkpoint-500/config.json\n","[INFO|configuration_utils.py:544] 2023-10-11 15:22:03,901 >> Configuration saved in ./lm_train_outputs/flan-t5-base_42/checkpoint-500/generation_config.json\n","[INFO|modeling_utils.py:2118] 2023-10-11 15:22:14,887 >> Model weights saved in ./lm_train_outputs/flan-t5-base_42/checkpoint-500/pytorch_model.bin\n","[INFO|trainer.py:3213] 2023-10-11 15:23:01,602 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:23:01,605 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:23:01,613 >>   Batch size = 8\n","[INFO|trainer.py:3213] 2023-10-11 15:23:57,496 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:23:57,502 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:23:57,503 >>   Batch size = 8\n","[INFO|trainer.py:3213] 2023-10-11 15:24:53,911 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:24:53,913 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:24:53,918 >>   Batch size = 8\n","[INFO|trainer.py:3213] 2023-10-11 15:25:49,795 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:25:49,800 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:25:49,802 >>   Batch size = 8\n","[INFO|trainer.py:3213] 2023-10-11 15:26:46,460 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:26:46,462 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:26:46,464 >>   Batch size = 8\n","[INFO|trainer.py:2939] 2023-10-11 15:27:03,855 >> Saving model checkpoint to ./lm_train_outputs/flan-t5-base_42/checkpoint-1000\n","[INFO|configuration_utils.py:460] 2023-10-11 15:27:03,859 >> Configuration saved in ./lm_train_outputs/flan-t5-base_42/checkpoint-1000/config.json\n","[INFO|configuration_utils.py:544] 2023-10-11 15:27:03,862 >> Configuration saved in ./lm_train_outputs/flan-t5-base_42/checkpoint-1000/generation_config.json\n","[INFO|modeling_utils.py:2118] 2023-10-11 15:27:15,043 >> Model weights saved in ./lm_train_outputs/flan-t5-base_42/checkpoint-1000/pytorch_model.bin\n","[INFO|trainer.py:3213] 2023-10-11 15:28:08,103 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:28:08,108 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:28:08,110 >>   Batch size = 8\n","[INFO|trainer.py:3213] 2023-10-11 15:29:03,522 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:29:03,527 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:29:03,528 >>   Batch size = 8\n","[INFO|trainer.py:3213] 2023-10-11 15:29:58,591 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:29:58,595 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:29:58,597 >>   Batch size = 8\n","[INFO|trainer.py:3213] 2023-10-11 15:30:53,599 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:30:53,604 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:30:53,606 >>   Batch size = 8\n","[INFO|trainer.py:3213] 2023-10-11 15:31:50,956 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:31:50,958 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:31:50,960 >>   Batch size = 8\n","[INFO|trainer.py:2939] 2023-10-11 15:32:08,795 >> Saving model checkpoint to ./lm_train_outputs/flan-t5-base_42/checkpoint-1500\n","[INFO|configuration_utils.py:460] 2023-10-11 15:32:08,799 >> Configuration saved in ./lm_train_outputs/flan-t5-base_42/checkpoint-1500/config.json\n","[INFO|configuration_utils.py:544] 2023-10-11 15:32:08,801 >> Configuration saved in ./lm_train_outputs/flan-t5-base_42/checkpoint-1500/generation_config.json\n","[INFO|modeling_utils.py:2118] 2023-10-11 15:32:12,391 >> Model weights saved in ./lm_train_outputs/flan-t5-base_42/checkpoint-1500/pytorch_model.bin\n","[INFO|trainer.py:3213] 2023-10-11 15:32:59,111 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:32:59,113 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:32:59,116 >>   Batch size = 8\n","[INFO|trainer.py:3213] 2023-10-11 15:33:55,532 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:33:55,537 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:33:55,538 >>   Batch size = 8\n","[INFO|trainer.py:3213] 2023-10-11 15:34:52,427 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:34:52,430 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:34:52,435 >>   Batch size = 8\n","[INFO|trainer.py:3213] 2023-10-11 15:35:48,172 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:35:48,177 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:35:48,180 >>   Batch size = 8\n","[INFO|trainer.py:3213] 2023-10-11 15:36:43,728 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:36:43,730 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:36:43,732 >>   Batch size = 8\n","[INFO|trainer.py:2939] 2023-10-11 15:37:00,763 >> Saving model checkpoint to ./lm_train_outputs/flan-t5-base_42/checkpoint-2000\n","[INFO|configuration_utils.py:460] 2023-10-11 15:37:00,767 >> Configuration saved in ./lm_train_outputs/flan-t5-base_42/checkpoint-2000/config.json\n","[INFO|configuration_utils.py:544] 2023-10-11 15:37:00,769 >> Configuration saved in ./lm_train_outputs/flan-t5-base_42/checkpoint-2000/generation_config.json\n","[INFO|modeling_utils.py:2118] 2023-10-11 15:37:04,198 >> Model weights saved in ./lm_train_outputs/flan-t5-base_42/checkpoint-2000/pytorch_model.bin\n","[INFO|trainer.py:3213] 2023-10-11 15:37:50,818 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:37:50,823 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:37:50,824 >>   Batch size = 8\n","[INFO|trainer.py:2017] 2023-10-11 15:38:42,027 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","[INFO|trainer.py:2939] 2023-10-11 15:38:42,132 >> Saving model checkpoint to ./lm_train_cache/\n","[INFO|configuration_utils.py:460] 2023-10-11 15:38:42,134 >> Configuration saved in ./lm_train_cache/config.json\n","[INFO|configuration_utils.py:544] 2023-10-11 15:38:42,137 >> Configuration saved in ./lm_train_cache/generation_config.json\n","[INFO|modeling_utils.py:2118] 2023-10-11 15:38:45,594 >> Model weights saved in ./lm_train_cache/pytorch_model.bin\n","[INFO|trainer.py:3213] 2023-10-11 15:38:45,602 >> ***** Running Evaluation *****\n","[INFO|trainer.py:3215] 2023-10-11 15:38:45,605 >>   Num examples = 1094\n","[INFO|trainer.py:3218] 2023-10-11 15:38:45,607 >>   Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='137' max='137' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [137/137 00:16]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"ZeroDivisionError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-624e913c5c16>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"flan-t5-base\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./data/lm_train_data/train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./data/lm_train_data/dev.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrastive_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrastive_train_lambd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdont_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdont_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepspeed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-340e0c2daaec>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model_name, prompt, train_path, eval_path, contrastive_train, contrastive_train_lambd, num_epochs, seed, lr, use_cuda, dont_train, dont_eval, out_path, cache_dir, prefix_prompt, batch_size, log_history, deepspeed, early_stopping)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_loss\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0macc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_df_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"records\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiddle_phrase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0macc_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_df_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"records\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiddle_phrase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy (test)\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/Fig-QA/src/models/gpt_score.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, tokenizer, test_set, middle_phrase, use_prefix, verbose, score_type, use_cuda, return_acc)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mP_x_1_y_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mP_x_1_y_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mP_x_1_correct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscore2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"]}],"source":["main(\"flan-t5-base\", \"\", \"./data/lm_train_data/train.txt\", \"./data/lm_train_data/dev.txt\", contrastive_train=True, contrastive_train_lambd=1, num_epochs=3, seed=42, lr=5e-5, use_cuda=True, dont_train=False, dont_eval=False, out_path=None, prefix_prompt=0, log_history=True, deepspeed=False, early_stopping=False)\n"]},{"cell_type":"markdown","metadata":{"id":"zjN5FCH1J8Up"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMavgjI111lRlrlEopotrOz","gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00e373beeefd46f096a578512fa47320":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"069dcd08fd0f490980e1aefbbb93e123":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f08d5e4a2ff415d8dd87aef20e88493":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b9c45395b3e4886a046bd49708a6578":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"23739646c4d946cca356074ff4043595":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25fd776d3cd24eb4908be41929f0e2bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"268653f0be014f2682ab5bf250da6fd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"28bf244993e6470bb7332231038438d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56d58831a3174892b055cb916b4550cb","placeholder":"​","style":"IPY_MODEL_b1e4308701ea40f7a62a22bcd9fe27df","value":"Downloading model.safetensors: 100%"}},"2b1583a156fa48ba94ec3ec0ee4b0003":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8faa55e7ccd4480ca39bb5e1c747cf80","placeholder":"​","style":"IPY_MODEL_3401855a561d448c9fcd1c4371093d87","value":" 2.54k/2.54k [00:00&lt;00:00, 62.3kB/s]"}},"2f468530885f4eed86b44ec50bba5a58":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3134a92e836f44daaddf7e7f039ad734":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"314f422561a14306a1492f998193eb16":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3401855a561d448c9fcd1c4371093d87":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34f2bf89da594d90b765346e7212c893":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53112a4825e64d46ba578adde2a445e7","placeholder":"​","style":"IPY_MODEL_d35d9a185cc04c1ebff2486a05ffe26b","value":" 2.20k/2.20k [00:00&lt;00:00, 158kB/s]"}},"356bac18fd444b09bb0994f088b34a42":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_feed22d87090449a89644a961c1ad996","max":1404,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b0cf00768aed49c795cc231442c248c6","value":1404}},"3ab247fdf1c8488e809c98474211ada1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c3a6fc2e5b44dcbb9f72b26c111f2bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d2462e1aa974eb8807cf3c1c38a307f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1321396a2c64d32891c99a71b51510c","IPY_MODEL_78774aa070d04130965f5f8905a505ec","IPY_MODEL_41c25770584248519f79ffefc5bf1306"],"layout":"IPY_MODEL_7cef0f81ec1a481e8b87e53966df3fe6"}},"3ed6bdc067ff4d62967753fd3ea73319":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd6de165c6d841e191742f3939cba1e5","placeholder":"​","style":"IPY_MODEL_268653f0be014f2682ab5bf250da6fd8","value":" 990M/990M [00:23&lt;00:00, 48.3MB/s]"}},"3f693a8ad15b447d966255be73f529af":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40297d74a3604357ac2bac071b64821b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_db33f2165ac143eeac9df8de8a14c29b","max":2424064,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1b9c45395b3e4886a046bd49708a6578","value":2424064}},"41c25770584248519f79ffefc5bf1306":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b095e0a1c1984fd0abf2abae4b8ff049","placeholder":"​","style":"IPY_MODEL_3c3a6fc2e5b44dcbb9f72b26c111f2bc","value":" 147/147 [00:00&lt;00:00, 10.0kB/s]"}},"49b3ad660df54aa593c0ccef80bea50a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ae3462b07424c24a2998a938f3d6e80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc2cd9b9d3f94fe084626c01e20cc11d","placeholder":"​","style":"IPY_MODEL_aed90cf6cc7d4f17a07e626c228adca3","value":"Downloading (…)cial_tokens_map.json: 100%"}},"4aea8568ac434fb4955d380e37524436":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_beb28585adfd454c82dd994813f5b143","max":2201,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84196de972124e91b9dbbb243973404c","value":2201}},"4d3f3660f24c4b38805deba660db7465":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50f008b3b3194997b8f2a86ba495910b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53112a4825e64d46ba578adde2a445e7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"541d94ea7d53489999d426b08cd6baaf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_769cbe6abaf247c0919570b0496d9df8","IPY_MODEL_646cd38235594cedad6ed80e56fb4e47","IPY_MODEL_2b1583a156fa48ba94ec3ec0ee4b0003"],"layout":"IPY_MODEL_d1a425818295408fa59a8d3acbccede7"}},"56676b88a3024b76bbea11a58ebcf99f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56d58831a3174892b055cb916b4550cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58d5492177d042ec89b15688f4c16c19":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_069dcd08fd0f490980e1aefbbb93e123","placeholder":"​","style":"IPY_MODEL_4d3f3660f24c4b38805deba660db7465","value":"Downloading (…)/main/tokenizer.json: 100%"}},"5c3a04d56ad145979ca22c463a0c4367":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5eb3141a8233462e861c99988b19289e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62c7255f5e0a4fca8e3094870fa5ba3d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"646cd38235594cedad6ed80e56fb4e47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_73baf6f146c34ba5b5c07be9e3f272aa","max":2537,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00e373beeefd46f096a578512fa47320","value":2537}},"64d261c5cde64557a0d0a398d492877c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6d3c9a5d828b41d0b30668b853a68a4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ae3462b07424c24a2998a938f3d6e80","IPY_MODEL_4aea8568ac434fb4955d380e37524436","IPY_MODEL_34f2bf89da594d90b765346e7212c893"],"layout":"IPY_MODEL_c4a95c72abfa4f449fcdf2f4105d956f"}},"7213c142ef9e4b8b8b276628802b41e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_abd92d83ccdb4fa0bf36e7516d35a93c","IPY_MODEL_356bac18fd444b09bb0994f088b34a42","IPY_MODEL_d2e43765f023467289c6df0dc26806bc"],"layout":"IPY_MODEL_23739646c4d946cca356074ff4043595"}},"721e706d762447c18d5bcfb274163a02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73baf6f146c34ba5b5c07be9e3f272aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"769cbe6abaf247c0919570b0496d9df8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3134a92e836f44daaddf7e7f039ad734","placeholder":"​","style":"IPY_MODEL_49b3ad660df54aa593c0ccef80bea50a","value":"Downloading (…)okenizer_config.json: 100%"}},"78774aa070d04130965f5f8905a505ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_62c7255f5e0a4fca8e3094870fa5ba3d","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e4bf294946d6441ab4ccc90cdd27c5a3","value":147}},"7ccfe49a405847f8a6db12c56aa3d894":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cef0f81ec1a481e8b87e53966df3fe6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84196de972124e91b9dbbb243973404c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8e129f17d42540019ba9cfe7adbd367f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8faa55e7ccd4480ca39bb5e1c747cf80":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9042349a3381442ab578fb761404a6fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50f008b3b3194997b8f2a86ba495910b","placeholder":"​","style":"IPY_MODEL_98f387b79422401d8406a341bd160766","value":" 792k/792k [00:00&lt;00:00, 2.90MB/s]"}},"98f387b79422401d8406a341bd160766":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f3d1d58fa6d49269b1910ce02b7ce45":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a470b738ffc64f7fbbadb3718c9560e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fd733f92ef63451ba15c491c2fd8058a","IPY_MODEL_e84ddf2936e3464fa336f5d699b03428","IPY_MODEL_9042349a3381442ab578fb761404a6fd"],"layout":"IPY_MODEL_ef3df86094274b29aecb10797f441eb6"}},"abd92d83ccdb4fa0bf36e7516d35a93c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f3d1d58fa6d49269b1910ce02b7ce45","placeholder":"​","style":"IPY_MODEL_c01e09f838bb46d1968661ca9fed3f37","value":"Downloading (…)lve/main/config.json: 100%"}},"aed90cf6cc7d4f17a07e626c228adca3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b095e0a1c1984fd0abf2abae4b8ff049":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0cf00768aed49c795cc231442c248c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b1e4308701ea40f7a62a22bcd9fe27df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5fabac8cd4f4d269cee5098b81f5564":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"beb28585adfd454c82dd994813f5b143":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c01e09f838bb46d1968661ca9fed3f37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c4a95c72abfa4f449fcdf2f4105d956f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c61afc6de45b4d6fb7db961275bdde27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_314f422561a14306a1492f998193eb16","placeholder":"​","style":"IPY_MODEL_721e706d762447c18d5bcfb274163a02","value":" 2.42M/2.42M [00:00&lt;00:00, 18.1MB/s]"}},"c8a96debca54492a964e2cb3a3e0a57a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e129f17d42540019ba9cfe7adbd367f","max":990345061,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5eb3141a8233462e861c99988b19289e","value":990345061}},"cd6de165c6d841e191742f3939cba1e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1a425818295408fa59a8d3acbccede7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2e43765f023467289c6df0dc26806bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c3a04d56ad145979ca22c463a0c4367","placeholder":"​","style":"IPY_MODEL_25fd776d3cd24eb4908be41929f0e2bb","value":" 1.40k/1.40k [00:00&lt;00:00, 62.8kB/s]"}},"d35d9a185cc04c1ebff2486a05ffe26b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4abae26745649c190327cf8e40d9911":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_58d5492177d042ec89b15688f4c16c19","IPY_MODEL_40297d74a3604357ac2bac071b64821b","IPY_MODEL_c61afc6de45b4d6fb7db961275bdde27"],"layout":"IPY_MODEL_2f468530885f4eed86b44ec50bba5a58"}},"db33f2165ac143eeac9df8de8a14c29b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1321396a2c64d32891c99a71b51510c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ab247fdf1c8488e809c98474211ada1","placeholder":"​","style":"IPY_MODEL_b5fabac8cd4f4d269cee5098b81f5564","value":"Downloading (…)neration_config.json: 100%"}},"e4bf294946d6441ab4ccc90cdd27c5a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e567e2883c90461db95ad6e3b3f24aac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_28bf244993e6470bb7332231038438d2","IPY_MODEL_c8a96debca54492a964e2cb3a3e0a57a","IPY_MODEL_3ed6bdc067ff4d62967753fd3ea73319"],"layout":"IPY_MODEL_0f08d5e4a2ff415d8dd87aef20e88493"}},"e84ddf2936e3464fa336f5d699b03428":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ccfe49a405847f8a6db12c56aa3d894","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_64d261c5cde64557a0d0a398d492877c","value":791656}},"ef3df86094274b29aecb10797f441eb6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc2cd9b9d3f94fe084626c01e20cc11d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd733f92ef63451ba15c491c2fd8058a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f693a8ad15b447d966255be73f529af","placeholder":"​","style":"IPY_MODEL_56676b88a3024b76bbea11a58ebcf99f","value":"Downloading spiece.model: 100%"}},"feed22d87090449a89644a961c1ad996":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
