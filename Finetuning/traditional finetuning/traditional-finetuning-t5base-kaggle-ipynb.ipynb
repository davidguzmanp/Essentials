{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom scipy.special import softmax\nimport pdb\nimport pandas as pd\nimport math\nfrom typing import List\nimport random\nimport argparse\nimport torch\n\n\ndef sent_scoring(model_tokenizer, text, cuda, score_type=\"loss\", output_attentions=False, length_normalize=False):\n    model = model_tokenizer[0]\n    tokenizer = model_tokenizer[1]\n    assert model is not None\n    assert tokenizer is not None\n    encoded_text = tokenizer.encode(text)\n    input_ids = torch.tensor(encoded_text).unsqueeze(0)\n    if cuda:\n        input_ids = input_ids.to('cuda')\n    with torch.no_grad():\n        outputs = model(input_ids, labels=input_ids, output_attentions=output_attentions)\n    loss, logits = outputs[:2]\n\n    sentence_prob = loss.item()\n    if score_type == \"prob\":\n        if length_normalize:\n            mult = 2\n        else:\n            mult = len(encoded_text)\n\n        sentence_prob = math.exp(-1.0 * loss * (mult - 1))\n\n    if output_attentions:\n        attn = outputs[\"attentions\"]\n        return sentence_prob, attn, input_ids\n\n    return sentence_prob\n\ndef confusion_matrix(P_forward_1, P_forward_2, P_backward_1, P_backward_2):\n    correct_forward = len(np.where(np.array(P_forward_1) >= 0.5)[0]) + len(np.where(np.array(P_forward_2) >=0.5)[0])\n    wrong_forward = len(P_forward_1) + len(P_forward_2) - correct_forward\n\n    correct_backward = len(np.where(np.array(P_backward_1) >= 0.5)[0]) + len(np.where(np.array(P_backward_2) >=0.5)[0])\n    wrong_backward = len(P_backward_1) + len(P_backward_2) - correct_backward\n\n    print(\"correct forward\", correct_forward, \"wrong forward\", wrong_forward, \"correct backward\", correct_backward, \"wrong_backward\", wrong_backward)\n\n    results = {\n        \"correct_forward\": correct_forward,\n        \"wrong_forward\": wrong_forward,\n        \"correct_backward\": correct_backward,\n        \"wrong_backward\": wrong_backward\n    }\n\n    return results\n\nfrom tqdm import tqdm\n\ndef evaluate_model(model, tokenizer, test_set, middle_phrase=\"\", use_prefix=0, verbose=True, score_type=\"prob\", use_cuda=False, return_acc=False, total = 1094) -> tuple:\n    preds = []\n    labels = []\n    x_1 = []\n    x_2 = []\n    y_1 = []\n    y_2 = []\n    P_x_1 = []\n    P_x_2 = []\n    P_y_1 = []\n    P_y_2 = []\n    P_x_1_y_1 = []\n    P_x_1_y_2 = []\n    P_x_2_y_1 = []\n    P_x_2_y_2 = []\n    P_x_1_correct = []\n    P_x_2_correct = []\n    P_y_1_correct = []\n    P_y_2_correct = []\n    correct = 0\n\n    for i, metaphor_data in tqdm(enumerate(test_set), total = total):\n        ctx, p1, p2 = metaphor_data[\"startphrase\"], metaphor_data[\"ending1\"], metaphor_data[\"ending2\"]\n        labels.append(int(metaphor_data[\"labels\"]))\n        if use_prefix > 0:\n            prefix_prompt = select_prefix_prompts(prompt_file, use_prefix) if use_prefix else \"\"\n        else:\n            prefix_prompt = \"\"\n\n        sent1 = prefix_prompt + ctx + \". \" + middle_phrase + p1 + \".\"\n        sent2 = prefix_prompt + ctx + \". \" + middle_phrase + p2 + \".\"\n\n        score1 = sent_scoring((model, tokenizer), sent1, use_cuda, score_type=score_type)\n        score2 = sent_scoring((model, tokenizer), sent2, use_cuda, score_type=score_type)\n\n        if score_type == \"loss\":\n            pred = 0 if score1 < score2 else 1\n        else:\n            pred = 1 if score1 < score2 else 0\n\n        pred_sent = sent1 if pred == 0 else sent2\n\n        if i % 2 == 0:\n            x_1.append(ctx)\n            x_1_score = sent_scoring((model, tokenizer), ctx + \".\", use_cuda, score_type=score_type)\n            P_x_1.append(x_1_score)\n            y_1.append(p1)\n            y_2.append(p2)\n            y1_score = sent_scoring((model, tokenizer), p1 + \".\", use_cuda, score_type=score_type)\n            y2_score = sent_scoring((model, tokenizer), p2 + \".\", use_cuda, score_type=score_type)\n            P_y_1.append(y1_score)\n            P_y_2.append(y2_score)\n\n            P_x_1_y_1.append(score1)\n            P_x_1_y_2.append(score2)\n            P_x_1_correct.append(score1/(score1 + score2))\n\n        else:\n            x_2.append(ctx)\n            x_2_score = sent_scoring((model, tokenizer), ctx + \".\", use_cuda, score_type=score_type)\n            P_x_2.append(x_2_score)\n            P_x_2_y_1.append(score1)\n            P_x_2_y_2.append(score2)\n            P_x_2_correct.append(score2/(score1 + score2))\n\n            P_y_1_correct.append(P_x_1_y_1[-1]/(P_x_1_y_1[-1] + score1))\n            P_y_2_correct.append(score2/(P_x_1_y_2[-1] + score2))\n\n        if verbose:\n            print(f\"Q: {ctx}: 1. {p1} 2. {p2}\")\n            print(f\"model says '{pred_sent}' is more likely\")\n            print(\"\\n\")\n        if pred == metaphor_data[\"labels\"]:\n            correct += 1\n        preds.append(pred)\n\n    cols = {\"x_1\": x_1, \"x_2\": x_2, \"y_1\": y_1, \"y_2\": y_2, \"P(x_1)\": P_x_1, \"P(x_2)\": P_x_2, \"P(y_1)\": P_y_1, \"P(y_2)\": P_y_2,\n        \"P(x_1, y_1)\": P_x_1_y_1, \"P(x_1, y_2)\": P_x_1_y_2, \"P(x_2, y_1)\": P_x_2_y_1, \"P(x_2, y_2)\": P_x_2_y_2,\n        \"P(y_1|x_1)\": P_x_1_correct, \"P(y_2|x_2)\": P_x_2_correct, \"P(x_1|y_1)\": P_y_1_correct, \"P(x_2|y_2)\": P_y_2_correct}\n    out_df = pd.DataFrame(cols)\n\n    if return_acc:\n        return correct/len(preds), out_df, preds, labels\n\n    return out_df, preds, labels\n\ndef compute_stats(total_df: pd.DataFrame, all_preds: List, all_labels: List) -> None:\n    print(\"overall accuracy: \")\n    accuracyy = len(np.where(np.array(all_preds) == np.array(all_labels))[0])/len(all_labels)\n    print(accuracyy)\n    print(\"confusion matrix: \")\n    matrix_dic = confusion_matrix(list(total_df[\"P(y_1|x_1)\"]), list(total_df[\"P(y_2|x_2)\"]), list(total_df[\"P(x_1|y_1)\"]), list(total_df[\"P(x_2|y_2)\"]))\n\n    return accuracyy, matrix_dic\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T00:09:44.328926Z","iopub.execute_input":"2023-10-12T00:09:44.329375Z","iopub.status.idle":"2023-10-12T00:09:48.449348Z","shell.execute_reply.started":"2023-10-12T00:09:44.329330Z","shell.execute_reply":"2023-10-12T00:09:48.448412Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n\n# Load T5 tokenizer and model\nmodel_name = \"t5-base\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-12T00:09:48.451052Z","iopub.execute_input":"2023-10-12T00:09:48.451747Z","iopub.status.idle":"2023-10-12T00:10:07.664261Z","shell.execute_reply.started":"2023-10-12T00:09:48.451711Z","shell.execute_reply":"2023-10-12T00:10:07.663138Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ae3c52758494ae88688bbf83e104c5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53dabc9eac0649ea85ec9a58f5f36c61"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:220: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  warnings.warn(\nYou are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0a0da0e417d4e47b1d04c72be9a337a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af6201e849684aeca54a53caf0c69a5a"}},"metadata":{}}]},{"cell_type":"code","source":"!git clone https://github.com/nightingal3/Fig-QA.git","metadata":{"execution":{"iopub.status.busy":"2023-10-12T00:10:07.675016Z","iopub.execute_input":"2023-10-12T00:10:07.675991Z","iopub.status.idle":"2023-10-12T00:10:09.793714Z","shell.execute_reply.started":"2023-10-12T00:10:07.675939Z","shell.execute_reply":"2023-10-12T00:10:09.792638Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Cloning into 'Fig-QA'...\nremote: Enumerating objects: 639, done.\u001b[K\nremote: Counting objects: 100% (208/208), done.\u001b[K\nremote: Compressing objects: 100% (119/119), done.\u001b[K\nremote: Total 639 (delta 130), reused 139 (delta 88), pack-reused 431\u001b[K\nReceiving objects: 100% (639/639), 2.81 MiB | 12.47 MiB/s, done.\nResolving deltas: 100% (353/353), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Define paths\ntrain_small_path = \"/kaggle/working/Fig-QA/data/filtered/train_s.csv\"\ntrain_path = \"/kaggle/working/Fig-QA/data/filtered/train.csv\"\nmturk_path = \"/kaggle/working/Fig-QA/data/filtered/mturk_processed - combined.csv\"\ntest_path = \"/kaggle/working/Fig-QA/data/filtered/test.csv\"\ntrain_xl_path = \"/kaggle/working/Fig-QA/data/filtered/train_xl.csv\"\noriginal_data_path = \"/kaggle/working/Fig-QA/data/filtered/original_data.csv\"\ndev_path = \"/kaggle/working/Fig-QA/data/filtered/dev.csv\"\n\n# Load CSV files into dataframes\ntrain_small_df = pd.read_csv(train_small_path)\ntrain_df = pd.read_csv(train_path)\nmturk_df = pd.read_csv(mturk_path)\ntest_df = pd.read_csv(test_path)\ntrain_xl_df = pd.read_csv(train_xl_path)\noriginal_data_df = pd.read_csv(original_data_path)\ndev_df = pd.read_csv(dev_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T00:10:09.795632Z","iopub.execute_input":"2023-10-12T00:10:09.795991Z","iopub.status.idle":"2023-10-12T00:10:09.852300Z","shell.execute_reply.started":"2023-10-12T00:10:09.795954Z","shell.execute_reply":"2023-10-12T00:10:09.851458Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_small_dataset = Dataset.from_pandas(train_small_df)\ntrain_dataset = Dataset.from_pandas(train_df)\nmturk_dataset = Dataset.from_pandas(mturk_df)\ntest_dataset = Dataset.from_pandas(test_df)\ntrain_xl_dataset = Dataset.from_pandas(train_xl_df)\noriginal_data_dataset = Dataset.from_pandas(original_data_df)\ndev_dataset = Dataset.from_pandas(dev_df)\n\ndatasets = {\n    \"train_small\": train_small_dataset,\n    \"train\": train_dataset,\n    \"mturk\": mturk_dataset,\n    \"test\": test_dataset,\n    \"train_xl\": train_xl_dataset,\n    \"original_data\": original_data_dataset,\n    \"dev\": dev_dataset,\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T00:10:09.853656Z","iopub.execute_input":"2023-10-12T00:10:09.854222Z","iopub.status.idle":"2023-10-12T00:10:09.902022Z","shell.execute_reply.started":"2023-10-12T00:10:09.854189Z","shell.execute_reply":"2023-10-12T00:10:09.901215Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dev_dataset[0]\nsubset_test_dataset = dev_dataset.select(range(30))","metadata":{"execution":{"iopub.status.busy":"2023-10-12T00:10:09.903151Z","iopub.execute_input":"2023-10-12T00:10:09.903690Z","iopub.status.idle":"2023-10-12T00:10:09.920982Z","shell.execute_reply.started":"2023-10-12T00:10:09.903658Z","shell.execute_reply":"2023-10-12T00:10:09.919988Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"subset_test_dataset = dev_dataset.select(range(30))\nout_df, preds, labels = evaluate_model(model, tokenizer, subset_test_dataset, verbose = False, total = 30)\nzero_shot_accuracy, conf_matrix_zero_shot =  compute_stats(out_df, preds, labels)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T00:10:09.922858Z","iopub.execute_input":"2023-10-12T00:10:09.923523Z","iopub.status.idle":"2023-10-12T00:10:45.566034Z","shell.execute_reply.started":"2023-10-12T00:10:09.923490Z","shell.execute_reply":"2023-10-12T00:10:45.565079Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 30/30 [00:35<00:00,  1.19s/it]","output_type":"stream"},{"name":"stdout","text":"overall accuracy: \n0.5666666666666667\nconfusion matrix: \ncorrect forward 17 wrong forward 13 correct backward 14 wrong_backward 16\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"def map_concatenation_and_tokenization(samples):\n    concatenated_phrases = []\n    input_ids_list = []\n    attention_mask_list = []\n\n    for i in range(len(samples['startphrase'])):\n        # Choose the ending based on the labels value for each sample in the batch\n        ending = samples['ending1'][i] if samples['labels'][i] == 0 else samples['ending2'][i]\n        concatenated_phrase = samples['startphrase'][i] + ' -> ' + ending\n        concatenated_phrases.append(concatenated_phrase)\n\n        # Tokenize the concatenated_phrase\n        tokens = tokenizer(concatenated_phrase, truncation=True, max_length=512, return_tensors='pt')\n        input_ids_list.append(tokens['input_ids'][0].tolist())\n        attention_mask_list.append(tokens['attention_mask'][0].tolist())\n\n    return {\n        'concatenated_phrase': concatenated_phrases,\n        'input_ids': input_ids_list,\n        'attention_mask': attention_mask_list\n    }\n\n# def map_concatenation_and_tokenization(samples):\n#     concatenated_phrases = []\n#     input_ids_list = []\n#     attention_mask_list = []\n\n#     for i in range(len(samples['startphrase'])):\n#         # Decide which ending is the correct and which is the wrong based on the 'labels' value\n#         correct_ending = samples['ending1'][i] if samples['labels'][i] == 0 else samples['ending2'][i]\n#         wrong_ending = samples['ending2'][i] if samples['labels'][i] == 0 else samples['ending1'][i]\n\n#         concatenated_phrase = samples['startphrase'][i] + \" That means that: \" + correct_ending + \" It is antonymous with: \" + wrong_ending\n#         concatenated_phrases.append(concatenated_phrase)\n\n#         # Tokenize the concatenated_phrase\n#         tokens = tokenizer(concatenated_phrase, truncation=True, max_length=512, return_tensors='pt')\n#         input_ids_list.append(tokens['input_ids'][0].tolist())\n#         attention_mask_list.append(tokens['attention_mask'][0].tolist())\n\n#     return {\n#         'concatenated_phrase': concatenated_phrases,\n#         'input_ids': input_ids_list,\n#         'attention_mask': attention_mask_list }\n\n# Apply the mapping function\n\ndata = train_xl_dataset.map(map_concatenation_and_tokenization, batched=True)\ndata_val = dev_dataset.map(map_concatenation_and_tokenization, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T00:10:45.567493Z","iopub.execute_input":"2023-10-12T00:10:45.567833Z","iopub.status.idle":"2023-10-12T00:10:47.896362Z","shell.execute_reply.started":"2023-10-12T00:10:45.567799Z","shell.execute_reply":"2023-10-12T00:10:47.895454Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f1193387de24127850cc45aa272af7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c7fa380955f4bc3be1ebf050f8404bb"}},"metadata":{}}]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-10-12T00:10:47.899475Z","iopub.execute_input":"2023-10-12T00:10:47.899857Z","iopub.status.idle":"2023-10-12T00:10:47.906583Z","shell.execute_reply.started":"2023-10-12T00:10:47.899833Z","shell.execute_reply":"2023-10-12T00:10:47.905656Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['startphrase', 'ending1', 'ending2', 'labels', 'valid', 'qid', 'concatenated_phrase', 'input_ids', 'attention_mask'],\n    num_rows: 8016\n})"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data['input_ids'])\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.data['input_ids'][idx],\n            'attention_mask': self.data['attention_mask'][idx],\n            'labels': self.data['input_ids'][idx]  # In T5, labels are the same as input_ids for the decoder.\n        }\n\ntrain_dataset = CustomDataset(data)\nval_dataset = CustomDataset(data_val)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T00:10:47.907837Z","iopub.execute_input":"2023-10-12T00:10:47.908797Z","iopub.status.idle":"2023-10-12T00:10:47.917654Z","shell.execute_reply.started":"2023-10-12T00:10:47.908752Z","shell.execute_reply":"2023-10-12T00:10:47.916783Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def custom_data_collator(batch):\n    # Find the maximum length of sequences in the batch for padding\n    max_length = max([len(item['input_ids']) for item in batch])\n    \n    # Pad each sequence to the max_length\n    input_ids = [item['input_ids'] + [tokenizer.pad_token_id] * (max_length - len(item['input_ids'])) for item in batch]\n    attention_mask = [item['attention_mask'] + [0] * (max_length - len(item['attention_mask'])) for item in batch]\n    labels = [item['labels'] + [tokenizer.pad_token_id] * (max_length - len(item['labels'])) for item in batch]\n    \n    return {\n        'input_ids': torch.tensor(input_ids),\n        'attention_mask': torch.tensor(attention_mask),\n        'labels': torch.tensor(labels)\n    }","metadata":{"execution":{"iopub.status.busy":"2023-10-12T00:10:47.918758Z","iopub.execute_input":"2023-10-12T00:10:47.919821Z","iopub.status.idle":"2023-10-12T00:10:47.934213Z","shell.execute_reply.started":"2023-10-12T00:10:47.919785Z","shell.execute_reply":"2023-10-12T00:10:47.933345Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n        per_device_train_batch_size=20,\n        gradient_accumulation_steps=4,\n        warmup_steps=2,\n        max_steps= 30,\n        #num_train_epochs=2,\n        eval_steps= 1,\n        learning_rate= 0.0001,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        #evaluation_strategy=\"steps\"\n    )\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator= custom_data_collator\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-10-12T00:10:47.935751Z","iopub.execute_input":"2023-10-12T00:10:47.936519Z","iopub.status.idle":"2023-10-12T00:33:09.300986Z","shell.execute_reply.started":"2023-10-12T00:10:47.936488Z","shell.execute_reply":"2023-10-12T00:33:09.300071Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231012_001144-4a3a91a9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/davinci/huggingface/runs/4a3a91a9' target=\"_blank\">skilled-forest-7</a></strong> to <a href='https://wandb.ai/davinci/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/davinci/huggingface' target=\"_blank\">https://wandb.ai/davinci/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/davinci/huggingface/runs/4a3a91a9' target=\"_blank\">https://wandb.ai/davinci/huggingface/runs/4a3a91a9</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30/30 19:53, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>7.892400</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>7.251100</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>7.478900</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>6.292500</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>4.553000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>3.758800</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>3.333500</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.966400</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.009700</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.830700</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.329100</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.003500</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.877700</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.716900</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.603900</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.497100</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.408300</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.383700</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.359000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.337200</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.313600</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.276200</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.240600</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.207400</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.230700</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.184200</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.181300</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.171000</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.161300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.165200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=30, training_loss=1.8671572382251422, metrics={'train_runtime': 1334.3516, 'train_samples_per_second': 3.597, 'train_steps_per_second': 0.022, 'total_flos': 209234125209600.0, 'train_loss': 1.8671572382251422, 'epoch': 0.6})"},"metadata":{}}]},{"cell_type":"code","source":"model.to('cpu')\nout_df, preds, labels = evaluate_model(model, tokenizer, subset_test_dataset, verbose = False, total = 30)\nzero_shot_accuracy, conf_matrix_zero_shot =  compute_stats(out_df, preds, labels)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T00:33:09.302416Z","iopub.execute_input":"2023-10-12T00:33:09.306332Z","iopub.status.idle":"2023-10-12T00:33:46.475244Z","shell.execute_reply.started":"2023-10-12T00:33:09.306292Z","shell.execute_reply":"2023-10-12T00:33:46.474392Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 30/30 [00:36<00:00,  1.21s/it]","output_type":"stream"},{"name":"stdout","text":"overall accuracy: \n0.36666666666666664\nconfusion matrix: \ncorrect forward 11 wrong forward 19 correct backward 10 wrong_backward 20\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}