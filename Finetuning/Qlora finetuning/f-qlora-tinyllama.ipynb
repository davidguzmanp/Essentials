{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"e585dc4361fc4f5c9f395e14c92c1ca0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc54e89c06f549759a55e7c3db9e3501","IPY_MODEL_757c2d29a718400ea309acdb3e8e5113","IPY_MODEL_11b8af75eedc4482866a222372219815"],"layout":"IPY_MODEL_9dbc05f106ae48a79f74f7fdcce64dae"}},"fc54e89c06f549759a55e7c3db9e3501":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_001925813a8044e0a73b759d787db5ff","placeholder":"​","style":"IPY_MODEL_19448306978749d39a1e08d6b55082dc","value":"Downloading pytorch_model.bin: 100%"}},"757c2d29a718400ea309acdb3e8e5113":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_42b28bb1abb349b481aa99d10852f478","max":2836623617,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8788661831d343e491c23cef0849a688","value":2836623617}},"11b8af75eedc4482866a222372219815":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5596352cfa7465f87dd5e6732ba7cae","placeholder":"​","style":"IPY_MODEL_219f98fc7e0b4ab296fc914bb679b956","value":" 2.84G/2.84G [00:26&lt;00:00, 135MB/s]"}},"9dbc05f106ae48a79f74f7fdcce64dae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"001925813a8044e0a73b759d787db5ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19448306978749d39a1e08d6b55082dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42b28bb1abb349b481aa99d10852f478":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8788661831d343e491c23cef0849a688":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c5596352cfa7465f87dd5e6732ba7cae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"219f98fc7e0b4ab296fc914bb679b956":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f84b48669ef4049bdd40a540f2529f0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c559a294d33742e390116907662f0abb","IPY_MODEL_cc229d3e170242a7b9b3a21567d83818","IPY_MODEL_062d0521aa0a4ca98ee983eefd65506f"],"layout":"IPY_MODEL_23d4e2d8daa440bdb37e7c2ebdc5db02"}},"c559a294d33742e390116907662f0abb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f947befc04d442aea7dfb25613bd9939","placeholder":"​","style":"IPY_MODEL_7c51c6f5d1344354963f811bf9a3bc06","value":"Downloading (…)neration_config.json: 100%"}},"cc229d3e170242a7b9b3a21567d83818":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d957b3a701084f8aafb9102a6238788c","max":69,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2864b5ed0a2d4955a83cbf6618e8af96","value":69}},"062d0521aa0a4ca98ee983eefd65506f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75c42ef8bd2b4b7fa72fd4803af9a603","placeholder":"​","style":"IPY_MODEL_13a0c04afa044ee1a122ecd6ff240e7f","value":" 69.0/69.0 [00:00&lt;00:00, 4.94kB/s]"}},"23d4e2d8daa440bdb37e7c2ebdc5db02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f947befc04d442aea7dfb25613bd9939":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c51c6f5d1344354963f811bf9a3bc06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d957b3a701084f8aafb9102a6238788c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2864b5ed0a2d4955a83cbf6618e8af96":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"75c42ef8bd2b4b7fa72fd4803af9a603":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13a0c04afa044ee1a122ecd6ff240e7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98351baf42934fe5b7524b3b64accbd8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d0671ee3f234656b8a2627389e07e9d","IPY_MODEL_15b50e5a174a40c3b7c0dd24943b80c0","IPY_MODEL_41f80043500046d79f105206a1c91610"],"layout":"IPY_MODEL_737604363ab84acab1980304dfae18b7"}},"3d0671ee3f234656b8a2627389e07e9d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d5ce9cf72a741508b0044e9d36b36f9","placeholder":"​","style":"IPY_MODEL_5fc16426f4c04d54acc2f2e0457a6416","value":"Downloading readme: 100%"}},"15b50e5a174a40c3b7c0dd24943b80c0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e690820d66e4416ba819ebbef9f2909e","max":5554,"min":0,"orientation":"horizontal","style":"IPY_MODEL_42716af8d24146c4b29393ddc2dc6a8a","value":5554}},"41f80043500046d79f105206a1c91610":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03421596b8f343eaafd392d951860431","placeholder":"​","style":"IPY_MODEL_71eda188275c493b89b4a2dcdbf3f336","value":" 5.55k/5.55k [00:00&lt;00:00, 169kB/s]"}},"737604363ab84acab1980304dfae18b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d5ce9cf72a741508b0044e9d36b36f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fc16426f4c04d54acc2f2e0457a6416":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e690820d66e4416ba819ebbef9f2909e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42716af8d24146c4b29393ddc2dc6a8a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"03421596b8f343eaafd392d951860431":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71eda188275c493b89b4a2dcdbf3f336":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e3dc5d6c0b540ccb7f7afa134e43dcf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cf463801021a47f284c7e0d3acfc8d63","IPY_MODEL_3e763d4588cd492baf6f5dcfbab241cc","IPY_MODEL_3c093130754d4825b2d1772e7ae1f2ae"],"layout":"IPY_MODEL_c50c4ada3b8f46429028c0fb11bb856a"}},"cf463801021a47f284c7e0d3acfc8d63":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24abc37e54ba41669611b618d77b332d","placeholder":"​","style":"IPY_MODEL_bc02bb96ad494682a4443818bf9e2b36","value":"Downloading data files: 100%"}},"3e763d4588cd492baf6f5dcfbab241cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_32c4670df43042e9b5258bf33dff8c7e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d4bb2ee46b2c489e954139c44fc9a192","value":1}},"3c093130754d4825b2d1772e7ae1f2ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4748ce5b31647e3ad85c750a7862e49","placeholder":"​","style":"IPY_MODEL_010e9589a46941dd83eb4cac8770a626","value":" 1/1 [00:00&lt;00:00,  2.95it/s]"}},"c50c4ada3b8f46429028c0fb11bb856a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24abc37e54ba41669611b618d77b332d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc02bb96ad494682a4443818bf9e2b36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"32c4670df43042e9b5258bf33dff8c7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4bb2ee46b2c489e954139c44fc9a192":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f4748ce5b31647e3ad85c750a7862e49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"010e9589a46941dd83eb4cac8770a626":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3286bad3a57445e9a41b81ea027e631d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_72abdfd49b8e4960b22d25e0615b6a6c","IPY_MODEL_76377837c9e346efaf05625d9ec9782d","IPY_MODEL_968a0fcf5fad48028404e342f677e664"],"layout":"IPY_MODEL_33e0587bb0b24accb05c16cc02d75b3d"}},"72abdfd49b8e4960b22d25e0615b6a6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7c0ccb695a64e6ba4bc08de65ac7739","placeholder":"​","style":"IPY_MODEL_c19ffd16e845410bbe563cecb1b3d612","value":"Downloading data: 100%"}},"76377837c9e346efaf05625d9ec9782d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a88eec22474455d80a541f1afc10dd6","max":646739,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cff47dc83ead4436903d9b651b9b4649","value":646739}},"968a0fcf5fad48028404e342f677e664":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5967538111e4487b8a49e40189f97c31","placeholder":"​","style":"IPY_MODEL_b464aa7530da497791ce86ae00bdc07e","value":" 647k/647k [00:00&lt;00:00, 2.11MB/s]"}},"33e0587bb0b24accb05c16cc02d75b3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7c0ccb695a64e6ba4bc08de65ac7739":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c19ffd16e845410bbe563cecb1b3d612":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a88eec22474455d80a541f1afc10dd6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cff47dc83ead4436903d9b651b9b4649":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5967538111e4487b8a49e40189f97c31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b464aa7530da497791ce86ae00bdc07e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a18b440ce6fe49f68a92cac0f0e1c2fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f394fcda1fd4f39957d14a5c4caa631","IPY_MODEL_06352b37a3de45199d28f0710e31742f","IPY_MODEL_9408db961369449a9f1716a185ac6e7d"],"layout":"IPY_MODEL_1ab6fddb871b4889bad1850aeb911e42"}},"5f394fcda1fd4f39957d14a5c4caa631":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d3cdf0730754282af4d57e8db5aef80","placeholder":"​","style":"IPY_MODEL_6fc4c705d1214cc193bf64963121e81b","value":"Extracting data files: 100%"}},"06352b37a3de45199d28f0710e31742f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9fcdf6c2ce5f4dcdb97e270d25d08a0d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7cdf6b8039e1414ca97892214106a41d","value":1}},"9408db961369449a9f1716a185ac6e7d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2bcfde8fbc1462b9a7f2334f592e615","placeholder":"​","style":"IPY_MODEL_ccc72c8249a04775b61d24a9edf914f9","value":" 1/1 [00:00&lt;00:00, 37.26it/s]"}},"1ab6fddb871b4889bad1850aeb911e42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d3cdf0730754282af4d57e8db5aef80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fc4c705d1214cc193bf64963121e81b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9fcdf6c2ce5f4dcdb97e270d25d08a0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cdf6b8039e1414ca97892214106a41d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f2bcfde8fbc1462b9a7f2334f592e615":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccc72c8249a04775b61d24a9edf914f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9bbfdae62be4fb9a4343c3dfd2c557a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d2c2407dec094aef82681b44ec2e3d2f","IPY_MODEL_9e8a6ae9602f48dbb83083b72ca53e1d","IPY_MODEL_899f4a2877e949b6a2117b81c9aed8a5"],"layout":"IPY_MODEL_60a2c67412904f369f8c046ab8ca29e2"}},"d2c2407dec094aef82681b44ec2e3d2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f99b02cc6f1149c08fadf7959c5c2685","placeholder":"​","style":"IPY_MODEL_fb8017a9dccb4103a6e4a012053e0fec","value":"Generating train split: "}},"9e8a6ae9602f48dbb83083b72ca53e1d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_96d68d033c774137a3db337e978d83a5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_990018b5851242c2a93a39761cd44e8c","value":1}},"899f4a2877e949b6a2117b81c9aed8a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02676ed015414b7bad8f234881fd1fe7","placeholder":"​","style":"IPY_MODEL_13bfd884d21e4579855fc8acf09533a8","value":" 2508/0 [00:00&lt;00:00, 48879.07 examples/s]"}},"60a2c67412904f369f8c046ab8ca29e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f99b02cc6f1149c08fadf7959c5c2685":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb8017a9dccb4103a6e4a012053e0fec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96d68d033c774137a3db337e978d83a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"990018b5851242c2a93a39761cd44e8c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"02676ed015414b7bad8f234881fd1fe7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13bfd884d21e4579855fc8acf09533a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6df57c6325c74747be601eb8b75d1dd5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5fef097a0cbe41d2bd7a33e694a82b5a","IPY_MODEL_426040f036ac497d83a8af02a1a26a63","IPY_MODEL_b3af7e680e6645a496bd0b4a48776fbb"],"layout":"IPY_MODEL_01f89584ce88486d9405e71c14817b37"}},"5fef097a0cbe41d2bd7a33e694a82b5a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b743164f8184e0cafc84c9883d8729d","placeholder":"​","style":"IPY_MODEL_388e42c3747c44fe95febfb0a495982a","value":"Map: 100%"}},"426040f036ac497d83a8af02a1a26a63":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31fa3a0b8ff84732aaa2ed4032373a4b","max":2508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0016ad3003c0499fb29df618377d5632","value":2508}},"b3af7e680e6645a496bd0b4a48776fbb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_680f8d845c0646b090e13fa5b25ae971","placeholder":"​","style":"IPY_MODEL_f7f5cabdccc84b3b83f9d93e5e03a5c0","value":" 2508/2508 [00:00&lt;00:00, 4393.26 examples/s]"}},"01f89584ce88486d9405e71c14817b37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b743164f8184e0cafc84c9883d8729d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"388e42c3747c44fe95febfb0a495982a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31fa3a0b8ff84732aaa2ed4032373a4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0016ad3003c0499fb29df618377d5632":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"680f8d845c0646b090e13fa5b25ae971":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7f5cabdccc84b3b83f9d93e5e03a5c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# `transformers` meets `bitsandbytes` for democratzing Large Language Models (LLMs) through 4bit quantization\n\n<center>\n<img src=\"https://github.com/huggingface/blog/blob/main/assets/96_hf_bitsandbytes_integration/Thumbnail_blue.png?raw=true\" alt=\"drawing\" width=\"700\" class=\"center\"/>\n</center>\n\nWelcome to this notebook that goes through the recent `bitsandbytes` integration that includes the work from XXX that introduces no performance degradation 4bit quantization techniques, for democratizing LLMs inference and training.\n\nIn this notebook, we will learn together how to load a large model in 4bit (`gpt-neo-x-20b`) and train it using Google Colab and PEFT library from Hugging Face 🤗.\n\n[In the general usage notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing), you can learn how to propely load a model in 4bit with all its variants.\n\nIf you liked the previous work for integrating [*LLM.int8*](https://arxiv.org/abs/2208.07339), you can have a look at the [introduction blogpost](https://huggingface.co/blog/hf-bitsandbytes-integration) to lean more about that quantization method.\n","metadata":{"id":"XIyP_0r6zuVc"}},{"cell_type":"code","source":"import numpy as np\nfrom scipy.special import softmax\nimport pdb\nimport pandas as pd\nimport math\nfrom typing import List\nimport random\nimport argparse\nimport torch\n\n\ndef sent_scoring(model_tokenizer, text, cuda, score_type=\"loss\", output_attentions=False, length_normalize=False):\n    model = model_tokenizer[0]\n    tokenizer = model_tokenizer[1]\n    assert model is not None\n    assert tokenizer is not None\n    encoded_text = tokenizer.encode(text)\n    input_ids = torch.tensor(encoded_text).unsqueeze(0)\n    if cuda:\n        input_ids = input_ids.to('cuda')\n    with torch.no_grad():\n        outputs = model(input_ids, labels=input_ids, output_attentions=output_attentions)\n    loss, logits = outputs[:2]\n\n    sentence_prob = loss.item()\n    if score_type == \"prob\":\n        if length_normalize:\n            mult = 2\n        else:\n            mult = len(encoded_text)\n\n        sentence_prob = math.exp(-1.0 * loss * (mult - 1))\n\n    if output_attentions:\n        attn = outputs[\"attentions\"]\n        return sentence_prob, attn, input_ids\n\n    return sentence_prob\n\ndef confusion_matrix(P_forward_1, P_forward_2, P_backward_1, P_backward_2):\n    correct_forward = len(np.where(np.array(P_forward_1) >= 0.5)[0]) + len(np.where(np.array(P_forward_2) >=0.5)[0])\n    wrong_forward = len(P_forward_1) + len(P_forward_2) - correct_forward\n\n    correct_backward = len(np.where(np.array(P_backward_1) >= 0.5)[0]) + len(np.where(np.array(P_backward_2) >=0.5)[0])\n    wrong_backward = len(P_backward_1) + len(P_backward_2) - correct_backward\n\n    print(\"correct forward\", correct_forward, \"wrong forward\", wrong_forward, \"correct backward\", correct_backward, \"wrong_backward\", wrong_backward)\n\n    results = {\n        \"correct_forward\": correct_forward,\n        \"wrong_forward\": wrong_forward,\n        \"correct_backward\": correct_backward,\n        \"wrong_backward\": wrong_backward\n    }\n\n    return results\n\nfrom tqdm import tqdm\n\ndef evaluate_model(model, tokenizer, test_set, middle_phrase=\"\", use_prefix=0, verbose=True, score_type=\"prob\", use_cuda=False, return_acc=False, total = 1094) -> tuple:\n    preds = []\n    labels = []\n    x_1 = []\n    x_2 = []\n    y_1 = []\n    y_2 = []\n    P_x_1 = []\n    P_x_2 = []\n    P_y_1 = []\n    P_y_2 = []\n    P_x_1_y_1 = []\n    P_x_1_y_2 = []\n    P_x_2_y_1 = []\n    P_x_2_y_2 = []\n    P_x_1_correct = []\n    P_x_2_correct = []\n    P_y_1_correct = []\n    P_y_2_correct = []\n    correct = 0\n\n    for i, metaphor_data in tqdm(enumerate(test_set), total = total):\n        ctx, p1, p2 = metaphor_data[\"startphrase\"], metaphor_data[\"ending1\"], metaphor_data[\"ending2\"]\n        labels.append(int(metaphor_data[\"labels\"]))\n        if use_prefix > 0:\n            prefix_prompt = select_prefix_prompts(prompt_file, use_prefix) if use_prefix else \"\"\n        else:\n            prefix_prompt = \"\"\n\n        sent1 = prefix_prompt + ctx + \". \" + middle_phrase + p1 + \".\"\n        sent2 = prefix_prompt + ctx + \". \" + middle_phrase + p2 + \".\"\n\n        score1 = sent_scoring((model, tokenizer), sent1, use_cuda, score_type=score_type)\n        score2 = sent_scoring((model, tokenizer), sent2, use_cuda, score_type=score_type)\n\n        if score_type == \"loss\":\n            pred = 0 if score1 < score2 else 1\n        else:\n            pred = 1 if score1 < score2 else 0\n\n        pred_sent = sent1 if pred == 0 else sent2\n\n        if i % 2 == 0:\n            x_1.append(ctx)\n            x_1_score = sent_scoring((model, tokenizer), ctx + \".\", use_cuda, score_type=score_type)\n            P_x_1.append(x_1_score)\n            y_1.append(p1)\n            y_2.append(p2)\n            y1_score = sent_scoring((model, tokenizer), p1 + \".\", use_cuda, score_type=score_type)\n            y2_score = sent_scoring((model, tokenizer), p2 + \".\", use_cuda, score_type=score_type)\n            P_y_1.append(y1_score)\n            P_y_2.append(y2_score)\n\n            P_x_1_y_1.append(score1)\n            P_x_1_y_2.append(score2)\n            P_x_1_correct.append(score1/(score1 + score2))\n\n        else:\n            x_2.append(ctx)\n            x_2_score = sent_scoring((model, tokenizer), ctx + \".\", use_cuda, score_type=score_type)\n            P_x_2.append(x_2_score)\n            P_x_2_y_1.append(score1)\n            P_x_2_y_2.append(score2)\n            P_x_2_correct.append(score2/(score1 + score2))\n\n            P_y_1_correct.append(P_x_1_y_1[-1]/(P_x_1_y_1[-1] + score1))\n            P_y_2_correct.append(score2/(P_x_1_y_2[-1] + score2))\n\n        if verbose:\n            print(f\"Q: {ctx}: 1. {p1} 2. {p2}\")\n            print(f\"model says '{pred_sent}' is more likely\")\n            print(\"\\n\")\n        if pred == metaphor_data[\"labels\"]:\n            correct += 1\n        preds.append(pred)\n\n    cols = {\"x_1\": x_1, \"x_2\": x_2, \"y_1\": y_1, \"y_2\": y_2, \"P(x_1)\": P_x_1, \"P(x_2)\": P_x_2, \"P(y_1)\": P_y_1, \"P(y_2)\": P_y_2,\n        \"P(x_1, y_1)\": P_x_1_y_1, \"P(x_1, y_2)\": P_x_1_y_2, \"P(x_2, y_1)\": P_x_2_y_1, \"P(x_2, y_2)\": P_x_2_y_2,\n        \"P(y_1|x_1)\": P_x_1_correct, \"P(y_2|x_2)\": P_x_2_correct, \"P(x_1|y_1)\": P_y_1_correct, \"P(x_2|y_2)\": P_y_2_correct}\n    out_df = pd.DataFrame(cols)\n\n    if return_acc:\n        return correct/len(preds), out_df, preds, labels\n\n    return out_df, preds, labels\n\ndef compute_stats(total_df: pd.DataFrame, all_preds: List, all_labels: List) -> None:\n    print(\"overall accuracy: \")\n    accuracyy = len(np.where(np.array(all_preds) == np.array(all_labels))[0])/len(all_labels)\n    print(accuracyy)\n    print(\"confusion matrix: \")\n    matrix_dic = confusion_matrix(list(total_df[\"P(y_1|x_1)\"]), list(total_df[\"P(y_2|x_2)\"]), list(total_df[\"P(x_1|y_1)\"]), list(total_df[\"P(x_2|y_2)\"]))\n\n    return accuracyy, matrix_dic\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall datasets -y\n!pip install datasets","metadata":{"id":"cheX4_C2xz2X","colab":{"base_uri":"https://localhost:8080/"},"outputId":"51ff543c-882d-45a4-e755-2d485ba80717","execution":{"iopub.status.busy":"2023-10-12T14:06:08.005078Z","iopub.execute_input":"2023-10-12T14:06:08.005490Z","iopub.status.idle":"2023-10-12T14:06:22.026899Z","shell.execute_reply.started":"2023-10-12T14:06:08.005466Z","shell.execute_reply":"2023-10-12T14:06:22.025656Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found existing installation: datasets 2.1.0\nUninstalling datasets-2.1.0:\n  Successfully uninstalled datasets-2.1.0\nCollecting datasets\n  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nCollecting fsspec[http]<2023.9.0,>=2023.1.0 (from datasets)\n  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nInstalling collected packages: fsspec, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.9.0\n    Uninstalling fsspec-2023.9.0:\n      Successfully uninstalled fsspec-2023.9.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ns3fs 2023.9.0 requires fsspec==2023.9.0, but you have fsspec 2023.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.14.5 fsspec-2023.6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install sentencepiece","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nlFxRGVGybOu","outputId":"f92f2906-456b-44cf-9b33-ccc485c3c835","execution":{"iopub.status.busy":"2023-10-12T14:06:22.028621Z","iopub.execute_input":"2023-10-12T14:06:22.029035Z","iopub.status.idle":"2023-10-12T14:06:31.400954Z","shell.execute_reply.started":"2023-10-12T14:06:22.028999Z","shell.execute_reply":"2023-10-12T14:06:31.399723Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q -U transformers","metadata":{"id":"hfgefLMDylI9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"07fa79e1-55d1-411e-cfc3-a3451ef43ddb","execution":{"iopub.status.busy":"2023-10-12T14:06:31.403902Z","iopub.execute_input":"2023-10-12T14:06:31.404600Z","iopub.status.idle":"2023-10-12T14:06:51.273716Z","shell.execute_reply.started":"2023-10-12T14:06:31.404560Z","shell.execute_reply":"2023-10-12T14:06:51.272294Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q datasets","metadata":{"id":"FuXIFTFapAMI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0a60b09e-966b-47f5-9276-08a9134a71cd","execution":{"iopub.status.busy":"2023-10-12T14:06:51.275428Z","iopub.execute_input":"2023-10-12T14:06:51.276499Z","iopub.status.idle":"2023-10-12T14:08:30.233616Z","shell.execute_reply.started":"2023-10-12T14:06:51.276463Z","shell.execute_reply":"2023-10-12T14:08:30.232310Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"id":"ddJWcW-y3suD","execution":{"iopub.status.busy":"2023-10-12T14:08:30.235595Z","iopub.execute_input":"2023-10-12T14:08:30.235953Z","iopub.status.idle":"2023-10-12T14:08:31.394030Z","shell.execute_reply.started":"2023-10-12T14:08:30.235920Z","shell.execute_reply":"2023-10-12T14:08:31.393081Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!pip install -q einops","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nmodel_id = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})","metadata":{"id":"E0Nl5mWL0k2T","colab":{"base_uri":"https://localhost:8080/","height":262,"referenced_widgets":["e585dc4361fc4f5c9f395e14c92c1ca0","fc54e89c06f549759a55e7c3db9e3501","757c2d29a718400ea309acdb3e8e5113","11b8af75eedc4482866a222372219815","9dbc05f106ae48a79f74f7fdcce64dae","001925813a8044e0a73b759d787db5ff","19448306978749d39a1e08d6b55082dc","42b28bb1abb349b481aa99d10852f478","8788661831d343e491c23cef0849a688","c5596352cfa7465f87dd5e6732ba7cae","219f98fc7e0b4ab296fc914bb679b956","6f84b48669ef4049bdd40a540f2529f0","c559a294d33742e390116907662f0abb","cc229d3e170242a7b9b3a21567d83818","062d0521aa0a4ca98ee983eefd65506f","23d4e2d8daa440bdb37e7c2ebdc5db02","f947befc04d442aea7dfb25613bd9939","7c51c6f5d1344354963f811bf9a3bc06","d957b3a701084f8aafb9102a6238788c","2864b5ed0a2d4955a83cbf6618e8af96","75c42ef8bd2b4b7fa72fd4803af9a603","13a0c04afa044ee1a122ecd6ff240e7f"]},"outputId":"62648f23-282b-43ac-e123-94e1dea6a730","execution":{"iopub.status.busy":"2023-10-12T15:00:12.825587Z","iopub.execute_input":"2023-10-12T15:00:12.826724Z","iopub.status.idle":"2023-10-12T15:07:55.269628Z","shell.execute_reply.started":"2023-10-12T15:00:12.826678Z","shell.execute_reply":"2023-10-12T15:07:55.266490Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5a5b3b557a442ecbca3712cb3ccd20d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c982a1db9ce2427e9decbffb2c0d2918"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63ba4a1a475e49058c62be9dccb5bd98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)in/added_tokens.json:   0%|          | 0.00/69.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"848dcd978b4146329aa65afde2e1813e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f1aaa89513a469db26f6ab68e69fb65"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"424320457eab4c89af34cda522c2efb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/4.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8f3e7ff83cd4143b16909ed263afc44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cae6b5c5419c46adbfe9d6eb7aa3ec33"}},"metadata":{}}]},{"cell_type":"markdown","source":"Then we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT.","metadata":{"id":"Mp2gMi1ZzGET"}},{"cell_type":"markdown","source":"## Metaphor Probabilities (zero-shot) for the pretrained model","metadata":{"id":"9cx78s7wxz2a"}},{"cell_type":"code","source":"dataset = load_dataset(\"nightingal3/fig-qa\")","metadata":{"id":"-MD2kwlmxz2b","execution":{"iopub.status.busy":"2023-10-12T14:08:31.395251Z","iopub.execute_input":"2023-10-12T14:08:31.395786Z","iopub.status.idle":"2023-10-12T14:08:33.558976Z","shell.execute_reply.started":"2023-10-12T14:08:31.395744Z","shell.execute_reply":"2023-10-12T14:08:33.558015Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/3.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66494f67855e40abb735df4d0a690a84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81198d09964b4d2b822d129d768709bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/155k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44a5cb010f6644629be3b0528867f4e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/21.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d1158b5a5bf4ae796693db11bb554a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/864k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d0149f00e354c5690605a3539a240ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/116k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0596854276843c2b652caf3c3679e93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/120k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17234f0f6719451daaddb4009cae2e82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f080d6b648e46e0a7406631e4ba0370"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da20c1f847bb49139fe60c6b63ad8dc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f089a3860f714b49a6c8ebbb98a2d53f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64c3e403bbb04fe3a4f304751304fd6c"}},"metadata":{}}]},{"cell_type":"code","source":"subset_len = 100\nsubset_test_dataset = dataset['validation'].select(range(subset_len))","metadata":{"id":"UkmIOMfV3elH","execution":{"iopub.status.busy":"2023-10-12T15:24:25.746215Z","iopub.execute_input":"2023-10-12T15:24:25.746590Z","iopub.status.idle":"2023-10-12T15:24:25.756770Z","shell.execute_reply.started":"2023-10-12T15:24:25.746562Z","shell.execute_reply":"2023-10-12T15:24:25.755497Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"out_df, preds, labels = evaluate_model(model, tokenizer, subset_test_dataset, verbose = False, total = subset_len, middle_phrase= ' That is to say: ', use_cuda=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AAuyTVnr31VL","outputId":"057336cb-0d3a-46a9-fa92-d20fa5f4179c","execution":{"iopub.status.busy":"2023-10-12T15:24:27.019978Z","iopub.execute_input":"2023-10-12T15:24:27.020347Z","iopub.status.idle":"2023-10-12T15:25:17.176015Z","shell.execute_reply.started":"2023-10-12T15:24:27.020318Z","shell.execute_reply":"2023-10-12T15:25:17.175051Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [00:50<00:00,  1.99it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"zero_shot_accuracy, conf_matrix_zero_shot =  compute_stats(out_df, preds, labels)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DV8w0TDc34fo","outputId":"9d561724-5024-49c3-a402-4732c9747c95","execution":{"iopub.status.busy":"2023-10-12T15:25:17.178083Z","iopub.execute_input":"2023-10-12T15:25:17.182789Z","iopub.status.idle":"2023-10-12T15:25:17.199456Z","shell.execute_reply.started":"2023-10-12T15:25:17.182750Z","shell.execute_reply":"2023-10-12T15:25:17.198432Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"overall accuracy: \n0.56\nconfusion matrix: \ncorrect forward 56 wrong forward 44 correct backward 56 wrong_backward 44\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\n\nmodel_id_string = model_id.replace(\"/\", \"-\")\n\n# Saving DataFrame to CSV\nout_df.to_csv(f'output_df_{model_id_string}.csv', sep=\"\\t\", index=False)\n\n# Saving other data as JSON\ndata_to_save = {\n    \"preds\": preds,\n    \"labels\": labels,\n    \"zero_shot_accuracy\": zero_shot_accuracy,\n    \"conf_matrix_zero_shot\": conf_matrix_zero_shot\n}\n\nwith open(f'output_data_{model_id_string}.json', 'w') as file:\n    json.dump(data_to_save, file)\n\nimport pickle\n\ndata_to_save_pick = {\n    \"out_df\": out_df,\n    \"preds\": preds,\n    \"labels\": labels,\n    \"zero_shot_accuracy\": zero_shot_accuracy,\n    \"conf_matrix_zero_shot\": conf_matrix_zero_shot\n}\n\nwith open(f'pickle_output_data_{model_id_string}.pkl', 'wb') as file:\n    pickle.dump(data_to_save_pick, file)\n\n","metadata":{"id":"f-Oxn_ffXelA","execution":{"iopub.status.busy":"2023-10-06T22:46:08.908753Z","iopub.execute_input":"2023-10-06T22:46:08.909087Z","iopub.status.idle":"2023-10-06T22:46:08.934752Z","shell.execute_reply.started":"2023-10-06T22:46:08.909060Z","shell.execute_reply":"2023-10-06T22:46:08.933737Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Finetuning","metadata":{"id":"d4uwXib2xz2b"}},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"id":"a9EUEDAl0ss3","colab":{"base_uri":"https://localhost:8080/","height":381},"outputId":"0cfa011b-1894-4d60-8abc-dc7a48916f87","execution":{"iopub.status.busy":"2023-10-12T15:09:15.318337Z","iopub.execute_input":"2023-10-12T15:09:15.318698Z","iopub.status.idle":"2023-10-12T15:09:15.338305Z","shell.execute_reply.started":"2023-10-12T15:09:15.318669Z","shell.execute_reply":"2023-10-12T15:09:15.337096Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def print_modules(model, prefix=''):\n    for name, module in model.named_children():\n        full_name = f\"{prefix}.{name}\" if prefix else name\n        print(full_name)\n        print_modules(module, full_name)\n\nprint_modules(model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3S0tJHkaDEde","outputId":"264b1736-3f27-449e-ea9a-e4fb7165ee19","execution":{"iopub.status.busy":"2023-10-12T15:09:17.793004Z","iopub.execute_input":"2023-10-12T15:09:17.793389Z","iopub.status.idle":"2023-10-12T15:09:17.866685Z","shell.execute_reply.started":"2023-10-12T15:09:17.793358Z","shell.execute_reply":"2023-10-12T15:09:17.865532Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"model\nmodel.embed_tokens\nmodel.layers\nmodel.layers.0\nmodel.layers.0.self_attn\nmodel.layers.0.self_attn.q_proj\nmodel.layers.0.self_attn.k_proj\nmodel.layers.0.self_attn.v_proj\nmodel.layers.0.self_attn.o_proj\nmodel.layers.0.self_attn.rotary_emb\nmodel.layers.0.mlp\nmodel.layers.0.mlp.gate_proj\nmodel.layers.0.mlp.up_proj\nmodel.layers.0.mlp.down_proj\nmodel.layers.0.mlp.act_fn\nmodel.layers.0.input_layernorm\nmodel.layers.0.post_attention_layernorm\nmodel.layers.1\nmodel.layers.1.self_attn\nmodel.layers.1.self_attn.q_proj\nmodel.layers.1.self_attn.k_proj\nmodel.layers.1.self_attn.v_proj\nmodel.layers.1.self_attn.o_proj\nmodel.layers.1.self_attn.rotary_emb\nmodel.layers.1.mlp\nmodel.layers.1.mlp.gate_proj\nmodel.layers.1.mlp.up_proj\nmodel.layers.1.mlp.down_proj\nmodel.layers.1.mlp.act_fn\nmodel.layers.1.input_layernorm\nmodel.layers.1.post_attention_layernorm\nmodel.layers.2\nmodel.layers.2.self_attn\nmodel.layers.2.self_attn.q_proj\nmodel.layers.2.self_attn.k_proj\nmodel.layers.2.self_attn.v_proj\nmodel.layers.2.self_attn.o_proj\nmodel.layers.2.self_attn.rotary_emb\nmodel.layers.2.mlp\nmodel.layers.2.mlp.gate_proj\nmodel.layers.2.mlp.up_proj\nmodel.layers.2.mlp.down_proj\nmodel.layers.2.mlp.act_fn\nmodel.layers.2.input_layernorm\nmodel.layers.2.post_attention_layernorm\nmodel.layers.3\nmodel.layers.3.self_attn\nmodel.layers.3.self_attn.q_proj\nmodel.layers.3.self_attn.k_proj\nmodel.layers.3.self_attn.v_proj\nmodel.layers.3.self_attn.o_proj\nmodel.layers.3.self_attn.rotary_emb\nmodel.layers.3.mlp\nmodel.layers.3.mlp.gate_proj\nmodel.layers.3.mlp.up_proj\nmodel.layers.3.mlp.down_proj\nmodel.layers.3.mlp.act_fn\nmodel.layers.3.input_layernorm\nmodel.layers.3.post_attention_layernorm\nmodel.layers.4\nmodel.layers.4.self_attn\nmodel.layers.4.self_attn.q_proj\nmodel.layers.4.self_attn.k_proj\nmodel.layers.4.self_attn.v_proj\nmodel.layers.4.self_attn.o_proj\nmodel.layers.4.self_attn.rotary_emb\nmodel.layers.4.mlp\nmodel.layers.4.mlp.gate_proj\nmodel.layers.4.mlp.up_proj\nmodel.layers.4.mlp.down_proj\nmodel.layers.4.mlp.act_fn\nmodel.layers.4.input_layernorm\nmodel.layers.4.post_attention_layernorm\nmodel.layers.5\nmodel.layers.5.self_attn\nmodel.layers.5.self_attn.q_proj\nmodel.layers.5.self_attn.k_proj\nmodel.layers.5.self_attn.v_proj\nmodel.layers.5.self_attn.o_proj\nmodel.layers.5.self_attn.rotary_emb\nmodel.layers.5.mlp\nmodel.layers.5.mlp.gate_proj\nmodel.layers.5.mlp.up_proj\nmodel.layers.5.mlp.down_proj\nmodel.layers.5.mlp.act_fn\nmodel.layers.5.input_layernorm\nmodel.layers.5.post_attention_layernorm\nmodel.layers.6\nmodel.layers.6.self_attn\nmodel.layers.6.self_attn.q_proj\nmodel.layers.6.self_attn.k_proj\nmodel.layers.6.self_attn.v_proj\nmodel.layers.6.self_attn.o_proj\nmodel.layers.6.self_attn.rotary_emb\nmodel.layers.6.mlp\nmodel.layers.6.mlp.gate_proj\nmodel.layers.6.mlp.up_proj\nmodel.layers.6.mlp.down_proj\nmodel.layers.6.mlp.act_fn\nmodel.layers.6.input_layernorm\nmodel.layers.6.post_attention_layernorm\nmodel.layers.7\nmodel.layers.7.self_attn\nmodel.layers.7.self_attn.q_proj\nmodel.layers.7.self_attn.k_proj\nmodel.layers.7.self_attn.v_proj\nmodel.layers.7.self_attn.o_proj\nmodel.layers.7.self_attn.rotary_emb\nmodel.layers.7.mlp\nmodel.layers.7.mlp.gate_proj\nmodel.layers.7.mlp.up_proj\nmodel.layers.7.mlp.down_proj\nmodel.layers.7.mlp.act_fn\nmodel.layers.7.input_layernorm\nmodel.layers.7.post_attention_layernorm\nmodel.layers.8\nmodel.layers.8.self_attn\nmodel.layers.8.self_attn.q_proj\nmodel.layers.8.self_attn.k_proj\nmodel.layers.8.self_attn.v_proj\nmodel.layers.8.self_attn.o_proj\nmodel.layers.8.self_attn.rotary_emb\nmodel.layers.8.mlp\nmodel.layers.8.mlp.gate_proj\nmodel.layers.8.mlp.up_proj\nmodel.layers.8.mlp.down_proj\nmodel.layers.8.mlp.act_fn\nmodel.layers.8.input_layernorm\nmodel.layers.8.post_attention_layernorm\nmodel.layers.9\nmodel.layers.9.self_attn\nmodel.layers.9.self_attn.q_proj\nmodel.layers.9.self_attn.k_proj\nmodel.layers.9.self_attn.v_proj\nmodel.layers.9.self_attn.o_proj\nmodel.layers.9.self_attn.rotary_emb\nmodel.layers.9.mlp\nmodel.layers.9.mlp.gate_proj\nmodel.layers.9.mlp.up_proj\nmodel.layers.9.mlp.down_proj\nmodel.layers.9.mlp.act_fn\nmodel.layers.9.input_layernorm\nmodel.layers.9.post_attention_layernorm\nmodel.layers.10\nmodel.layers.10.self_attn\nmodel.layers.10.self_attn.q_proj\nmodel.layers.10.self_attn.k_proj\nmodel.layers.10.self_attn.v_proj\nmodel.layers.10.self_attn.o_proj\nmodel.layers.10.self_attn.rotary_emb\nmodel.layers.10.mlp\nmodel.layers.10.mlp.gate_proj\nmodel.layers.10.mlp.up_proj\nmodel.layers.10.mlp.down_proj\nmodel.layers.10.mlp.act_fn\nmodel.layers.10.input_layernorm\nmodel.layers.10.post_attention_layernorm\nmodel.layers.11\nmodel.layers.11.self_attn\nmodel.layers.11.self_attn.q_proj\nmodel.layers.11.self_attn.k_proj\nmodel.layers.11.self_attn.v_proj\nmodel.layers.11.self_attn.o_proj\nmodel.layers.11.self_attn.rotary_emb\nmodel.layers.11.mlp\nmodel.layers.11.mlp.gate_proj\nmodel.layers.11.mlp.up_proj\nmodel.layers.11.mlp.down_proj\nmodel.layers.11.mlp.act_fn\nmodel.layers.11.input_layernorm\nmodel.layers.11.post_attention_layernorm\nmodel.layers.12\nmodel.layers.12.self_attn\nmodel.layers.12.self_attn.q_proj\nmodel.layers.12.self_attn.k_proj\nmodel.layers.12.self_attn.v_proj\nmodel.layers.12.self_attn.o_proj\nmodel.layers.12.self_attn.rotary_emb\nmodel.layers.12.mlp\nmodel.layers.12.mlp.gate_proj\nmodel.layers.12.mlp.up_proj\nmodel.layers.12.mlp.down_proj\nmodel.layers.12.mlp.act_fn\nmodel.layers.12.input_layernorm\nmodel.layers.12.post_attention_layernorm\nmodel.layers.13\nmodel.layers.13.self_attn\nmodel.layers.13.self_attn.q_proj\nmodel.layers.13.self_attn.k_proj\nmodel.layers.13.self_attn.v_proj\nmodel.layers.13.self_attn.o_proj\nmodel.layers.13.self_attn.rotary_emb\nmodel.layers.13.mlp\nmodel.layers.13.mlp.gate_proj\nmodel.layers.13.mlp.up_proj\nmodel.layers.13.mlp.down_proj\nmodel.layers.13.mlp.act_fn\nmodel.layers.13.input_layernorm\nmodel.layers.13.post_attention_layernorm\nmodel.layers.14\nmodel.layers.14.self_attn\nmodel.layers.14.self_attn.q_proj\nmodel.layers.14.self_attn.k_proj\nmodel.layers.14.self_attn.v_proj\nmodel.layers.14.self_attn.o_proj\nmodel.layers.14.self_attn.rotary_emb\nmodel.layers.14.mlp\nmodel.layers.14.mlp.gate_proj\nmodel.layers.14.mlp.up_proj\nmodel.layers.14.mlp.down_proj\nmodel.layers.14.mlp.act_fn\nmodel.layers.14.input_layernorm\nmodel.layers.14.post_attention_layernorm\nmodel.layers.15\nmodel.layers.15.self_attn\nmodel.layers.15.self_attn.q_proj\nmodel.layers.15.self_attn.k_proj\nmodel.layers.15.self_attn.v_proj\nmodel.layers.15.self_attn.o_proj\nmodel.layers.15.self_attn.rotary_emb\nmodel.layers.15.mlp\nmodel.layers.15.mlp.gate_proj\nmodel.layers.15.mlp.up_proj\nmodel.layers.15.mlp.down_proj\nmodel.layers.15.mlp.act_fn\nmodel.layers.15.input_layernorm\nmodel.layers.15.post_attention_layernorm\nmodel.layers.16\nmodel.layers.16.self_attn\nmodel.layers.16.self_attn.q_proj\nmodel.layers.16.self_attn.k_proj\nmodel.layers.16.self_attn.v_proj\nmodel.layers.16.self_attn.o_proj\nmodel.layers.16.self_attn.rotary_emb\nmodel.layers.16.mlp\nmodel.layers.16.mlp.gate_proj\nmodel.layers.16.mlp.up_proj\nmodel.layers.16.mlp.down_proj\nmodel.layers.16.mlp.act_fn\nmodel.layers.16.input_layernorm\nmodel.layers.16.post_attention_layernorm\nmodel.layers.17\nmodel.layers.17.self_attn\nmodel.layers.17.self_attn.q_proj\nmodel.layers.17.self_attn.k_proj\nmodel.layers.17.self_attn.v_proj\nmodel.layers.17.self_attn.o_proj\nmodel.layers.17.self_attn.rotary_emb\nmodel.layers.17.mlp\nmodel.layers.17.mlp.gate_proj\nmodel.layers.17.mlp.up_proj\nmodel.layers.17.mlp.down_proj\nmodel.layers.17.mlp.act_fn\nmodel.layers.17.input_layernorm\nmodel.layers.17.post_attention_layernorm\nmodel.layers.18\nmodel.layers.18.self_attn\nmodel.layers.18.self_attn.q_proj\nmodel.layers.18.self_attn.k_proj\nmodel.layers.18.self_attn.v_proj\nmodel.layers.18.self_attn.o_proj\nmodel.layers.18.self_attn.rotary_emb\nmodel.layers.18.mlp\nmodel.layers.18.mlp.gate_proj\nmodel.layers.18.mlp.up_proj\nmodel.layers.18.mlp.down_proj\nmodel.layers.18.mlp.act_fn\nmodel.layers.18.input_layernorm\nmodel.layers.18.post_attention_layernorm\nmodel.layers.19\nmodel.layers.19.self_attn\nmodel.layers.19.self_attn.q_proj\nmodel.layers.19.self_attn.k_proj\nmodel.layers.19.self_attn.v_proj\nmodel.layers.19.self_attn.o_proj\nmodel.layers.19.self_attn.rotary_emb\nmodel.layers.19.mlp\nmodel.layers.19.mlp.gate_proj\nmodel.layers.19.mlp.up_proj\nmodel.layers.19.mlp.down_proj\nmodel.layers.19.mlp.act_fn\nmodel.layers.19.input_layernorm\nmodel.layers.19.post_attention_layernorm\nmodel.layers.20\nmodel.layers.20.self_attn\nmodel.layers.20.self_attn.q_proj\nmodel.layers.20.self_attn.k_proj\nmodel.layers.20.self_attn.v_proj\nmodel.layers.20.self_attn.o_proj\nmodel.layers.20.self_attn.rotary_emb\nmodel.layers.20.mlp\nmodel.layers.20.mlp.gate_proj\nmodel.layers.20.mlp.up_proj\nmodel.layers.20.mlp.down_proj\nmodel.layers.20.mlp.act_fn\nmodel.layers.20.input_layernorm\nmodel.layers.20.post_attention_layernorm\nmodel.layers.21\nmodel.layers.21.self_attn\nmodel.layers.21.self_attn.q_proj\nmodel.layers.21.self_attn.k_proj\nmodel.layers.21.self_attn.v_proj\nmodel.layers.21.self_attn.o_proj\nmodel.layers.21.self_attn.rotary_emb\nmodel.layers.21.mlp\nmodel.layers.21.mlp.gate_proj\nmodel.layers.21.mlp.up_proj\nmodel.layers.21.mlp.down_proj\nmodel.layers.21.mlp.act_fn\nmodel.layers.21.input_layernorm\nmodel.layers.21.post_attention_layernorm\nmodel.norm\nlm_head\n","output_type":"stream"}]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"id":"gkIcwsSU01EB","execution":{"iopub.status.busy":"2023-10-12T15:09:38.981146Z","iopub.execute_input":"2023-10-12T15:09:38.981636Z","iopub.status.idle":"2023-10-12T15:09:38.996874Z","shell.execute_reply.started":"2023-10-12T15:09:38.981599Z","shell.execute_reply":"2023-10-12T15:09:38.995869Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    #target_modules=[\"query_key_value\"],\n    target_modules=[\"self_attn.q_proj\" , \"self_attn.k_proj\", \"self_attn.v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel_peft = get_peft_model(model, config)\nprint_trainable_parameters(model_peft)","metadata":{"id":"Ybeyl20n3dYH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e3b48447-b507-4a2c-8cfa-379c6c75f762","execution":{"iopub.status.busy":"2023-10-12T15:26:27.207382Z","iopub.execute_input":"2023-10-12T15:26:27.207757Z","iopub.status.idle":"2023-10-12T15:26:27.289091Z","shell.execute_reply.started":"2023-10-12T15:26:27.207726Z","shell.execute_reply":"2023-10-12T15:26:27.288048Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"trainable params: 2252800 || all params: 617871360 || trainable%: 0.36460663915543845\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's load a common dataset, english quotes, to fine tune our model on famous quotes.","metadata":{"id":"FCc64bfnmd3j"}},{"cell_type":"code","source":"text = \"\"\"I'm selfish, impatient and a little insecure.\"\"\"\n\ndevice = \"cuda:0\"\n\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)\noutputs = model_peft.generate(**inputs, max_new_tokens=150)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:10:30.276664Z","iopub.execute_input":"2023-10-12T15:10:30.277686Z","iopub.status.idle":"2023-10-12T15:10:42.580526Z","shell.execute_reply.started":"2023-10-12T15:10:30.277643Z","shell.execute_reply":"2023-10-12T15:10:42.579396Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"I'm selfish, impatient and a little insecure. I like to be in control and I like to be the center of attention. I like to be in the spotlight and I like to be the center of attention. I like to be in control and I like to be the center of attention. I like to be in the spotlight and I like to be the center of attention. I like to be in control and I like to be the center of attention. I like to be in the spotlight and I like to be the center of attention. I like to be in control and I like to be the center of attention. I like to be in the spotlight and I like to be the center of attention. I like to be in control and I like to be the center of\n","output_type":"stream"}]},{"cell_type":"code","source":"def map_concatenation_and_tokenization(samples):\n    concatenated_phrases = []\n    input_ids_list = []\n    attention_mask_list = []\n\n    for i in range(len(samples['startphrase'])):\n        # Choose the ending based on the labels value for each sample in the batch\n        ending = samples['ending1'][i] if samples['labels'][i] == 0 else samples['ending2'][i]\n        concatenated_phrase = samples['startphrase'][i] + ' That is to say: ' + ending\n        concatenated_phrases.append(concatenated_phrase)\n\n        # Tokenize the concatenated_phrase\n        tokens = tokenizer(concatenated_phrase, truncation=True, max_length=512, return_tensors='pt')\n        input_ids_list.append(tokens['input_ids'][0].tolist())\n        attention_mask_list.append(tokens['attention_mask'][0].tolist())\n\n    return {\n        'concatenated_phrase': concatenated_phrases,\n        'input_ids': input_ids_list,\n        'attention_mask': attention_mask_list\n    }\n\n# Apply the mapping function\ndata = load_dataset(\"nightingal3/fig-qa\")\ndata = data.map(map_concatenation_and_tokenization, batched=True)\n\n# data = dataset\n# data = data.map(lambda samples: tokenizer(samples[\"startphrase\"]), batched=True)","metadata":{"id":"s6f4z8EYmcJ6","colab":{"base_uri":"https://localhost:8080/","height":209,"referenced_widgets":["98351baf42934fe5b7524b3b64accbd8","3d0671ee3f234656b8a2627389e07e9d","15b50e5a174a40c3b7c0dd24943b80c0","41f80043500046d79f105206a1c91610","737604363ab84acab1980304dfae18b7","7d5ce9cf72a741508b0044e9d36b36f9","5fc16426f4c04d54acc2f2e0457a6416","e690820d66e4416ba819ebbef9f2909e","42716af8d24146c4b29393ddc2dc6a8a","03421596b8f343eaafd392d951860431","71eda188275c493b89b4a2dcdbf3f336","3e3dc5d6c0b540ccb7f7afa134e43dcf","cf463801021a47f284c7e0d3acfc8d63","3e763d4588cd492baf6f5dcfbab241cc","3c093130754d4825b2d1772e7ae1f2ae","c50c4ada3b8f46429028c0fb11bb856a","24abc37e54ba41669611b618d77b332d","bc02bb96ad494682a4443818bf9e2b36","32c4670df43042e9b5258bf33dff8c7e","d4bb2ee46b2c489e954139c44fc9a192","f4748ce5b31647e3ad85c750a7862e49","010e9589a46941dd83eb4cac8770a626","3286bad3a57445e9a41b81ea027e631d","72abdfd49b8e4960b22d25e0615b6a6c","76377837c9e346efaf05625d9ec9782d","968a0fcf5fad48028404e342f677e664","33e0587bb0b24accb05c16cc02d75b3d","d7c0ccb695a64e6ba4bc08de65ac7739","c19ffd16e845410bbe563cecb1b3d612","3a88eec22474455d80a541f1afc10dd6","cff47dc83ead4436903d9b651b9b4649","5967538111e4487b8a49e40189f97c31","b464aa7530da497791ce86ae00bdc07e","a18b440ce6fe49f68a92cac0f0e1c2fb","5f394fcda1fd4f39957d14a5c4caa631","06352b37a3de45199d28f0710e31742f","9408db961369449a9f1716a185ac6e7d","1ab6fddb871b4889bad1850aeb911e42","0d3cdf0730754282af4d57e8db5aef80","6fc4c705d1214cc193bf64963121e81b","9fcdf6c2ce5f4dcdb97e270d25d08a0d","7cdf6b8039e1414ca97892214106a41d","f2bcfde8fbc1462b9a7f2334f592e615","ccc72c8249a04775b61d24a9edf914f9","c9bbfdae62be4fb9a4343c3dfd2c557a","d2c2407dec094aef82681b44ec2e3d2f","9e8a6ae9602f48dbb83083b72ca53e1d","899f4a2877e949b6a2117b81c9aed8a5","60a2c67412904f369f8c046ab8ca29e2","f99b02cc6f1149c08fadf7959c5c2685","fb8017a9dccb4103a6e4a012053e0fec","96d68d033c774137a3db337e978d83a5","990018b5851242c2a93a39761cd44e8c","02676ed015414b7bad8f234881fd1fe7","13bfd884d21e4579855fc8acf09533a8","6df57c6325c74747be601eb8b75d1dd5","5fef097a0cbe41d2bd7a33e694a82b5a","426040f036ac497d83a8af02a1a26a63","b3af7e680e6645a496bd0b4a48776fbb","01f89584ce88486d9405e71c14817b37","0b743164f8184e0cafc84c9883d8729d","388e42c3747c44fe95febfb0a495982a","31fa3a0b8ff84732aaa2ed4032373a4b","0016ad3003c0499fb29df618377d5632","680f8d845c0646b090e13fa5b25ae971","f7f5cabdccc84b3b83f9d93e5e03a5c0"]},"outputId":"2dcfb618-fc3d-4633-fffe-0c8358336876","execution":{"iopub.status.busy":"2023-10-12T15:10:51.350049Z","iopub.execute_input":"2023-10-12T15:10:51.350421Z","iopub.status.idle":"2023-10-12T15:10:55.617153Z","shell.execute_reply.started":"2023-10-12T15:10:51.350393Z","shell.execute_reply":"2023-10-12T15:10:55.615896Z"},"trusted":true},"execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9674 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a92023722bc450182d142c4e2742265"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1094 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25649b45fcee45a487ee0d4eba255860"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1146 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1281e472549f4320a4b3e0b54d2f0e42"}},"metadata":{}}]},{"cell_type":"code","source":"data[\"train\"]","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:28:22.528302Z","iopub.execute_input":"2023-10-12T15:28:22.528686Z","iopub.status.idle":"2023-10-12T15:28:22.537626Z","shell.execute_reply.started":"2023-10-12T15:28:22.528657Z","shell.execute_reply":"2023-10-12T15:28:22.536274Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['startphrase', 'ending1', 'ending2', 'labels', 'valid', 'concatenated_phrase', 'input_ids', 'attention_mask'],\n    num_rows: 9674\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Run the cell below to run the training! For the sake of the demo, we just ran it for few steps just to showcase how to use this integration with existing tools on the HF ecosystem.","metadata":{"id":"_0MOtwf3zdZp"}},{"cell_type":"code","source":"model_peft.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:26:36.577972Z","iopub.execute_input":"2023-10-12T15:26:36.578332Z","iopub.status.idle":"2023-10-12T15:26:36.620816Z","shell.execute_reply.started":"2023-10-12T15:26:36.578306Z","shell.execute_reply":"2023-10-12T15:26:36.619754Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32003, 2048)\n        (layers): ModuleList(\n          (0-21): 22 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): Linear4bit(\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n              )\n              (k_proj): Linear4bit(\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n              )\n              (v_proj): Linear4bit(\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n              )\n              (o_proj): Linear4bit(\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n              (act_fn): SiLUActivation()\n            )\n            (input_layernorm): LlamaRMSNorm()\n            (post_attention_layernorm): LlamaRMSNorm()\n          )\n        )\n        (norm): LlamaRMSNorm()\n      )\n      (lm_head): Linear(in_features=2048, out_features=32003, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"import transformers\n\n# needed for gpt-neo-x tokenizer\ntokenizer.pad_token = tokenizer.eos_token\n\ntrainer = transformers.Trainer(\n    model=model_peft,\n    train_dataset=data[\"train\"],\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=20,\n        gradient_accumulation_steps=4,\n        warmup_steps=2,\n        max_steps=70,\n        learning_rate=0.0001,#2e-4\n        fp16=True,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"paged_adamw_8bit\"\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\nmodel_peft.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()","metadata":{"id":"jq0nX33BmfaC","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"f9686f25-c90a-4d75-9a41-4dfc4f269113","execution":{"iopub.status.busy":"2023-10-12T15:28:42.005260Z","iopub.execute_input":"2023-10-12T15:28:42.005628Z","iopub.status.idle":"2023-10-12T15:51:35.847559Z","shell.execute_reply.started":"2023-10-12T15:28:42.005600Z","shell.execute_reply":"2023-10-12T15:51:35.846338Z"},"trusted":true},"execution_count":75,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [70/70 22:33, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.894000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.929800</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.825400</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3.897900</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>3.788600</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>3.713500</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>3.726500</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>3.680200</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>3.645200</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>3.589600</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>3.566100</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>3.490400</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>3.439300</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>3.368100</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>3.355200</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>3.299000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>3.252400</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>3.137400</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>3.147200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>3.158200</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>3.090700</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>3.034400</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>2.988000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>2.930600</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>2.913600</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>2.836000</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>2.861900</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>2.801600</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>2.797100</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.701500</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>2.638200</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>2.645300</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>2.617300</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>2.647200</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>2.520800</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>2.546100</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>2.631200</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>2.567800</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>2.486600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.609800</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>2.474400</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>2.480500</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>2.470100</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>2.413300</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>2.427700</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>2.432800</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>2.312100</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>2.361900</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>2.353600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.363400</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>2.310800</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>2.309900</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>2.335200</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>2.221100</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>2.322200</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>2.291100</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>2.238800</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>2.335500</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>2.330800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.298200</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>2.254900</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>2.291300</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>2.233100</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>2.197300</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>2.299100</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>2.230800</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>2.280700</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>2.170300</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>2.295100</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>2.261100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=70, training_loss=2.7909540142331806, metrics={'train_runtime': 1373.2407, 'train_samples_per_second': 8.156, 'train_steps_per_second': 0.051, 'total_flos': 2800427181146112.0, 'train_loss': 2.7909540142331806, 'epoch': 1.16})"},"metadata":{}}]},{"cell_type":"code","source":"model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\nmodel_to_save.save_pretrained(\"outputs\")","metadata":{"id":"p66mZk1RAlOR","execution":{"iopub.status.busy":"2023-10-12T15:51:35.849761Z","iopub.execute_input":"2023-10-12T15:51:35.854156Z","iopub.status.idle":"2023-10-12T15:51:35.922454Z","shell.execute_reply.started":"2023-10-12T15:51:35.854108Z","shell.execute_reply":"2023-10-12T15:51:35.921297Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"lora_config = LoraConfig.from_pretrained('outputs')\nmodel_peft = get_peft_model(model_peft, lora_config)","metadata":{"id":"L2Hllu-bCuN6","execution":{"iopub.status.busy":"2023-10-12T15:51:35.923919Z","iopub.execute_input":"2023-10-12T15:51:35.928720Z","iopub.status.idle":"2023-10-12T15:51:36.022292Z","shell.execute_reply.started":"2023-10-12T15:51:35.928673Z","shell.execute_reply":"2023-10-12T15:51:36.021153Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"text = \"\"\"I'm selfish, impatient and a little insecure.\"\"\"\n\ndevice = \"cuda:0\"\n\nmodel_peft.to(device)\n\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)\noutputs = model_peft.generate(**inputs, max_new_tokens=150)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:51:36.024382Z","iopub.execute_input":"2023-10-12T15:51:36.024962Z","iopub.status.idle":"2023-10-12T15:52:01.280908Z","shell.execute_reply.started":"2023-10-12T15:51:36.024906Z","shell.execute_reply":"2023-10-12T15:52:01.279793Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"I'm selfish, impatient and a little insecure. I like to be in control and I like to be the center of attention. I like to be in the spotlight and I like to be the center of attention. I like to be in control and I like to be the center of attention. I like to be in the spotlight and I like to be the center of attention. I like to be in control and I like to be the center of attention. I like to be in the spotlight and I like to be the center of attention. I like to be in control and I like to be the center of attention. I like to be in the spotlight and I like to be the center of attention. I like to be in control and I like to be the center of\n","output_type":"stream"}]},{"cell_type":"code","source":"out_df_finetuned, preds_finetuned, labels_finetuned = evaluate_model(model_peft, tokenizer, subset_test_dataset, verbose = False, total = subset_len, middle_phrase= ' That is to say: ')","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:52:01.285812Z","iopub.execute_input":"2023-10-12T15:52:01.288357Z","iopub.status.idle":"2023-10-12T15:52:51.593358Z","shell.execute_reply.started":"2023-10-12T15:52:01.288315Z","shell.execute_reply":"2023-10-12T15:52:51.592366Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [00:50<00:00,  1.99it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"zero_shot_accuracy_finetuned, conf_matrix_zero_shot_finetuned =  compute_stats(out_df_finetuned, preds_finetuned, labels_finetuned)","metadata":{"id":"kEESIVXyESi-","execution":{"iopub.status.busy":"2023-10-12T15:52:51.597131Z","iopub.execute_input":"2023-10-12T15:52:51.600676Z","iopub.status.idle":"2023-10-12T15:52:51.613163Z","shell.execute_reply.started":"2023-10-12T15:52:51.600633Z","shell.execute_reply":"2023-10-12T15:52:51.611938Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"overall accuracy: \n0.56\nconfusion matrix: \ncorrect forward 56 wrong forward 44 correct backward 56 wrong_backward 44\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\n\nmodel_id_string = model_id.replace(\"/\", \"-\")\n\n# Saving DataFrame to CSV\nout_df_finetuned.to_csv(f'output_df_{model_id_string}_finetuned.csv', sep=\"\\t\", index=False)\n\n# Saving other data as JSON\ndata_to_save = {\n    \"preds\": preds_finetuned,\n    \"labels\": labels_finetuned,\n    \"zero_shot_accuracy\": zero_shot_accuracy_finetuned,\n    \"conf_matrix_zero_shot\": conf_matrix_zero_shot_finetuned\n}\n\nwith open(f'output_data_{model_id_string}_finetuned.json', 'w') as file:\n    json.dump(data_to_save, file)\n\nimport pickle\n\ndata_to_save_pick = {\n    \"out_df\": out_df_finetuned,\n    \"preds\": preds_finetuned,\n    \"labels\": labels_finetuned,\n    \"zero_shot_accuracy\": zero_shot_accuracy_finetuned,\n    \"conf_matrix_zero_shot\": conf_matrix_zero_shot_finetuned\n}\n\nwith open(f'pickle_output_data_{model_id_string}_finetuned.pkl', 'wb') as file:\n    pickle.dump(data_to_save_pick, file)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-06T23:39:29.795078Z","iopub.execute_input":"2023-10-06T23:39:29.795402Z","iopub.status.idle":"2023-10-06T23:39:29.829418Z","shell.execute_reply.started":"2023-10-06T23:39:29.795372Z","shell.execute_reply":"2023-10-06T23:39:29.828462Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-10-06T23:43:18.068726Z","iopub.execute_input":"2023-10-06T23:43:18.069063Z","iopub.status.idle":"2023-10-06T23:43:18.078453Z","shell.execute_reply.started":"2023-10-06T23:43:18.069035Z","shell.execute_reply":"2023-10-06T23:43:18.076716Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['startphrase', 'ending1', 'ending2', 'labels', 'valid', 'input_ids', 'attention_mask'],\n        num_rows: 9674\n    })\n    validation: Dataset({\n        features: ['startphrase', 'ending1', 'ending2', 'labels', 'valid', 'input_ids', 'attention_mask'],\n        num_rows: 1094\n    })\n    test: Dataset({\n        features: ['startphrase', 'ending1', 'ending2', 'labels', 'valid', 'input_ids', 'attention_mask'],\n        num_rows: 1146\n    })\n})"},"metadata":{}}]}]}