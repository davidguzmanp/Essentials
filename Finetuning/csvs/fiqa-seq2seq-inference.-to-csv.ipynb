{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Efficiently train Large Language Models with LoRA and Hugging Face\n\nIn this blog, we are going to show you how to apply [Low-Rank Adaptation of Large Language Models (LoRA)](https://arxiv.org/abs/2106.09685) to fine-tune FLAN-T5 XXL (11 billion parameters) on a single GPU. We are going to leverage Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), and [PEFT](https://github.com/huggingface/peft). \n\nYou will learn how to:\n\n1. Setup Development Environment\n2. Load and prepare the dataset\n3. Fine-Tune T5 with LoRA and bnb int-8\n4. Evaluate & run Inference with LoRA FLAN-T5\n5. Cost performance comparison\n\n### Quick intro: PEFT or Parameter Efficient Fine-tunin\n\n[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT currently includes techniques for:\n\n- LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n- Prefix Tuning: [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n- P-Tuning: [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n- Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n\n*Note: This tutorial was created and run on a g5.2xlarge AWS EC2 Instance, including 1 NVIDIA A10G.*","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup Development Environment\n\nIn our example, we use the [PyTorch Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-pytorch.html) with already set up CUDA drivers and PyTorch installed. We still have to install the Hugging Face Libraries, including transformers and datasets. Running the following cell will install all the required packages.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom scipy.special import softmax\nimport pdb\nimport pandas as pd\nimport math\nfrom typing import List\nimport random\nimport argparse\nimport torch\n\n\ndef sent_scoring(model_tokenizer, text, cuda, score_type=\"loss\", output_attentions=False, length_normalize=False):\n    model = model_tokenizer[0]\n    tokenizer = model_tokenizer[1]\n    assert model is not None\n    assert tokenizer is not None\n    encoded_text = tokenizer.encode(text)\n    input_ids = torch.tensor(encoded_text).unsqueeze(0)\n    if cuda:\n        input_ids = input_ids.to('cuda')\n    with torch.no_grad():\n        outputs = model(input_ids, labels=input_ids, output_attentions=output_attentions)\n    loss, logits = outputs[:2]\n\n    sentence_prob = loss.item()\n    if score_type == \"prob\":\n        if length_normalize:\n            mult = 2\n        else:\n            mult = len(encoded_text)\n\n        sentence_prob = math.exp(-1.0 * loss * (mult - 1))\n\n    if output_attentions:\n        attn = outputs[\"attentions\"]\n        return sentence_prob, attn, input_ids\n\n    return sentence_prob\n\ndef confusion_matrix(P_forward_1, P_forward_2, P_backward_1, P_backward_2):\n    correct_forward = len(np.where(np.array(P_forward_1) >= 0.5)[0]) + len(np.where(np.array(P_forward_2) >=0.5)[0])\n    wrong_forward = len(P_forward_1) + len(P_forward_2) - correct_forward\n\n    correct_backward = len(np.where(np.array(P_backward_1) >= 0.5)[0]) + len(np.where(np.array(P_backward_2) >=0.5)[0])\n    wrong_backward = len(P_backward_1) + len(P_backward_2) - correct_backward\n\n    print(\"correct forward\", correct_forward, \"wrong forward\", wrong_forward, \"correct backward\", correct_backward, \"wrong_backward\", wrong_backward)\n\n    results = {\n        \"correct_forward\": correct_forward,\n        \"wrong_forward\": wrong_forward,\n        \"correct_backward\": correct_backward,\n        \"wrong_backward\": wrong_backward\n    }\n\n    return results\n\nfrom tqdm import tqdm\n\ndef evaluate_model(model, tokenizer, test_set, middle_phrase=\"\", use_prefix=0, verbose=True, score_type=\"prob\", use_cuda=False, return_acc=False, total = 1094) -> tuple:\n    preds = []\n    labels = []\n    x_1 = []\n    x_2 = []\n    y_1 = []\n    y_2 = []\n    P_x_1 = []\n    P_x_2 = []\n    P_y_1 = []\n    P_y_2 = []\n    P_x_1_y_1 = []\n    P_x_1_y_2 = []\n    P_x_2_y_1 = []\n    P_x_2_y_2 = []\n    P_x_1_correct = []\n    P_x_2_correct = []\n    P_y_1_correct = []\n    P_y_2_correct = []\n    correct = 0\n\n    for i, metaphor_data in tqdm(enumerate(test_set), total = total):\n        ctx, p1, p2 = metaphor_data[\"startphrase\"], metaphor_data[\"ending1\"], metaphor_data[\"ending2\"]\n        labels.append(int(metaphor_data[\"labels\"]))\n        if use_prefix > 0:\n            prefix_prompt = select_prefix_prompts(prompt_file, use_prefix) if use_prefix else \"\"\n        else:\n            prefix_prompt = \"\"\n\n        sent1 = prefix_prompt + ctx + \". \" + middle_phrase + p1 + \".\"\n        sent2 = prefix_prompt + ctx + \". \" + middle_phrase + p2 + \".\"\n\n        score1 = sent_scoring((model, tokenizer), sent1, use_cuda, score_type=score_type)\n        score2 = sent_scoring((model, tokenizer), sent2, use_cuda, score_type=score_type)\n\n        if score_type == \"loss\":\n            pred = 0 if score1 < score2 else 1\n        else:\n            pred = 1 if score1 < score2 else 0\n\n        pred_sent = sent1 if pred == 0 else sent2\n\n        if i % 2 == 0:\n            x_1.append(ctx)\n            x_1_score = sent_scoring((model, tokenizer), ctx + \".\", use_cuda, score_type=score_type)\n            P_x_1.append(x_1_score)\n            y_1.append(p1)\n            y_2.append(p2)\n            y1_score = sent_scoring((model, tokenizer), p1 + \".\", use_cuda, score_type=score_type)\n            y2_score = sent_scoring((model, tokenizer), p2 + \".\", use_cuda, score_type=score_type)\n            P_y_1.append(y1_score)\n            P_y_2.append(y2_score)\n\n            P_x_1_y_1.append(score1)\n            P_x_1_y_2.append(score2)\n            P_x_1_correct.append(score1/(score1 + score2))\n\n        else:\n            x_2.append(ctx)\n            x_2_score = sent_scoring((model, tokenizer), ctx + \".\", use_cuda, score_type=score_type)\n            P_x_2.append(x_2_score)\n            P_x_2_y_1.append(score1)\n            P_x_2_y_2.append(score2)\n            P_x_2_correct.append(score2/(score1 + score2))\n\n            P_y_1_correct.append(P_x_1_y_1[-1]/(P_x_1_y_1[-1] + score1))\n            P_y_2_correct.append(score2/(P_x_1_y_2[-1] + score2))\n\n        if verbose:\n            print(f\"Q: {ctx}: 1. {p1} 2. {p2}\")\n            print(f\"model says '{pred_sent}' is more likely\")\n            print(\"\\n\")\n        if pred == metaphor_data[\"labels\"]:\n            correct += 1\n        preds.append(pred)\n\n    cols = {\"x_1\": x_1, \"x_2\": x_2, \"y_1\": y_1, \"y_2\": y_2, \"P(x_1)\": P_x_1, \"P(x_2)\": P_x_2, \"P(y_1)\": P_y_1, \"P(y_2)\": P_y_2,\n        \"P(x_1, y_1)\": P_x_1_y_1, \"P(x_1, y_2)\": P_x_1_y_2, \"P(x_2, y_1)\": P_x_2_y_1, \"P(x_2, y_2)\": P_x_2_y_2,\n        \"P(y_1|x_1)\": P_x_1_correct, \"P(y_2|x_2)\": P_x_2_correct, \"P(x_1|y_1)\": P_y_1_correct, \"P(x_2|y_2)\": P_y_2_correct}\n    out_df = pd.DataFrame(cols)\n\n    if return_acc:\n        return correct/len(preds), out_df, preds, labels\n\n    return out_df, preds, labels\n\ndef compute_stats(total_df: pd.DataFrame, all_preds: List, all_labels: List) -> None:\n    print(\"overall accuracy: \")\n    accuracyy = len(np.where(np.array(all_preds) == np.array(all_labels))[0])/len(all_labels)\n    print(accuracyy)\n    print(\"confusion matrix: \")\n    matrix_dic = confusion_matrix(list(total_df[\"P(y_1|x_1)\"]), list(total_df[\"P(y_2|x_2)\"]), list(total_df[\"P(x_1|y_1)\"]), list(total_df[\"P(x_2|y_2)\"]))\n\n    return accuracyy, matrix_dic\n","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:29:45.045018Z","iopub.execute_input":"2023-10-17T06:29:45.045249Z","iopub.status.idle":"2023-10-17T06:29:49.380885Z","shell.execute_reply.started":"2023-10-17T06:29:45.045226Z","shell.execute_reply":"2023-10-17T06:29:49.379921Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip uninstall datasets -y\n!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:29:49.385818Z","iopub.execute_input":"2023-10-17T06:29:49.387987Z","iopub.status.idle":"2023-10-17T06:30:02.275063Z","shell.execute_reply.started":"2023-10-17T06:29:49.387955Z","shell.execute_reply":"2023-10-17T06:30:02.273919Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found existing installation: datasets 2.1.0\nUninstalling datasets-2.1.0:\n  Successfully uninstalled datasets-2.1.0\nCollecting datasets\n  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nCollecting fsspec[http]<2023.9.0,>=2023.1.0 (from datasets)\n  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nInstalling collected packages: fsspec, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.9.0\n    Uninstalling fsspec-2023.9.0:\n      Successfully uninstalled fsspec-2023.9.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ns3fs 2023.9.0 requires fsspec==2023.9.0, but you have fsspec 2023.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.14.5 fsspec-2023.6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:30:02.276738Z","iopub.execute_input":"2023-10-17T06:30:02.277862Z","iopub.status.idle":"2023-10-17T06:31:32.878436Z","shell.execute_reply.started":"2023-10-17T06:30:02.277821Z","shell.execute_reply":"2023-10-17T06:31:32.877114Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# install Hugging Face Libraries\n#!pip install \"peft==0.2.0\"\n#!pip install \"transformers==4.27.1\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes==0.37.1\" loralib --upgrade --quiet\n# install additional dependencies needed for training\n!pip install rouge-score tensorboard py7zr ","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:31:32.881080Z","iopub.execute_input":"2023-10-17T06:31:32.882029Z","iopub.status.idle":"2023-10-17T06:31:46.002831Z","shell.execute_reply.started":"2023-10-17T06:31:32.881991Z","shell.execute_reply":"2023-10-17T06:31:46.001615Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.12.3)\nCollecting py7zr\n  Downloading py7zr-0.20.6-py3-none-any.whl (66 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.4.3)\nRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (68.0.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.3.7)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.40.0)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.6.7)\nCollecting pycryptodomex>=3.6.6 (from py7zr)\n  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pyzstd>=0.14.4 (from py7zr)\n  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyppmd<1.1.0,>=0.18.1 (from py7zr)\n  Downloading pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pybcj>=0.6.0 (from py7zr)\n  Downloading pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr)\n  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nCollecting brotli>=1.0.9 (from py7zr)\n  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting inflate64>=0.3.1 (from py7zr)\n  Downloading inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from py7zr) (5.9.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=565a3a9603f543e32c0301e77c6267fec83dafb8b35e3528788b6480d9288489\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, rouge-score, py7zr\nSuccessfully installed brotli-1.1.0 inflate64-0.3.1 multivolumefile-0.2.3 py7zr-0.20.6 pybcj-1.0.1 pycryptodomex-3.19.0 pyppmd-1.0.0 pyzstd-0.15.9 rouge-score-0.1.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Load and prepare the dataset\n\nwe will use the [samsum](https://huggingface.co/datasets/samsum) dataset, a collection of about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English.\n\n```python\n{\n  \"id\": \"13818513\",\n  \"summary\": \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n  \"dialogue\": \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n}\n```\n\nTo load the `samsum` dataset, we use the **`load_dataset()`** method from the 🤗 Datasets library.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# huggingface hub model id\n\n#model_id = \"philschmid/flan-t5-xxl-sharded-fp16\"\nmodel_id = \"google/flan-t5-large\"\n\n# load model from the hub\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n# Load dataset from the hub\ndataset = load_dataset(\"nightingal3/fig-qa\")\n\nprint(f\"Validation dataset size: {len(dataset['validation'])}\")\n\n# %%\nmodel_id=\"google/flan-t5-large\"\n\n# Load tokenizer of FLAN-t5-XL\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# %%\ndef preprocess_function(sample, padding=\"max_length\"):\n    # Your startphrase will be the input and the correct ending will be your target\n    inputs = sample['startphrase']\n    \n    # Choose the correct ending based on the labels value for each sample in the batch\n    targets = [sample['ending1'][i] if sample['labels'][i] == 0 else sample['ending2'][i] for i in range(len(sample['labels']))]\n\n    # Tokenize inputs\n    model_inputs = tokenizer(inputs, max_length=50, padding=padding, truncation=True)\n    \n    # Tokenize targets\n    labels = tokenizer(targets, max_length=50, padding=padding, truncation=True)\n\n    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n    # padding in the loss.\n    if padding == \"max_length\":\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Applying the preprocessing function on the dataset\ntokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"startphrase\", \"ending1\", \"ending2\", \"labels\", \"valid\"])\n\nprint(f\"Keys of tokenized dataset: {list(tokenized_dataset['validation'].features)}\")\n\n# save datasets to disk for later easy loading\ntokenized_dataset[\"validation\"].save_to_disk(\"data/eval\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:31:46.004751Z","iopub.execute_input":"2023-10-17T06:31:46.005401Z","iopub.status.idle":"2023-10-17T06:31:52.446048Z","shell.execute_reply.started":"2023-10-17T06:31:46.005359Z","shell.execute_reply":"2023-10-17T06:31:52.445132Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/3.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09067bde71624d789741a6d57f5f37c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7382f32ac8114ccbb9624c68de7532c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/155k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70c70e2d0a1449e6904cc6e2b86e3406"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/21.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"196ec7b88e4548808820dc39be65a5f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/864k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78b4fa16c6bd472da9254b0397816458"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/116k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b50fe011dcb43e5a176075ffccd8d48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/120k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7ef86ded6f74bbe8d2bb90cfde57fa4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1378fffa94cd40cdbc344d76a4000283"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd3f0fd7d9e942c88496a2fe7512ae59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"860b6e5a4a564528b493facecc650e5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15c4bd83b7334c23b2a2a470f96cc27b"}},"metadata":{}},{"name":"stdout","text":"Validation dataset size: 1094\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f53fe8a68c34d88bdc0472f8b554376"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68c3b3177e054d5ea0694a6ade86273f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75ce5ffc1b194b40aba9648dab23a925"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa418436489044bf9b8d2e975f7a6584"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9674 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d92b6dd82be48a8874cefd2690d943d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1094 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16e8520822354e70b6e1f12ac2b634ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1146 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66ec7a7e27e1432b96dfb4c1aade5d6d"}},"metadata":{}},{"name":"stdout","text":"Keys of tokenized dataset: ['labels', 'input_ids', 'attention_mask']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1094 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bca07da313184125a9bc7856dc9da758"}},"metadata":{}}]},{"cell_type":"markdown","source":"dataset['validation'].","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# Print out a few examples from the raw dataset\nprint(\"Raw Data Examples: \", dataset['validation'][:3])\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# Apply preprocessing function to a few examples\n\ndummy = dataset['validation'].select(range(3))\n\npreprocessed_examples = dummy.map(preprocess_function, batched=True, remove_columns=[\"startphrase\", \"ending1\", \"ending2\", \"labels\", \"valid\"])\n\n\n# Print out preprocessed examples\nprint(\"Preprocessed Data Examples: \", preprocessed_examples)\n\n# Prepare a batch of data\ninput_ids = torch.tensor([example['input_ids'] for example in preprocessed_examples])\nattention_mask = torch.tensor([example['attention_mask'] for example in preprocessed_examples])\nlabels = torch.tensor([example['labels'] for example in preprocessed_examples])\n\n# Print out a few labels\nprint(\"Labels: \", labels)\n\n# Print out model input\nprint(\"Model Input: \", {\n    'input_ids': input_ids,\n    'attention_mask': attention_mask,\n    'labels': labels\n})\n\n# Decoding input_ids\ndecoded_input_ids = [tokenizer.decode(input_id) for input_id in input_ids]\nprint(\"Decoded Input IDs: \", decoded_input_ids)\n\n# Replace -100 with tokenizer.pad_token_id\nlabels_replaced = labels.clone()\nlabels_replaced[labels == -100] = tokenizer.pad_token_id\n\n# Decoding labels\ndecoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels_replaced]\nprint(\"Decoded Labels: \", decoded_labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-14T00:38:10.460153Z","iopub.execute_input":"2023-10-14T00:38:10.461271Z","iopub.status.idle":"2023-10-14T00:38:10.764520Z","shell.execute_reply.started":"2023-10-14T00:38:10.461235Z","shell.execute_reply":"2023-10-14T00:38:10.763394Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Raw Data Examples:  {'startphrase': ['The girl had the flightiness of a sparrow', 'The girl had the flightiness of a rock', 'It was as peaceful as a church.'], 'ending1': ['The girl was very fickle.', 'The girl was very fickle.', 'It was very peaceful.'], 'ending2': ['The girl was very stable.', 'The girl was very stable.', 'It was full of conflict and danger, not peace.'], 'labels': [0, 1, 0], 'valid': [1, 1, 1]}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74ef90805e6f4ecb891d296dd7fba7fb"}},"metadata":{}},{"name":"stdout","text":"Preprocessed Data Examples:  Dataset({\n    features: ['labels', 'input_ids', 'attention_mask'],\n    num_rows: 3\n})\nLabels:  tensor([[   37,  3202,    47,   182,   361, 19376,     5,     1,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n        [   37,  3202,    47,   182,  5711,     5,     1,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n        [   94,    47,   182,  9257,     5,     1,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])\nModel Input:  {'input_ids': tensor([[   37,  3202,   141,     8,  3777,  6096,    13,     3,     9, 14144,\n          3623,     1,     0,     0,     0,     0,     0,     0,     0,     0],\n        [   37,  3202,   141,     8,  3777,  6096,    13,     3,     9,  2480,\n             1,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [   94,    47,    38,  9257,    38,     3,     9,  2078,     5,     1,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[   37,  3202,    47,   182,   361, 19376,     5,     1,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n        [   37,  3202,    47,   182,  5711,     5,     1,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n        [   94,    47,   182,  9257,     5,     1,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])}\nDecoded Input IDs:  ['The girl had the flightiness of a sparrow</s><pad><pad><pad><pad><pad><pad><pad><pad>', 'The girl had the flightiness of a rock</s><pad><pad><pad><pad><pad><pad><pad><pad><pad>', 'It was as peaceful as a church.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\nDecoded Labels:  ['The girl was very fickle.', 'The girl was very stable.', 'It was very peaceful.']\n","output_type":"stream"}]},{"cell_type":"code","source":"subset_test_dataset = dataset['validation'].select(range(500))","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:37:45.639052Z","iopub.execute_input":"2023-10-17T06:37:45.639411Z","iopub.status.idle":"2023-10-17T06:37:45.649507Z","shell.execute_reply.started":"2023-10-17T06:37:45.639379Z","shell.execute_reply":"2023-10-17T06:37:45.648562Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"out_df, preds, labels = evaluate_model(model, tokenizer, subset_test_dataset, verbose = False, total = 500)","metadata":{"execution":{"iopub.status.busy":"2023-10-14T00:38:10.774367Z","iopub.execute_input":"2023-10-14T00:38:10.775200Z","iopub.status.idle":"2023-10-14T00:38:11.304981Z","shell.execute_reply.started":"2023-10-14T00:38:10.775141Z","shell.execute_reply":"2023-10-14T00:38:11.303099Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out_df, preds, labels \u001b[38;5;241m=\u001b[39m evaluate_model(\u001b[43mmodel\u001b[49m, tokenizer, subset_test_dataset, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}]},{"cell_type":"code","source":"zero_shot_accuracy, conf_matrix_zero_shot =  compute_stats(out_df, preds, labels)","metadata":{"execution":{"iopub.status.busy":"2023-10-14T00:38:11.306151Z","iopub.status.idle":"2023-10-14T00:38:11.306840Z","shell.execute_reply.started":"2023-10-14T00:38:11.306577Z","shell.execute_reply":"2023-10-14T00:38:11.306602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:37:24.423989Z","iopub.execute_input":"2023-10-17T06:37:24.424321Z","iopub.status.idle":"2023-10-17T06:37:32.997130Z","shell.execute_reply.started":"2023-10-17T06:37:24.424294Z","shell.execute_reply":"2023-10-17T06:37:32.995930Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.1)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.14.5)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nimport numpy as np\nfrom datasets import load_from_disk\nfrom tqdm import tqdm\n\n# Metric\nmetric = evaluate.load(\"rouge\")\n\ndef evaluate_peft_model(sample, max_target_length=50):\n    # generate summary\n    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    \n    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n    \n    # decode eval sample\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n    labels = tokenizer.decode(labels, skip_special_tokens=True)\n\n    # Print inputs, predicted summary, and reference summary\n    input_text = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\n    print(f\"Input: {input_text}\")\n    print(f\"Predicted Summary: {prediction}\")\n    print(f\"Reference Summary: {labels}\")\n    print(\"=\"*50)  # prints a separator\n\n    # Some simple post-processing\n    return prediction, labels\n\n# load test dataset from distk\ntest_dataset = load_from_disk(\"data/eval/\").with_format(\"torch\")\n\n# run predictions\n# this can take ~45 minutes\npredictions, references = [] , []\nfor i,sample in tqdm(enumerate(test_dataset)):\n    p,l = evaluate_peft_model(sample)\n    predictions.append(p)\n    references.append(l)\n    if i == 20:\n        break\n\n# compute metric \nrogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n\n# print results \nprint(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\nprint(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\nprint(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\nprint(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")\n\n# Rogue1: 50.386161%\n# rouge2: 24.842412%\n# rougeL: 41.370130%\n# rougeLsum: 41.394230%","metadata":{"execution":{"iopub.status.busy":"2023-10-14T00:40:09.395874Z","iopub.execute_input":"2023-10-14T00:40:09.396666Z","iopub.status.idle":"2023-10-14T00:40:12.192999Z","shell.execute_reply.started":"2023-10-14T00:40:09.396628Z","shell.execute_reply":"2023-10-14T00:40:12.191575Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edac9ea9300942a89a4ab385a80715ad"}},"metadata":{}},{"name":"stderr","text":"0it [00:00, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m predictions, references \u001b[38;5;241m=\u001b[39m [] , []\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,sample \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(test_dataset)):\n\u001b[0;32m---> 36\u001b[0m     p,l \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[1;32m     38\u001b[0m     references\u001b[38;5;241m.\u001b[39mappend(l)\n","Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36mevaluate_peft_model\u001b[0;34m(sample, max_target_length)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_peft_model\u001b[39m(sample, max_target_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# generate summary\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39msample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda(), do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, max_new_tokens\u001b[38;5;241m=\u001b[39mmax_target_length)    \n\u001b[1;32m     12\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# decode eval sample\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Replace -100 in the labels as we can't decode them.\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"## 3. Fine-Tune T5 with LoRA and bnb int-8\n\nIn addition to the LoRA technique, we will use [bitsanbytes LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration) to quantize out frozen LLM to int8. This allows us to reduce the needed memory for FLAN-T5 XXL ~4x.  \n\nThe first step of our training is to load the model. We are going to use [philschmid/flan-t5-xxl-sharded-fp16](https://huggingface.co/philschmid/flan-t5-xxl-sharded-fp16), which is a sharded version of [google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl). The sharding will help us to not run off of memory when loading the model.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:32:36.594393Z","iopub.execute_input":"2023-10-17T06:32:36.594945Z","iopub.status.idle":"2023-10-17T06:32:36.626431Z","shell.execute_reply.started":"2023-10-17T06:32:36.594915Z","shell.execute_reply":"2023-10-17T06:32:36.625599Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cb32bc522e548a9ba8fba9e82ebaaa7"}},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\npeft_model_id = \"davidguzmanp/T5-Large-figQA-seq2seq\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\ninference_model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\nmodel = PeftModel.from_pretrained(inference_model, peft_model_id)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T07:01:13.560529Z","iopub.execute_input":"2023-10-17T07:01:13.560934Z","iopub.status.idle":"2023-10-17T07:02:20.038322Z","shell.execute_reply.started":"2023-10-17T07:01:13.560906Z","shell.execute_reply":"2023-10-17T07:02:20.037362Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"----","metadata":{}},{"cell_type":"code","source":"out_df, preds, labels = evaluate_model(model, tokenizer, subset_test_dataset, verbose = False, total = 500)\nzero_shot_accuracy, conf_matrix_zero_shot =  compute_stats(out_df, preds, labels)","metadata":{"execution":{"iopub.status.busy":"2023-10-14T00:58:33.135430Z","iopub.execute_input":"2023-10-14T00:58:33.135807Z","iopub.status.idle":"2023-10-14T01:09:05.220627Z","shell.execute_reply.started":"2023-10-14T00:58:33.135776Z","shell.execute_reply":"2023-10-14T01:09:05.219672Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"100%|██████████| 500/500 [10:32<00:00,  1.26s/it]","output_type":"stream"},{"name":"stdout","text":"overall accuracy: \n0.626\nconfusion matrix: \ncorrect forward 313 wrong forward 187 correct backward 314 wrong_backward 186\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"model.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:38:52.156330Z","iopub.execute_input":"2023-10-17T06:38:52.156691Z","iopub.status.idle":"2023-10-17T06:38:53.032521Z","shell.execute_reply.started":"2023-10-17T06:38:52.156661Z","shell.execute_reply":"2023-10-17T06:38:53.031622Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"PeftModelForSeq2SeqLM(\n  (base_model): LoraModel(\n    (model): T5ForConditionalGeneration(\n      (shared): Embedding(32128, 1024)\n      (encoder): T5Stack(\n        (embed_tokens): Embedding(32128, 1024)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n                  (v): Linear(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n                  (relative_attention_bias): Embedding(32, 16)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n                  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1-23): 23 x T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n                  (v): Linear(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n                  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (decoder): T5Stack(\n        (embed_tokens): Embedding(32128, 1024)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n                  (v): Linear(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n                  (relative_attention_bias): Embedding(32, 16)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n                  (v): Linear(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n                  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1-23): 23 x T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n                  (v): Linear(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n                  (v): Linear(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n                  (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"import evaluate\nimport numpy as np\nfrom datasets import load_from_disk\nfrom tqdm import tqdm\n\n# Metric\nmetric = evaluate.load(\"rouge\")\n\n# def evaluate_peft_model(sample,max_target_length=50):\n#     # generate summary\n#     outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    \n#     prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n#     # decode eval sample\n#     # Replace -100 in the labels as we can't decode them.\n#     labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n#     labels = tokenizer.decode(labels, skip_special_tokens=True)\n\n#     # Some simple post-processing\n#     return prediction, labels\n\ndef evaluate_peft_model(sample, max_target_length=50):\n    # generate summary\n    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    \n    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n    \n    # decode eval sample\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n    labels = tokenizer.decode(labels, skip_special_tokens=True)\n\n    # Print inputs, predicted summary, and reference summary\n    input_text = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\n    print(f\"Input: {input_text}\")\n    print(f\"Predicted Summary: {prediction}\")\n    print(f\"Reference Summary: {labels}\")\n    print(\"=\"*50)  # prints a separator\n\n    # Some simple post-processing\n    return prediction, labels\n\n# load test dataset from distk\ntest_dataset = load_from_disk(\"data/eval/\").with_format(\"torch\")\n\n# run predictions\n# this can take ~45 minutes\npredictions, references = [] , []\nfor i,sample in tqdm(enumerate(test_dataset)):\n    p,l = evaluate_peft_model(sample)\n    predictions.append(p)\n    references.append(l)\n    if i == 20:\n        break\n\n# compute metric \nrogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n\n# print results \nprint(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\nprint(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\nprint(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\nprint(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")\n\n# Rogue1: 50.386161%\n# rouge2: 24.842412%\n# rougeL: 41.370130%\n# rougeLsum: 41.394230%","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:38:57.379970Z","iopub.execute_input":"2023-10-17T06:38:57.380881Z","iopub.status.idle":"2023-10-17T06:39:07.231050Z","shell.execute_reply.started":"2023-10-17T06:38:57.380848Z","shell.execute_reply":"2023-10-17T06:39:07.230073Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"1it [00:03,  3.45s/it]","output_type":"stream"},{"name":"stdout","text":"Input: The girl had the flightiness of a sparrow\nPredicted Summary: The girl was quick-witted and flighty\nReference Summary: The girl was very fickle.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"2it [00:03,  1.61s/it]","output_type":"stream"},{"name":"stdout","text":"Input: The girl had the flightiness of a rock\nPredicted Summary: The girl was not flighty\nReference Summary: The girl was very stable.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"3it [00:04,  1.02it/s]","output_type":"stream"},{"name":"stdout","text":"Input: It was as peaceful as a church.\nPredicted Summary: It was peaceful.\nReference Summary: It was very peaceful.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"4it [00:04,  1.40it/s]","output_type":"stream"},{"name":"stdout","text":"Input: It was as peaceful as a battlefield.\nPredicted Summary: It was tense.\nReference Summary: It was full of conflict and danger, not peace.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"5it [00:04,  1.86it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The leaves were as green as grass\nPredicted Summary: The leaves were green\nReference Summary: The leaves were very green\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"6it [00:04,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The leaves were as green as dirt\nPredicted Summary: The leaves were dirty\nReference Summary: The leaves were brown and not green at all.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"7it [00:05,  2.28it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Shopping for groceries is finding shells on a sunny beach\nPredicted Summary: shopping for groceries is a fun, relaxing experience\nReference Summary: Shopping for groceries is a fun, rewarding chore\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"8it [00:05,  2.21it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Shopping for groceries is a scavenger hunt with a list created by a lunatic\nPredicted Summary: Shopping for groceries is a very boring and repetitive process\nReference Summary: Shopping for groceries is a crazy, nearly impossible chore\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"9it [00:05,  2.52it/s]","output_type":"stream"},{"name":"stdout","text":"Input: War is an amputation on the wrong limb\nPredicted Summary: War is very confusing.\nReference Summary: War is the wrong solution to a problem\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"10it [00:06,  2.47it/s]","output_type":"stream"},{"name":"stdout","text":"Input: War is an amputation to save your life\nPredicted Summary: War is the best way to stay alive\nReference Summary: War is a necessary solution\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"11it [00:06,  2.86it/s]","output_type":"stream"},{"name":"stdout","text":"Input: It's as green as grass in the spring\nPredicted Summary: It's green\nReference Summary: It's fairy green\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"13it [00:07,  3.67it/s]","output_type":"stream"},{"name":"stdout","text":"Input: It's as green as grass during a hot summer\nPredicted Summary: it's green\nReference Summary: It's not too green\n==================================================\nInput: This is as peaceful as a sleeping puppy\nPredicted Summary: This is calm\nReference Summary: It's very peaceful\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"14it [00:07,  3.85it/s]","output_type":"stream"},{"name":"stdout","text":"Input: This is as peaceful as European in the '40s\nPredicted Summary: This is very peaceful\nReference Summary: It's not very peaceful\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"15it [00:07,  3.85it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The music was loud like a siren.\nPredicted Summary: The music was loud.\nReference Summary: The music was very loud.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"16it [00:07,  3.65it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The music was loud like a whisper.\nPredicted Summary: The music was very quiet.\nReference Summary: The music was very quiet.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"17it [00:08,  3.70it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Jobs are as available as a marriage man.\nPredicted Summary: Jobs are not available.\nReference Summary: Jobs are not available.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"19it [00:08,  4.14it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Jobs are as available as a bachelor.\nPredicted Summary: Jobs are very scarce.\nReference Summary: Jobs are very available.\n==================================================\nInput: Peace is a human flying\nPredicted Summary: Peace is peaceful\nReference Summary: Peace is impossible\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"20it [00:09,  2.86it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Peace is a human walking\nPredicted Summary: Peace is an individual that knows he's not a leader\nReference Summary: Peace is possible\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"20it [00:09,  2.13it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Plants are a lullaby\nPredicted Summary: Plants are not scary\nReference Summary: Plants are calming\n==================================================\nRogue1: 61.168833%\nrouge2: 44.466211%\nrougeL: 60.693087%\nrougeLsum: 60.796864%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:47:28.242324Z","iopub.execute_input":"2023-10-17T06:47:28.242703Z","iopub.status.idle":"2023-10-17T06:47:28.253259Z","shell.execute_reply.started":"2023-10-17T06:47:28.242675Z","shell.execute_reply":"2023-10-17T06:47:28.252070Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'labels': [37,\n  3202,\n  47,\n  182,\n  361,\n  19376,\n  5,\n  1,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100],\n 'input_ids': [37,\n  3202,\n  141,\n  8,\n  3777,\n  6096,\n  13,\n  3,\n  9,\n  14144,\n  3623,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0]}"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\n\ndef evaluate_peft_model(sample, max_target_length=50):\n    # generate summary\n    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    \n    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n    \n    # decode eval sample\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n    labels = tokenizer.decode(labels, skip_special_tokens=True)\n\n    # Print inputs, predicted summary, and reference summary\n    input_text = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\n    print(f\"Input: {input_text}\")\n    print(f\"Predicted Summary: {prediction}\")\n    print(f\"Reference Summary: {labels}\")\n    print(\"=\"*50)  # prints a separator\n\n    # Some simple post-processing\n    return input_text, prediction, labels\n\n\n# run predictions\nmetaphors, predictions, references = [], [], []\nfor i, sample in tqdm(enumerate(test_dataset)):\n    m, p, l = evaluate_peft_model(sample)\n    metaphors.append(m)\n    predictions.append(p)\n    references.append(l)\n    if i == 49:  # stop after 50 samples\n        break\n\n# Convert the results to a DataFrame and save to a CSV\ndf = pd.DataFrame({\n    'metaphor': metaphors,\n    'model interpretation': predictions,\n    'reference correct interpretation': references\n})\n\ndf.to_csv('predictions.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:51:45.401659Z","iopub.execute_input":"2023-10-17T06:51:45.402047Z","iopub.status.idle":"2023-10-17T06:52:01.470996Z","shell.execute_reply.started":"2023-10-17T06:51:45.402022Z","shell.execute_reply":"2023-10-17T06:52:01.470082Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"1it [00:00,  3.11it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The girl had the flightiness of a sparrow\nPredicted Summary: The girl was flighty.\nReference Summary: The girl was very fickle.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"2it [00:00,  3.28it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The girl had the flightiness of a rock\nPredicted Summary: The girl is very calm\nReference Summary: The girl was very stable.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"3it [00:00,  3.28it/s]","output_type":"stream"},{"name":"stdout","text":"Input: It was as peaceful as a church.\nPredicted Summary: It was quiet and peaceful.\nReference Summary: It was very peaceful.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"4it [00:01,  3.29it/s]","output_type":"stream"},{"name":"stdout","text":"Input: It was as peaceful as a battlefield.\nPredicted Summary: The experience was not peaceful.\nReference Summary: It was full of conflict and danger, not peace.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"5it [00:01,  3.15it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The leaves were as green as grass\nPredicted Summary: The leaves were green\nReference Summary: The leaves were very green\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"6it [00:01,  2.88it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The leaves were as green as dirt\nPredicted Summary: The leaves were brown.\nReference Summary: The leaves were brown and not green at all.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"7it [00:02,  2.36it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Shopping for groceries is finding shells on a sunny beach\nPredicted Summary: Shopping for groceries is a joyous experience\nReference Summary: Shopping for groceries is a fun, rewarding chore\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"9it [00:03,  2.60it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Shopping for groceries is a scavenger hunt with a list created by a lunatic\nPredicted Summary: Shopping for groceries is full of confusion and boredom.\nReference Summary: Shopping for groceries is a crazy, nearly impossible chore\n==================================================\nInput: War is an amputation on the wrong limb\nPredicted Summary: War is bad\nReference Summary: War is the wrong solution to a problem\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"10it [00:03,  3.09it/s]","output_type":"stream"},{"name":"stdout","text":"Input: War is an amputation to save your life\nPredicted Summary: War is painful\nReference Summary: War is a necessary solution\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"11it [00:03,  3.24it/s]","output_type":"stream"},{"name":"stdout","text":"Input: It's as green as grass in the spring\nPredicted Summary: It's pretty green\nReference Summary: It's fairy green\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"13it [00:04,  3.78it/s]","output_type":"stream"},{"name":"stdout","text":"Input: It's as green as grass during a hot summer\nPredicted Summary: It's very green\nReference Summary: It's not too green\n==================================================\nInput: This is as peaceful as a sleeping puppy\nPredicted Summary: This is peaceful\nReference Summary: It's very peaceful\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"14it [00:04,  3.96it/s]","output_type":"stream"},{"name":"stdout","text":"Input: This is as peaceful as European in the '40s\nPredicted Summary: This is very peaceful\nReference Summary: It's not very peaceful\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"15it [00:04,  3.72it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The music was loud like a siren.\nPredicted Summary: The music was very loud.\nReference Summary: The music was very loud.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"16it [00:05,  3.58it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The music was loud like a whisper.\nPredicted Summary: The music was very quiet.\nReference Summary: The music was very quiet.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"17it [00:05,  3.35it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Jobs are as available as a marriage man.\nPredicted Summary: Jobs are very hard to find.\nReference Summary: Jobs are not available.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"19it [00:05,  3.77it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Jobs are as available as a bachelor.\nPredicted Summary: Job openings are scarce.\nReference Summary: Jobs are very available.\n==================================================\nInput: Peace is a human flying\nPredicted Summary: Peace is stable\nReference Summary: Peace is impossible\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"20it [00:06,  3.62it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Peace is a human walking\nPredicted Summary: Peace is a living thing\nReference Summary: Peace is possible\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"21it [00:06,  3.84it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Plants are a lullaby\nPredicted Summary: Plants are quiet\nReference Summary: Plants are calming\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"22it [00:06,  3.97it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Plants are a loud drum\nPredicted Summary: Plants are noisy\nReference Summary: Plants are disturbing\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"23it [00:06,  4.09it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The car was as ugly as a one eyed rat\nPredicted Summary: The car was ugly\nReference Summary: it was hideous\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"24it [00:07,  3.99it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The car was as ugly as a swan\nPredicted Summary: The car was beautiful.\nReference Summary: it was beautiful\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"25it [00:07,  4.13it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The man was as handsome as a prince\nPredicted Summary: The man was beautiful\nReference Summary: he was good looking\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"26it [00:07,  4.19it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The man was as handsome as a hobo\nPredicted Summary: The man was ugly\nReference Summary: he was ugly\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"27it [00:07,  3.54it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The conversation was sharp as a tack\nPredicted Summary: The conversation was concise and easy to understand\nReference Summary: The conversation was sharp and witty.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"28it [00:08,  3.69it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The conversation was sharp as a rock\nPredicted Summary: The conversation was quiet\nReference Summary: The conversation was dull and not sharp.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"29it [00:08,  3.37it/s]","output_type":"stream"},{"name":"stdout","text":"Input: He ate it like a fat boy eats cake\nPredicted Summary: He ate it very fast\nReference Summary: The food was tasty to him\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"30it [00:08,  3.47it/s]","output_type":"stream"},{"name":"stdout","text":"Input: He ate it like a young boy eats broccoli\nPredicted Summary: He ate it slowly\nReference Summary: The food was unpalatable to him\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"31it [00:09,  3.69it/s]","output_type":"stream"},{"name":"stdout","text":"Input: He picked it up like a mother holding a baby\nPredicted Summary: He picked it up\nReference Summary: He held it with pride and care\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"32it [00:09,  3.43it/s]","output_type":"stream"},{"name":"stdout","text":"Input: He picked it up like a playboy holding a condom\nPredicted Summary: He didn't pick it up\nReference Summary: He held it with disgust and caution\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"33it [00:09,  3.14it/s]","output_type":"stream"},{"name":"stdout","text":"Input: He rushed through the math test like an ape\nPredicted Summary: He had no time for the math test\nReference Summary: He rushed because he is dumb\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"34it [00:10,  2.69it/s]","output_type":"stream"},{"name":"stdout","text":"Input: He rushed through the math test like a rocket scientist\nPredicted Summary: He rushed through the test in a hurry.\nReference Summary: He rushed because he is smart\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"35it [00:10,  2.95it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The bear was as hungry as a lion\nPredicted Summary: the bear was very hungry\nReference Summary: it was starving\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"36it [00:10,  3.15it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The bear was as hungry as a piece of paper\nPredicted Summary: The bear was not hungry\nReference Summary: it didn't need food\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"37it [00:11,  3.08it/s]","output_type":"stream"},{"name":"stdout","text":"Input: That conversation had the ease of doing your taxes blindfolded.\nPredicted Summary: The conversation was easy to understand.\nReference Summary: Having that conversation was difficult\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"38it [00:11,  3.04it/s]","output_type":"stream"},{"name":"stdout","text":"Input: That conversation had the ease of a Sunday morning.\nPredicted Summary: The conversation was not very easy.\nReference Summary: Having that conversation was easy.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"39it [00:11,  3.10it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Their conversations were artillery bombardments.\nPredicted Summary: Their conversations were demoralizing\nReference Summary: Their conversations were heated and antagonistic.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"40it [00:12,  3.04it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Their conversations were a hug with words.\nPredicted Summary: Their conversations were warm and kind.\nReference Summary: Their conversations were friendly.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"41it [00:12,  3.23it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The story was as disturbing as a nightmare\nPredicted Summary: The story was very disturbing\nReference Summary: The story was very disturbing.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"42it [00:12,  3.26it/s]","output_type":"stream"},{"name":"stdout","text":"Input: The story was as disturbing as a newborn puppy\nPredicted Summary: The story was not very disturbing\nReference Summary: The story failed to be disturbing, and in fact seemed cute.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"43it [00:13,  2.85it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Their expectations of the house they could afford turned into melted ice.\nPredicted Summary: The house they could afford was not very nice.\nReference Summary: Their expectations were not met.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"44it [00:13,  2.46it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Their expectations of the house they could afford leapt past the second story.\nPredicted Summary: They expected a big house to be large and modern.\nReference Summary: Their expectations were exceeded.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"45it [00:14,  2.64it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Those that heard the child sing were carried away on gentle waves.\nPredicted Summary: The child was singing well.\nReference Summary: Those that heard the singing were pleasantly entertained.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"46it [00:14,  2.22it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Those that heard the child sing were tortured by the intruding notes.\nPredicted Summary: The child's voice sounded strange and disorienting.\nReference Summary: Those that heard the singing were unpleasantly inundated.\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"47it [00:14,  2.54it/s]","output_type":"stream"},{"name":"stdout","text":"Input: She sings like an angel\nPredicted Summary: She sings very well\nReference Summary: her voice is magical\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"48it [00:15,  2.57it/s]","output_type":"stream"},{"name":"stdout","text":"Input: She sings like a bullfrog\nPredicted Summary: She's really bad at singing.\nReference Summary: her voice is awful\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"49it [00:15,  2.66it/s]","output_type":"stream"},{"name":"stdout","text":"Input: HIs opinions were as firm as concrete\nPredicted Summary: HIs opinions were logical\nReference Summary: He was very certain of his opinion\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"49it [00:16,  3.05it/s]","output_type":"stream"},{"name":"stdout","text":"Input: HIs opinions were as firm as a cotton ball\nPredicted Summary: HIs opinions were shaky\nReference Summary: He was very uncertain of his opinion\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:54:16.129125Z","iopub.execute_input":"2023-10-17T06:54:16.129684Z","iopub.status.idle":"2023-10-17T06:54:16.150296Z","shell.execute_reply.started":"2023-10-17T06:54:16.129652Z","shell.execute_reply":"2023-10-17T06:54:16.149198Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                    metaphor  \\\n0  The girl had the flightiness of a sparrow   \n1     The girl had the flightiness of a rock   \n2            It was as peaceful as a church.   \n3       It was as peaceful as a battlefield.   \n4          The leaves were as green as grass   \n\n               model interpretation  \\\n0             The girl was flighty.   \n1             The girl is very calm   \n2        It was quiet and peaceful.   \n3  The experience was not peaceful.   \n4             The leaves were green   \n\n                 reference correct interpretation  \n0                       The girl was very fickle.  \n1                       The girl was very stable.  \n2                           It was very peaceful.  \n3  It was full of conflict and danger, not peace.  \n4                      The leaves were very green  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>metaphor</th>\n      <th>model interpretation</th>\n      <th>reference correct interpretation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The girl had the flightiness of a sparrow</td>\n      <td>The girl was flighty.</td>\n      <td>The girl was very fickle.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The girl had the flightiness of a rock</td>\n      <td>The girl is very calm</td>\n      <td>The girl was very stable.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>It was as peaceful as a church.</td>\n      <td>It was quiet and peaceful.</td>\n      <td>It was very peaceful.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>It was as peaceful as a battlefield.</td>\n      <td>The experience was not peaceful.</td>\n      <td>It was full of conflict and danger, not peace.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The leaves were as green as grass</td>\n      <td>The leaves were green</td>\n      <td>The leaves were very green</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}