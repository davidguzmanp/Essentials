{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Efficiently train Large Language Models with LoRA and Hugging Face\n\nIn this blog, we are going to show you how to apply [Low-Rank Adaptation of Large Language Models (LoRA)](https://arxiv.org/abs/2106.09685) to fine-tune FLAN-T5 XXL (11 billion parameters) on a single GPU. We are going to leverage Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), and [PEFT](https://github.com/huggingface/peft). \n\nYou will learn how to:\n\n1. Setup Development Environment\n2. Load and prepare the dataset\n3. Fine-Tune T5 with LoRA and bnb int-8\n4. Evaluate & run Inference with LoRA FLAN-T5\n5. Cost performance comparison\n\n### Quick intro: PEFT or Parameter Efficient Fine-tunin\n\n[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT currently includes techniques for:\n\n- LoRA:¬†[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n- Prefix Tuning:¬†[P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n- P-Tuning:¬†[GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n- Prompt Tuning:¬†[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n\n*Note: This tutorial was created and run on a g5.2xlarge AWS EC2 Instance, including 1 NVIDIA A10G.*","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup Development Environment\n\nIn our example, we use the [PyTorch Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-pytorch.html) with already set up CUDA drivers and PyTorch installed. We still have to install the Hugging Face Libraries, including transformers and datasets. Running the following cell will install all the required packages.","metadata":{}},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:04:21.390015Z","iopub.execute_input":"2023-10-13T20:04:21.390424Z","iopub.status.idle":"2023-10-13T20:06:10.228684Z","shell.execute_reply.started":"2023-10-13T20:04:21.390400Z","shell.execute_reply":"2023-10-13T20:06:10.227222Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# install Hugging Face Libraries\n#!pip install \"peft==0.2.0\"\n#!pip install \"transformers==4.27.1\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes==0.37.1\" loralib --upgrade --quiet\n# install additional dependencies needed for training\n!pip install rouge-score tensorboard py7zr ","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:06:10.231497Z","iopub.execute_input":"2023-10-13T20:06:10.231959Z","iopub.status.idle":"2023-10-13T20:06:26.107663Z","shell.execute_reply.started":"2023-10-13T20:06:10.231915Z","shell.execute_reply":"2023-10-13T20:06:26.106368Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.12.3)\nCollecting py7zr\n  Downloading py7zr-0.20.6-py3-none-any.whl (66 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.7/66.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.4.3)\nRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (68.0.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.3.7)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.40.0)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.6.7)\nCollecting pycryptodomex>=3.6.6 (from py7zr)\n  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting pyzstd>=0.14.4 (from py7zr)\n  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyppmd<1.1.0,>=0.18.1 (from py7zr)\n  Downloading pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pybcj>=0.6.0 (from py7zr)\n  Downloading pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr)\n  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nCollecting brotli>=1.0.9 (from py7zr)\n  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting inflate64>=0.3.1 (from py7zr)\n  Downloading inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from py7zr) (5.9.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=11c37c0dca42ff8d4f945e5c571795b886928f58594dfd8555406078be5fc536\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, rouge-score, py7zr\nSuccessfully installed brotli-1.1.0 inflate64-0.3.1 multivolumefile-0.2.3 py7zr-0.20.6 pybcj-1.0.1 pycryptodomex-3.19.0 pyppmd-1.0.0 pyzstd-0.15.9 rouge-score-0.1.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Load and prepare the dataset\n\nwe will use the¬†[samsum](https://huggingface.co/datasets/samsum)¬†dataset, a collection of about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English.\n\n```python\n{\n  \"id\": \"13818513\",\n  \"summary\": \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n  \"dialogue\": \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n}\n```\n\nTo load the¬†`samsum`¬†dataset, we use the¬†**`load_dataset()`**¬†method from the ü§ó Datasets library.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load dataset from the hub\ndataset = load_dataset(\"samsum\")\n\nprint(f\"Train dataset size: {len(dataset['train'])}\")\nprint(f\"Test dataset size: {len(dataset['test'])}\")\n\n# Train dataset size: 14732\n# Test dataset size: 819","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:06:26.113392Z","iopub.execute_input":"2023-10-13T20:06:26.116351Z","iopub.status.idle":"2023-10-13T20:06:32.804501Z","shell.execute_reply.started":"2023-10-13T20:06:26.116306Z","shell.execute_reply":"2023-10-13T20:06:32.803626Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"150d91621e454e0690ad5a9e863b2949"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/770 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7308e7264a8e417fa5a376d2a535c370"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset samsum/samsum (download: 2.81 MiB, generated: 10.04 MiB, post-processed: Unknown size, total: 12.85 MiB) to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53e5e2ee95af4b699952b441729fdf39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset samsum downloaded and prepared to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a73d6e6832047a0b50ca35837aa6c78"}},"metadata":{}},{"name":"stdout","text":"Train dataset size: 14732\nTest dataset size: 819\n","output_type":"stream"}]},{"cell_type":"markdown","source":"To train our model, we need to convert our inputs (text) to token IDs. This is done by a ü§ó Transformers Tokenizer. If you are not sure what this means, check out¬†**[chapter 6](https://huggingface.co/course/chapter6/1?fw=tf)**¬†of the Hugging Face Course.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n#model_id=\"google/flan-t5-xxl\"\nmodel_id=\"google/flan-t5-base\"\n\n# Load tokenizer of FLAN-t5-XL\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:06:32.806245Z","iopub.execute_input":"2023-10-13T20:06:32.807289Z","iopub.status.idle":"2023-10-13T20:06:36.460296Z","shell.execute_reply.started":"2023-10-13T20:06:32.807244Z","shell.execute_reply":"2023-10-13T20:06:36.459228Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2caa992b1dc4cdca45b645cd2582486"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc7662e18a9543b4968f819593d742f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"461b661410044f65894fbfd11f447d4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"154dd3dfb17b4004b3a85e566167d675"}},"metadata":{}}]},{"cell_type":"markdown","source":"Before we can start training, we need to preprocess our data. Abstractive Summarization is a text-generation task. Our model will take a text as input and generate a summary as output. We want to understand how long our input and output will take to batch our data efficiently.","metadata":{}},{"cell_type":"code","source":"from datasets import concatenate_datasets\nimport numpy as np\n# The maximum total input sequence length after tokenization. \n# Sequences longer than this will be truncated, sequences shorter will be padded.\ntokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\ninput_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n# take 85 percentile of max length for better utilization\nmax_source_length = int(np.percentile(input_lenghts, 85))\nprint(f\"Max source length: {max_source_length}\")\n\n# The maximum total sequence length for target text after tokenization. \n# Sequences longer than this will be truncated, sequences shorter will be padded.\"\ntokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\ntarget_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n# take 90 percentile of max length for better utilization\nmax_target_length = int(np.percentile(target_lenghts, 90))\nprint(f\"Max target length: {max_target_length}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:06:43.766921Z","iopub.execute_input":"2023-10-13T20:06:43.767255Z","iopub.status.idle":"2023-10-13T20:06:53.282569Z","shell.execute_reply.started":"2023-10-13T20:06:43.767227Z","shell.execute_reply":"2023-10-13T20:06:53.281438Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72676b95b38945559ac87be62b71585f"}},"metadata":{}},{"name":"stdout","text":"Max source length: 255\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3768a6e1da943958713cc227bcb6156"}},"metadata":{}},{"name":"stdout","text":"Max target length: 50\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We preprocess our dataset before training and save it to disk. You could run this step on your local machine or a CPU and upload it to the [Hugging Face Hub](https://huggingface.co/docs/hub/datasets-overview).","metadata":{}},{"cell_type":"code","source":"def preprocess_function(sample,padding=\"max_length\"):\n    # add prefix to the input for t5\n    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n\n    # tokenize inputs\n    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n\n    # Tokenize targets with the `text_target` keyword argument\n    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n\n    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n    # padding in the loss.\n    if padding == \"max_length\":\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\nprint(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n\n# save datasets to disk for later easy loading\ntokenized_dataset[\"train\"].save_to_disk(\"data/train\")\ntokenized_dataset[\"test\"].save_to_disk(\"data/eval\")","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:06:53.285142Z","iopub.execute_input":"2023-10-13T20:06:53.285971Z","iopub.status.idle":"2023-10-13T20:07:02.491984Z","shell.execute_reply.started":"2023-10-13T20:06:53.285927Z","shell.execute_reply":"2023-10-13T20:07:02.490906Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b82f8825b4744907912425abc43679c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a621eb2f4a54338a444981fa2230d43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2788e0ee2664c6cba19219365b01f12"}},"metadata":{}},{"name":"stdout","text":"Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3. Fine-Tune T5 with LoRA and bnb int-8\n\nIn addition to the LoRA technique, we will use [bitsanbytes LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration) to quantize out frozen LLM to int8. This allows us to reduce the needed memory for FLAN-T5 XXL ~4x.  \n\nThe first step of our training is to load the model. We are going to use [philschmid/flan-t5-xxl-sharded-fp16](https://huggingface.co/philschmid/flan-t5-xxl-sharded-fp16), which is a sharded version of [google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl). The sharding will help us to not run off of memory when loading the model.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# huggingface hub model id\n\n#model_id = \"philschmid/flan-t5-xxl-sharded-fp16\"\nmodel_id = \"google/flan-t5-base\"\n\n# load model from the hub\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:07:47.221479Z","iopub.execute_input":"2023-10-13T20:07:47.222063Z","iopub.status.idle":"2023-10-13T20:08:04.200941Z","shell.execute_reply.started":"2023-10-13T20:07:47.222027Z","shell.execute_reply":"2023-10-13T20:08:04.199907Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a8f30baa26a4281ae665183eb1705cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba917fa8ed534506969ac1f1f995faa9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5f35a379b8a47e78d5c716bd7b900d4"}},"metadata":{}}]},{"cell_type":"markdown","source":"Now, we can prepare our model for the LoRA int-8 training using¬†`peft`.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n\n# Define LoRA Config \nlora_config = LoraConfig(\n r=16, \n lora_alpha=32,\n target_modules=[\"q\", \"v\"],\n lora_dropout=0.05,\n bias=\"none\",\n task_type=TaskType.SEQ_2_SEQ_LM\n)\n# prepare int-8 model for training\nmodel = prepare_model_for_int8_training(model)\n\n# add LoRA adaptor\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# trainable params: 18874368 || all params: 11154206720 || trainable%: 0.16921300163961817","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:08:11.134053Z","iopub.execute_input":"2023-10-13T20:08:11.134420Z","iopub.status.idle":"2023-10-13T20:08:11.273540Z","shell.execute_reply.started":"2023-10-13T20:08:11.134390Z","shell.execute_reply":"2023-10-13T20:08:11.272653Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"trainable params: 1,769,472 || all params: 249,347,328 || trainable%: 0.7096414524241463\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:107: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As you can see, here we are only training 0.16% of the parameters of the model! This huge memory gain will enable us to fine-tune the model without memory issues.\n\nNext is to create a¬†`DataCollator`¬†that will take care of padding our inputs and labels. We will use the¬†`DataCollatorForSeq2Seq`¬†from the ü§ó Transformers library.","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\n# we want to ignore tokenizer pad token in the loss\nlabel_pad_token_id = -100\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=label_pad_token_id,\n    pad_to_multiple_of=8\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:08:33.226776Z","iopub.execute_input":"2023-10-13T20:08:33.227158Z","iopub.status.idle":"2023-10-13T20:08:45.066905Z","shell.execute_reply.started":"2023-10-13T20:08:33.227127Z","shell.execute_reply":"2023-10-13T20:08:45.065960Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"The last step is to define the hyperparameters (`TrainingArguments`) we want to use for our training.","metadata":{}},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\noutput_dir=\"lora-flan-t5-base\"\n\n# Define training args\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=output_dir,\n\t\t#auto_find_batch_size=True,\n    per_device_train_batch_size=100,\n    learning_rate=1e-3, # higher learning rate\n    num_train_epochs=2,\n    logging_dir=f\"{output_dir}/logs\",\n    logging_strategy=\"steps\",\n    logging_steps=500,\n    save_strategy=\"no\",\n    report_to=\"tensorboard\",\n)\n\n# Create Trainer instance\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_dataset[\"train\"],\n)\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:14:16.469505Z","iopub.execute_input":"2023-10-13T20:14:16.469943Z","iopub.status.idle":"2023-10-13T20:14:16.482827Z","shell.execute_reply.started":"2023-10-13T20:14:16.469910Z","shell.execute_reply":"2023-10-13T20:14:16.481637Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Let's now train our model and run the cells below. Note that for T5, some layers are kept in¬†`float32`¬†for stability purposes.","metadata":{}},{"cell_type":"code","source":"# train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:14:25.724184Z","iopub.execute_input":"2023-10-13T20:14:25.724497Z","iopub.status.idle":"2023-10-13T20:53:38.862471Z","shell.execute_reply.started":"2023-10-13T20:14:25.724471Z","shell.execute_reply":"2023-10-13T20:53:38.861552Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='296' max='296' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [296/296 39:04, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=296, training_loss=1.525888391443201, metrics={'train_runtime': 2352.7363, 'train_samples_per_second': 12.523, 'train_steps_per_second': 0.126, 'total_flos': 1.0167925786804224e+16, 'train_loss': 1.525888391443201, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"The training took ~10:36:00 and cost `~13.22$` for 10h of training. For comparison a [full fine-tuning on FLAN-T5-XXL](https://www.philschmid.de/fine-tune-flan-t5-deepspeed#3-results--experiments) with the same duration (10h) requires 8x A100 40GBs and costs ~322$. \n\nWe can save our model to use it for inference and evaluate it. We will save it to disk for now, but you could also upload it to the [Hugging Face Hub](https://huggingface.co/docs/hub/main) using the `model.push_to_hub` method.","metadata":{}},{"cell_type":"code","source":"# Save our LoRA model & tokenizer results\npeft_model_id=\"results\"\ntrainer.model.save_pretrained(peft_model_id)\ntokenizer.save_pretrained(peft_model_id)\n# if you want to save the base model to call\n# trainer.model.base_model.save_pretrained(peft_model_id)","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:53:55.048408Z","iopub.execute_input":"2023-10-13T20:53:55.048805Z","iopub.status.idle":"2023-10-13T20:53:55.156598Z","shell.execute_reply.started":"2023-10-13T20:53:55.048767Z","shell.execute_reply":"2023-10-13T20:53:55.155701Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"('results/tokenizer_config.json',\n 'results/special_tokens_map.json',\n 'results/spiece.model',\n 'results/added_tokens.json',\n 'results/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"Our LoRA checkpoint is only 84MB small and includes all of the learnt knowleddge for samsum.\n\n## 4. Evaluate & run Inference with LoRA FLAN-T5\n\nAfter the training is done we want to evaluate and test it. The most commonly used metric to evaluate summarization task is¬†[rogue_score](https://en.wikipedia.org/wiki/ROUGE_(metric))¬†short for Recall-Oriented Understudy for Gisting Evaluation). This metric does not behave like the standard accuracy: it will compare a generated summary against a set of reference summaries.\n\nWe are going to use¬†`evaluate`¬†library to evaluate the¬†`rogue`¬†score. We can run inference using `PEFT` and `transformers`. For our FLAN-T5 XXL model, we need at least 18GB of GPU memory.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# Load peft config for pre-trained checkpoint etc. \npeft_model_id = \"results\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\n\n# load base LLM model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={\"\":0})\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\n# Load the Lora model\nmodel = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\nmodel.eval()\n\nprint(\"Peft model loaded\")","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:54:01.273205Z","iopub.execute_input":"2023-10-13T20:54:01.273862Z","iopub.status.idle":"2023-10-13T20:54:03.687494Z","shell.execute_reply.started":"2023-10-13T20:54:01.273830Z","shell.execute_reply":"2023-10-13T20:54:03.686562Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Peft model loaded\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let‚Äôs load the dataset again with a random sample to try the summarization.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset \nfrom random import randrange\n\n\n# Load dataset from the hub and get a sample\ndataset = load_dataset(\"samsum\")\nsample = dataset['test'][randrange(len(dataset[\"test\"]))]\n\ninput_ids = tokenizer(sample[\"dialogue\"], return_tensors=\"pt\", truncation=True).input_ids.cuda()\n# with torch.inference_mode():\noutputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9)\nprint(f\"input sentence: {sample['dialogue']}\\n{'---'* 20}\")\n\nprint(f\"summary:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:54:22.320492Z","iopub.execute_input":"2023-10-13T20:54:22.321626Z","iopub.status.idle":"2023-10-13T20:54:27.756601Z","shell.execute_reply.started":"2023-10-13T20:54:22.321546Z","shell.execute_reply":"2023-10-13T20:54:27.755666Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c30dfbb2769944a69dd8fb97797677bb"}},"metadata":{}},{"name":"stdout","text":"input sentence: Lincoln: Heeyyy ;* whats up\nFatima: I talked to Jenson, he‚Äôs not too happy ;p\nLincoln: the place sucks??\nFatima: No, the place is ok, I think, we can go there, it‚Äôs about Alene\nLincoln: typical, dont worry about it\nFatima: He thinks she may have a depression :[\nLincoln: nothin new, everyone has it, she needs a doctor then\nFatima: But she won‚Äôt go ;/\nLincoln: so she‚Äôs destroying her life fuck it its not your problem\nFatima: It is, they‚Äôre both my friends!\nLincoln: you better think what to do if they break up\nFatima: Ehh yes Ill have a problem ;//\nLincoln: both blaming each other and talking with you about it, perfect\nFatima: Alene is just troubled‚Ä¶ She‚Äôd been through a lot‚Ä¶\nLincoln: everyone has their problems, the question is are ya doin sth about them\nFatima: She has problems facing it, don‚Äôt be surprised :[\nLincoln: then it is her problem\nFatima: You are so cruel at times‚Ä¶ o.O\nLincoln: maybe, for me its just a common sense\nFatima: Why can‚Äôt everyone be just happy???\nLincoln: youll not understand, you had good childhood, nice parents, you have no idea\nFatima: Probably, true‚Ä¶ Well I can be just grateful o.o\nLincoln: do that and stop worrying about others, youre way to bautful for that <3\nFatima: :*:*:*\n------------------------------------------------------------\nsummary:\nFatima talked to Jenson and he's not too happy about the place. It's about Alene, a depression. He thinks she's destroying her life. Both her friends need a doctor. Fatima knows her problems are hers, but\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Nice! our model works! Now, lets take a closer look and evaluate it against the `test` set of processed dataset from `samsum`. Therefore we need to use and create some utilities to generate the summaries and group them together. The most commonly used metrics to evaluate summarization task is [rogue_score](https://en.wikipedia.org/wiki/ROUGE_(metric)) short for Recall-Oriented Understudy for Gisting Evaluation). This metric does not behave like the standard accuracy: it will compare a generated summary against a set of reference summaries.","metadata":{}},{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:55:53.506929Z","iopub.execute_input":"2023-10-13T20:55:53.507285Z","iopub.status.idle":"2023-10-13T20:56:02.719524Z","shell.execute_reply.started":"2023-10-13T20:55:53.507257Z","shell.execute_reply":"2023-10-13T20:56:02.718379Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nimport numpy as np\nfrom datasets import load_from_disk\nfrom tqdm import tqdm\n\n\nmodel_id = \"google/flan-t5-base\"\n\n# load model from the hub\nmodel_original = AutoModelForSeq2SeqLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n\n# Metric\nmetric = evaluate.load(\"rouge\")\n\ndef evaluate_peft_model(sample,max_target_length=50):\n    # generate summary\n    outputs = model_original.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    \n    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n    # decode eval sample\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n    labels = tokenizer.decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    return prediction, labels\n\n# load test dataset from distk\ntest_dataset = load_from_disk(\"data/eval/\").with_format(\"torch\")\n\n# run predictions\n# this can take ~45 minutes\npredictions, references = [] , []\nfor i,sample in tqdm(enumerate(test_dataset)):\n    p,l = evaluate_peft_model(sample)\n    predictions.append(p)\n    references.append(l)\n    if i == 20:\n        break\n\n# compute metric \nrogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n\n# print results \nprint(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\nprint(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\nprint(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\nprint(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")\n\n# Rogue1: 50.386161%\n# rouge2: 24.842412%\n# rougeL: 41.370130%\n# rougeLsum: 41.394230%","metadata":{"execution":{"iopub.status.busy":"2023-10-13T21:01:20.601406Z","iopub.execute_input":"2023-10-13T21:01:20.602099Z","iopub.status.idle":"2023-10-13T21:01:55.446768Z","shell.execute_reply.started":"2023-10-13T21:01:20.602063Z","shell.execute_reply":"2023-10-13T21:01:55.445738Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"20it [00:32,  1.60s/it]","output_type":"stream"},{"name":"stdout","text":"Rogue1: 42.952128%\nrouge2: 15.706550%\nrougeL: 32.636315%\nrougeLsum: 32.472090%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"----","metadata":{}},{"cell_type":"code","source":"import evaluate\nimport numpy as np\nfrom datasets import load_from_disk\nfrom tqdm import tqdm\n\n# Metric\nmetric = evaluate.load(\"rouge\")\n\ndef evaluate_peft_model(sample,max_target_length=50):\n    # generate summary\n    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    \n    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n    # decode eval sample\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n    labels = tokenizer.decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    return prediction, labels\n\n# load test dataset from distk\ntest_dataset = load_from_disk(\"data/eval/\").with_format(\"torch\")\n\n# run predictions\n# this can take ~45 minutes\npredictions, references = [] , []\nfor i,sample in tqdm(enumerate(test_dataset)):\n    p,l = evaluate_peft_model(sample)\n    predictions.append(p)\n    references.append(l)\n    if i == 20:\n        break\n\n# compute metric \nrogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n\n# print results \nprint(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\nprint(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\nprint(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\nprint(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")\n\n# Rogue1: 50.386161%\n# rouge2: 24.842412%\n# rougeL: 41.370130%\n# rougeLsum: 41.394230%","metadata":{"execution":{"iopub.status.busy":"2023-10-13T21:01:55.448712Z","iopub.execute_input":"2023-10-13T21:01:55.449306Z","iopub.status.idle":"2023-10-13T21:02:54.559658Z","shell.execute_reply.started":"2023-10-13T21:01:55.449269Z","shell.execute_reply":"2023-10-13T21:02:54.558762Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"20it [00:58,  2.91s/it]\n","output_type":"stream"},{"name":"stdout","text":"Rogue1: 43.526789%\nrouge2: 15.797947%\nrougeL: 31.992391%\nrougeLsum: 31.986698%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Our PEFT fine-tuned FLAN-T5-XXL achieved a rogue1 score of `50.38%` on the test dataset. For comparison a [full fine-tuning of flan-t5-base achieved a rouge1 score of 47.23](https://www.philschmid.de/fine-tune-flan-t5). That is a `3%` improvements. \n\nIt is incredible to see that our LoRA checkpoint is only 84MB small and model achieves better performance than a smaller fully fine-tuned model.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}